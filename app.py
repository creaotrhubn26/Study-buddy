import streamlit as st
import pandas as pd
from openai import OpenAI
import os
import json
import time
import re
import base64
from datetime import datetime, timedelta, date
from streamlit.components.v1 import html
try:
    from spellchecker import SpellChecker
    SPELLCHECK_AVAILABLE = True
except ImportError:
    SPELLCHECK_AVAILABLE = False

# Material Icons CSS and helper function
MATERIAL_ICONS_CSS = """
<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
<style>
.material-icons {
    font-family: 'Material Icons';
    font-weight: normal;
    font-style: normal;
    font-size: 24px;
    line-height: 1;
    letter-spacing: normal;
    text-transform: none;
    display: inline-block;
    white-space: nowrap;
    word-wrap: normal;
    direction: ltr;
    -webkit-font-feature-settings: 'liga';
    -webkit-font-smoothing: antialiased;
    vertical-align: middle;
}
.material-icons.md-18 { font-size: 18px; }
.material-icons.md-24 { font-size: 24px; }
.material-icons.md-36 { font-size: 36px; }
.material-icons.md-48 { font-size: 48px; }
.mui-icon { 
    font-family: 'Material Icons';
    font-size: 20px;
    vertical-align: middle;
    margin-right: 4px;
}
</style>
"""
st.markdown(MATERIAL_ICONS_CSS, unsafe_allow_html=True)

def mui_icon(icon_name, size=20):
    """Helper function to render Material Icons"""
    return f'<span class="mui-icon" style="font-size: {size}px;">{icon_name}</span>'
    
def mui_title(icon_name, text):
    st.markdown(f"# {mui_icon(icon_name, 30)} {text}", unsafe_allow_html=True)

def mui_subheader(icon_name, text):
    st.markdown(f"### {mui_icon(icon_name, 22)} {text}", unsafe_allow_html=True)


EMOJI_HEADING_PATTERN = re.compile(r'[\U0001F300-\U0001FAFF\u2600-\u27BF]')


def _escape_svg_text(value):
    return (
        value.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
    )


def _build_heading_banner(title, level):
    palette = {
        2: ("#E8F3FF", "#4A90D9", "#1F4E79"),
        3: ("#EAFBF1", "#39A96B", "#1D6B44"),
        4: ("#FFF4E8", "#E58E26", "#8A4A00"),
        5: ("#F3F0FF", "#7E57C2", "#4A2B87"),
        6: ("#F5F5F5", "#8E8E8E", "#333333"),
    }
    bg, border, text_color = palette.get(level, palette[4])
    safe_title = _escape_svg_text(title)
    return f"""
<svg width=\"680\" height=\"72\" viewBox=\"0 0 680 72\" xmlns=\"http://www.w3.org/2000/svg\" role=\"img\" aria-label=\"Section visual banner\">
  <rect x=\"1\" y=\"1\" width=\"678\" height=\"70\" rx=\"10\" fill=\"{bg}\" stroke=\"{border}\" stroke-width=\"2\"/>
  <text x=\"20\" y=\"44\" fill=\"{text_color}\" font-size=\"20\" font-weight=\"700\">{safe_title}</text>
</svg>
"""


def convert_emoji_headings_to_visuals(markdown_text):
    lines = markdown_text.splitlines()
    transformed = []
    in_code_block = False

    for line in lines:
        stripped = line.strip()

        if stripped.startswith("```"):
            in_code_block = not in_code_block
            transformed.append(line)
            continue

        if in_code_block:
            transformed.append(line)
            continue

        heading_match = re.match(r'^(#{2,6})\s+(.+)$', stripped)
        if not heading_match:
            transformed.append(line)
            continue

        hashes = heading_match.group(1)
        heading_text = heading_match.group(2)

        if not EMOJI_HEADING_PATTERN.search(heading_text):
            transformed.append(line)
            continue

        clean_heading = EMOJI_HEADING_PATTERN.sub('', heading_text)
        clean_heading = re.sub(r'\s{2,}', ' ', clean_heading).strip(' -')
        if not clean_heading:
            clean_heading = heading_text

        level = len(hashes)
        transformed.append(_build_heading_banner(clean_heading, level))
        transformed.append(f"{hashes} {clean_heading}")

    return "\n".join(transformed)

st.set_page_config(
    page_title="Data Analyst Study App",
    page_icon="ðŸ“Š",
    layout="wide"
)

class MissingOpenAIClient:
    def __getattr__(self, _name):
        return self

    def __call__(self, *args, **kwargs):
        raise RuntimeError(
            "OpenAI API key is not configured. Set OPENAI_API_KEY or AI_INTEGRATIONS_OPENAI_API_KEY to use AI features."
        )


openai_api_key = os.environ.get("AI_INTEGRATIONS_OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY")
openai_base_url = os.environ.get("AI_INTEGRATIONS_OPENAI_BASE_URL") or os.environ.get("OPENAI_BASE_URL")

if openai_api_key:
    client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_base_url
    )
else:
    client = MissingOpenAIClient()

# Training modules with structured lessons
training_modules = {
    "Introduction to business intelligence and big data": {
        "course": "Data Analysis Fundamentals",
        "description": "Learn what Business Intelligence (BI) and Big Data mean, and how they help companies make better decisions.",
        "lessons": [
            {
                "title": "What is Business Intelligence?",
                "content": """
**Business Intelligence (BI)** is the process of collecting, analyzing, and presenting business data to help companies make better decisions.

**Key Components of BI:**
- **Data Collection**: Gathering data from various sources (sales, customers, operations)
- **Data Storage**: Storing data in databases or data warehouses
- **Data Analysis**: Using tools to find patterns and insights
- **Reporting**: Creating dashboards and reports to share findings

**Real-World Example:**
A retail store uses BI to analyze which products sell best during different seasons. They discover that winter jackets sell 300% more in November than in July, so they stock up accordingly.
                """,
                "key_points": ["BI helps companies make data-driven decisions", "It involves collecting, analyzing, and presenting data", "Dashboards and reports are key outputs"]
            },
            {
                "title": "What is Big Data?",
                "content": """
**Big Data** refers to extremely large datasets that are too complex for traditional data processing tools.

**The 3 V's of Big Data:**
- **Volume**: Massive amounts of data (terabytes, petabytes)
- **Velocity**: Data is generated very quickly (real-time)
- **Variety**: Different types of data (text, images, videos, sensors)

**Examples of Big Data Sources:**
- Social media posts (millions per minute)
- IoT sensors (temperature, location, movement)
- Online transactions
- Website clickstreams

**Real-World Example:**
Netflix analyzes viewing habits of 200+ million users to recommend shows. This involves processing billions of data points daily.
                """,
                "key_points": ["Big Data = Volume + Velocity + Variety", "Traditional tools cannot handle Big Data", "Companies like Netflix and Amazon rely on Big Data"]
            },
            {
                "title": "BI vs Big Data: Key Differences",
                "content": """
**Business Intelligence** and **Big Data** work together but serve different purposes:

| Aspect | Business Intelligence | Big Data |
|--------|----------------------|----------|
| **Focus** | Analyzing structured data | Processing all data types |
| **Data Size** | Gigabytes to Terabytes | Terabytes to Petabytes |
| **Questions** | "What happened?" | "What might happen?" |
| **Tools** | Tableau, Power BI, Excel | Hadoop, Spark, Python |
| **Users** | Business analysts | Data scientists |

**How They Work Together:**
1. Big Data systems collect and process massive datasets
2. BI tools visualize and report on the processed data
3. Decision-makers use BI dashboards to take action
                """,
                "key_points": ["BI focuses on structured data and reporting", "Big Data handles massive, varied datasets", "They complement each other in modern analytics"]
            }
        ],
        "exercises": [
            {
                "title": "Identify BI Use Cases",
                "type": "scenario",
                "question": "A coffee shop chain wants to know which menu items are most popular at each location. They have sales data from 50 stores over 2 years. Is this a BI or Big Data problem?",
                "answer": "This is a Business Intelligence problem. The data is structured (sales records), moderate in size (2 years Ã— 50 stores), and the goal is to analyze 'what happened' to make business decisions. BI tools like Excel or Tableau would be appropriate.",
                "hint": "Think about the 3 V's - is this data extremely large, fast, or varied?"
            },
            {
                "title": "Big Data Scenario",
                "type": "scenario",
                "question": "A social media company needs to analyze 500 million posts per day, including text, images, and videos, to detect trending topics in real-time. Is this BI or Big Data?",
                "answer": "This is a Big Data problem. It involves massive Volume (500M posts/day), high Velocity (real-time), and Variety (text, images, videos). Big Data tools like Hadoop or Spark would be needed.",
                "hint": "Check all 3 V's: Volume, Velocity, and Variety"
            },
            {
                "title": "Calculate Data Volume",
                "type": "practical",
                "question": "A company stores 1,000 customer transactions per day. Each transaction record is 2 KB. How much data do they generate in one year?",
                "answer": "1,000 transactions Ã— 2 KB Ã— 365 days = 730,000 KB = 730 MB per year. This is manageable with traditional BI tools.",
                "hint": "Multiply daily transactions Ã— size Ã— days in a year"
            }
        ],
        "quiz": [
            {
                "question": "What does BI stand for?",
                "options": ["Big Information", "Business Intelligence", "Binary Integration", "Basic Insights"],
                "correct": 1,
                "explanation": "BI stands for Business Intelligence - the process of analyzing business data to make better decisions."
            },
            {
                "question": "Which is NOT one of the 3 V's of Big Data?",
                "options": ["Volume", "Velocity", "Value", "Variety"],
                "correct": 2,
                "explanation": "The 3 V's are Volume, Velocity, and Variety. Value is sometimes added as a 4th V, but it's not part of the original definition."
            },
            {
                "question": "Which tool is typically used for Business Intelligence?",
                "options": ["Hadoop", "Tableau", "Spark", "TensorFlow"],
                "correct": 1,
                "explanation": "Tableau is a BI visualization tool. Hadoop and Spark are Big Data processing tools, and TensorFlow is for machine learning."
            },
            {
                "question": "A dataset of 10 million social media posts updated every second is an example of:",
                "options": ["Business Intelligence", "Big Data", "Traditional database", "Spreadsheet data"],
                "correct": 1,
                "explanation": "This exhibits Big Data characteristics: high Volume (10M posts), high Velocity (every second), and likely Variety (text, images, etc.)."
            }
        ]
    },
    "Statistical methodologies to extract KPIs": {
        "course": "Statistical Tools",
        "description": "Learn how to use statistics to calculate and analyze Key Performance Indicators (KPIs).",
        "lessons": [
            {
                "title": "What are KPIs?",
                "content": """
**Key Performance Indicators (KPIs)** are measurable values that show how effectively a company is achieving its objectives.

**Characteristics of Good KPIs:**
- **Specific**: Clearly defined
- **Measurable**: Can be quantified
- **Achievable**: Realistic targets
- **Relevant**: Aligned with business goals
- **Time-bound**: Has a deadline

**Common Business KPIs:**
- Revenue growth rate
- Customer acquisition cost
- Employee turnover rate
- Net Promoter Score (NPS)
- Conversion rate
                """,
                "key_points": ["KPIs measure business performance", "Good KPIs are SMART", "Different industries have different KPIs"]
            },
            {
                "title": "Statistical Measures for KPIs",
                "content": """
**Basic Statistical Measures:**

**Mean (Average):**
Sum of all values Ã· Number of values
Example: Sales of 100, 150, 200 â†’ Mean = 150

**Median:**
The middle value when sorted
Example: 100, 150, 200 â†’ Median = 150

**Standard Deviation:**
Measures how spread out values are
Low SD = values close to mean
High SD = values spread out

**Percentage Change:**
((New - Old) / Old) Ã— 100
Example: Sales went from 1000 to 1200
Change = ((1200-1000)/1000) Ã— 100 = 20%
                """,
                "key_points": ["Mean shows the average", "Median is better when there are outliers", "Standard deviation shows variability"]
            },
            {
                "title": "Calculating KPIs in Practice",
                "content": """
**Example: Customer Retention Rate**

Formula: ((Customers at End - New Customers) / Customers at Start) Ã— 100

**Scenario:**
- Start of month: 1000 customers
- New customers acquired: 200
- End of month: 1050 customers

Calculation:
((1050 - 200) / 1000) Ã— 100 = 85%

**Interpretation:**
85% retention rate means 15% of customers left (churned).

**Example: Average Order Value (AOV)**

Formula: Total Revenue / Number of Orders

**Scenario:**
- Monthly revenue: $50,000
- Total orders: 1,000

AOV = $50,000 / 1,000 = $50

This means customers spend $50 on average per order.
                """,
                "key_points": ["KPIs use simple formulas", "Always compare KPIs over time", "Context matters for interpretation"]
            }
        ],
        "exercises": [
            {
                "title": "Calculate Mean Sales",
                "type": "practical",
                "question": "A store had daily sales of: $500, $750, $600, $800, $650. Calculate the mean (average) daily sales.",
                "answer": "Mean = (500 + 750 + 600 + 800 + 650) / 5 = 3300 / 5 = $660. The average daily sales is $660.",
                "hint": "Add all values and divide by the count"
            },
            {
                "title": "Calculate Percentage Change",
                "type": "practical",
                "question": "Website traffic was 10,000 visitors last month and 12,500 this month. What is the percentage change?",
                "answer": "Percentage Change = ((12,500 - 10,000) / 10,000) Ã— 100 = (2,500 / 10,000) Ã— 100 = 25%. Traffic increased by 25%.",
                "hint": "Use the formula: ((New - Old) / Old) Ã— 100"
            },
            {
                "title": "Calculate Customer Retention",
                "type": "practical",
                "question": "A company started with 500 customers, gained 100 new customers, and ended with 480 customers. What is the retention rate?",
                "answer": "Retention Rate = ((480 - 100) / 500) Ã— 100 = (380 / 500) Ã— 100 = 76%. The company retained 76% of its original customers.",
                "hint": "Subtract new customers from ending total, then divide by starting customers"
            }
        ],
        "quiz": [
            {
                "question": "What does KPI stand for?",
                "options": ["Key Performance Index", "Key Performance Indicator", "Knowledge Process Integration", "Key Process Information"],
                "correct": 1,
                "explanation": "KPI stands for Key Performance Indicator - a measurable value that shows business performance."
            },
            {
                "question": "If sales were 100, 200, 300, 400, 500, what is the median?",
                "options": ["300", "280", "350", "250"],
                "correct": 0,
                "explanation": "The median is the middle value when sorted. In 100, 200, 300, 400, 500, the middle value is 300."
            },
            {
                "question": "Revenue went from $80,000 to $100,000. What is the percentage increase?",
                "options": ["20%", "25%", "15%", "30%"],
                "correct": 1,
                "explanation": "((100,000 - 80,000) / 80,000) Ã— 100 = (20,000 / 80,000) Ã— 100 = 25%"
            }
        ]
    },
    "Correlation, regression, ANOVA, histogram and covariance analysis": {
        "course": "Statistical Tools",
        "description": "Learn essential statistical analysis techniques used in data analysis.",
        "lessons": [
            {
                "title": "Understanding Correlation",
                "content": """
**Correlation** measures the relationship between two variables.

**Correlation Coefficient (r):**
- Ranges from -1 to +1
- **+1**: Perfect positive correlation (as X increases, Y increases)
- **0**: No correlation
- **-1**: Perfect negative correlation (as X increases, Y decreases)

**Interpreting Correlation:**
- 0.7 to 1.0: Strong positive
- 0.4 to 0.7: Moderate positive
- 0.0 to 0.4: Weak or no correlation

**Example:**
Ice cream sales and temperature have a positive correlation (r â‰ˆ 0.8).
When temperature rises, ice cream sales rise too.

**Important:** Correlation â‰  Causation!
Just because two things correlate doesn't mean one causes the other.
                """,
                "key_points": ["Correlation ranges from -1 to +1", "Positive correlation = both increase together", "Correlation does not prove causation"]
            },
            {
                "title": "Introduction to Regression",
                "content": """
**Regression** predicts one variable based on another.

**Simple Linear Regression:**
y = mx + b
- y = predicted value
- m = slope (how much y changes for each unit of x)
- x = input value
- b = y-intercept (value of y when x = 0)

**Example:**
Predicting sales based on advertising spend:
Sales = 2.5 Ã— Ad_Spend + 1000

If you spend $500 on ads:
Sales = 2.5 Ã— 500 + 1000 = $2,250

**R-squared (RÂ²):**
- Measures how well the regression fits the data
- Ranges from 0 to 1
- RÂ² = 0.85 means 85% of variation is explained by the model
                """,
                "key_points": ["Regression predicts outcomes", "y = mx + b is the basic formula", "RÂ² shows how good the prediction is"]
            },
            {
                "title": "Histograms and Data Distribution",
                "content": """
**Histogram** is a chart showing how data is distributed across ranges (bins).

**How to Read a Histogram:**
- X-axis: Value ranges (bins)
- Y-axis: Frequency (count)
- Tall bars = many values in that range

**Common Distribution Shapes:**
- **Normal (Bell Curve)**: Most values in the middle
- **Skewed Right**: Tail extends to the right
- **Skewed Left**: Tail extends to the left
- **Bimodal**: Two peaks

**Example:**
Employee salary histogram might show:
- Most employees earn $40,000-$60,000
- Few earn above $100,000
- This is right-skewed (long tail toward high salaries)
                """,
                "key_points": ["Histograms show data distribution", "Normal distribution is bell-shaped", "Skewness shows data is not symmetric"]
            },
            {
                "title": "ANOVA and Covariance Basics",
                "content": """
**ANOVA (Analysis of Variance)**
Compares means across multiple groups to see if differences are significant.

**When to Use ANOVA:**
- Comparing sales across 3+ regions
- Comparing test scores across different teaching methods
- Comparing customer satisfaction across product lines

**Example:**
Testing if coffee brand affects taste ratings:
- Brand A: average rating 4.2
- Brand B: average rating 3.8
- Brand C: average rating 4.5
ANOVA tells you if these differences are statistically significant.

**Covariance**
Measures how two variables change together.
- Positive covariance: both increase/decrease together
- Negative covariance: one increases as other decreases
- Similar to correlation but not standardized
                """,
                "key_points": ["ANOVA compares means across groups", "Use ANOVA for 3+ groups", "Covariance shows joint variability"]
            }
        ],
        "exercises": [
            {
                "title": "Interpret Correlation",
                "type": "scenario",
                "question": "A study finds correlation of r = 0.85 between study hours and exam scores. What does this mean?",
                "answer": "This is a strong positive correlation. Students who study more hours tend to score higher on exams. However, this doesn't prove that studying causes higher scores - there could be other factors.",
                "hint": "0.85 is close to 1, indicating a strong positive relationship"
            },
            {
                "title": "Use Regression Formula",
                "type": "practical",
                "question": "A regression model shows: Revenue = 3 Ã— Marketing_Spend + 5000. If you spend $2000 on marketing, what is the predicted revenue?",
                "answer": "Revenue = 3 Ã— 2000 + 5000 = 6000 + 5000 = $11,000. The predicted revenue is $11,000.",
                "hint": "Substitute the marketing spend value into the formula"
            },
            {
                "title": "Choose the Right Test",
                "type": "scenario",
                "question": "You want to compare customer satisfaction scores across 4 different store locations. Which statistical test should you use?",
                "answer": "Use ANOVA (Analysis of Variance). ANOVA is designed to compare means across 3 or more groups, making it perfect for comparing satisfaction across 4 store locations.",
                "hint": "You're comparing means across multiple groups"
            }
        ],
        "quiz": [
            {
                "question": "A correlation of r = -0.9 means:",
                "options": ["Strong positive relationship", "No relationship", "Strong negative relationship", "Weak relationship"],
                "correct": 2,
                "explanation": "-0.9 is close to -1, indicating a strong negative relationship. As one variable increases, the other decreases."
            },
            {
                "question": "In y = mx + b, what does 'm' represent?",
                "options": ["Y-intercept", "Slope", "Correlation", "Mean"],
                "correct": 1,
                "explanation": "In the linear equation, m is the slope - it shows how much y changes for each unit increase in x."
            },
            {
                "question": "When should you use ANOVA?",
                "options": ["Comparing 2 groups", "Comparing 3+ groups", "Finding correlation", "Creating histograms"],
                "correct": 1,
                "explanation": "ANOVA is used to compare means across 3 or more groups. For 2 groups, you would use a t-test."
            }
        ]
    },
    "Z-scores and z-testing for outlier reduction": {
        "course": "Statistical Tools",
        "description": "Learn how to identify and handle outliers using z-scores.",
        "lessons": [
            {
                "title": "What is a Z-Score?",
                "content": """
**Z-Score** measures how many standard deviations a value is from the mean.

**Formula:**
Z = (Value - Mean) / Standard Deviation

**Example:**
- Mean height = 170 cm
- Standard Deviation = 10 cm
- Your height = 190 cm

Z = (190 - 170) / 10 = 2

This means you are 2 standard deviations above the mean.

**Interpreting Z-Scores:**
- Z = 0: Value equals the mean
- Z = 1: Value is 1 SD above mean
- Z = -1: Value is 1 SD below mean
- Z > 3 or Z < -3: Likely an outlier
                """,
                "key_points": ["Z-score shows distance from mean in SDs", "Z = 0 means value equals the mean", "High absolute Z-scores indicate outliers"]
            },
            {
                "title": "Identifying Outliers with Z-Scores",
                "content": """
**Outliers** are data points that are significantly different from other values.

**Common Rule:**
- |Z| > 3 = Outlier (99.7% of data falls within Â±3 SD)
- |Z| > 2 = Potential outlier (95% of data falls within Â±2 SD)

**Example Dataset:** Sales: 100, 120, 110, 115, 105, 500
- Mean = 175
- SD = 150

Z-score for 500:
Z = (500 - 175) / 150 = 2.17

This is a potential outlier!

**Why Remove Outliers?**
- Outliers can skew averages
- They may indicate errors in data
- They can affect statistical models
                """,
                "key_points": ["Z > 3 or Z < -3 typically indicates outliers", "Outliers can distort analysis", "Always investigate why outliers exist"]
            },
            {
                "title": "Practical Outlier Handling",
                "content": """
**Steps to Handle Outliers:**

1. **Calculate Z-scores** for all data points
2. **Identify outliers** (|Z| > 3 or your chosen threshold)
3. **Investigate** why they exist:
   - Data entry error?
   - Measurement error?
   - Genuine unusual value?
4. **Decide action:**
   - Remove if error
   - Keep if genuine (but note it)
   - Transform data (e.g., use median instead of mean)

**Example in Excel/Sheets:**
```
=STANDARDIZE(A2, AVERAGE(A:A), STDEV(A:A))
```
This calculates the Z-score for cell A2.

**Best Practice:**
Always document which outliers were removed and why!
                """,
                "key_points": ["Calculate Z-scores for all values", "Investigate before removing", "Document your decisions"]
            }
        ],
        "exercises": [
            {
                "title": "Calculate Z-Score",
                "type": "practical",
                "question": "Mean = 50, Standard Deviation = 10. Calculate the Z-score for a value of 75.",
                "answer": "Z = (75 - 50) / 10 = 25 / 10 = 2.5. The value is 2.5 standard deviations above the mean.",
                "hint": "Use the formula: Z = (Value - Mean) / SD"
            },
            {
                "title": "Identify the Outlier",
                "type": "practical",
                "question": "Dataset: 20, 22, 21, 19, 23, 20, 85. Mean = 30, SD = 23. Which value is likely an outlier? Calculate its Z-score.",
                "answer": "The value 85 is the outlier. Z = (85 - 30) / 23 = 55 / 23 = 2.39. While not above 3, it's clearly different from the other values which all cluster around 20-23.",
                "hint": "The value that's very different from the others"
            },
            {
                "title": "Decision Making",
                "type": "scenario",
                "question": "Your sales data shows one transaction of $50,000 when average is $500 with SD of $200. Z-score is 247.5. Should you remove this value?",
                "answer": "You should investigate first! This extreme Z-score (247.5) suggests either: 1) Data entry error ($500 became $50,000), 2) A legitimate large order. Check the original records before removing.",
                "hint": "Don't automatically remove - investigate the cause"
            }
        ],
        "quiz": [
            {
                "question": "What does a Z-score of 0 mean?",
                "options": ["Value is missing", "Value equals the mean", "Value is an outlier", "Value is negative"],
                "correct": 1,
                "explanation": "A Z-score of 0 means the value is exactly at the mean (0 standard deviations away)."
            },
            {
                "question": "A value with Z-score of -2.5 is:",
                "options": ["2.5 SDs above the mean", "2.5 SDs below the mean", "Exactly at the mean", "Not calculable"],
                "correct": 1,
                "explanation": "A negative Z-score means the value is below the mean. -2.5 means 2.5 standard deviations below."
            },
            {
                "question": "Which Z-score most likely indicates an outlier?",
                "options": ["Z = 0.5", "Z = 1.2", "Z = 2.0", "Z = 3.5"],
                "correct": 3,
                "explanation": "Z = 3.5 is beyond the Â±3 threshold commonly used to identify outliers. Only 0.3% of data falls beyond Â±3 SDs."
            }
        ]
    },
    "Tool: Excel & Google Sheets": {
        "course": "Spreadsheet Fundamentals",
        "tool_type": "spreadsheet",
        "description": "Master spreadsheet tools for data analysis - learn formulas, pivot tables, data cleaning, and automation techniques.",
        "lessons": [
            {
                "title": "Essential Formulas for Data Analysis",
                "content": """
**Key Formulas Every Data Analyst Needs:**

**1. VLOOKUP / XLOOKUP (Finding Data)**
```
=VLOOKUP(lookup_value, table_array, col_index, FALSE)
=XLOOKUP(lookup_value, lookup_array, return_array)
```
*Use case:* Match customer IDs to their names across different sheets.

**2. SUMIF / COUNTIF (Conditional Calculations)**
```
=SUMIF(range, criteria, sum_range)
=COUNTIF(range, criteria)
```
*Use case:* Sum all sales for a specific product or count orders from a region.

**3. IF / IFS (Logic)**
```
=IF(condition, value_if_true, value_if_false)
=IFS(condition1, value1, condition2, value2, ...)
```
*Use case:* Categorize sales as "High", "Medium", or "Low" based on amount.

**4. INDEX/MATCH (Advanced Lookup)**
```
=INDEX(return_range, MATCH(lookup_value, lookup_range, 0))
```
*Use case:* More flexible than VLOOKUP - can look left or right in a table.

**Real-World Scenario:**
You have a sales report with 10,000 rows. Your manager wants to know total sales by region. Instead of manually calculating, you use:
```
=SUMIF(B:B, "North", D:D)
```
This instantly sums all sales where region (column B) equals "North".
                """,
                "key_points": ["VLOOKUP finds data across sheets", "SUMIF/COUNTIF calculate with conditions", "INDEX/MATCH is more powerful than VLOOKUP"]
            },
            {
                "title": "Pivot Tables for Quick Analysis",
                "content": """
**Pivot Tables: Your Data Analysis Superpower**

Pivot tables summarize thousands of rows of data in seconds without writing formulas.

**Creating a Pivot Table (Excel):**
1. Select your data (including headers)
2. Insert â†’ Pivot Table
3. Drag fields to Rows, Columns, Values, and Filters

**Example Scenario:**
You have a dataset of 50,000 sales transactions with columns:
- Date, Product, Region, Salesperson, Amount

**Analysis Tasks Made Easy:**

| Task | Pivot Table Setup |
|------|-------------------|
| Total sales by product | Rows: Product, Values: Sum of Amount |
| Sales by region and month | Rows: Region, Columns: Month, Values: Sum of Amount |
| Top salespeople | Rows: Salesperson, Values: Sum of Amount (sort descending) |
| Average order size by product | Rows: Product, Values: Average of Amount |

**Key Features:**
- **Filters**: Show only specific products or date ranges
- **Grouping**: Group dates by month/quarter/year
- **Calculated Fields**: Create new calculations within the pivot
- **Slicers**: Visual filters for easy data exploration

**Pro Tip:** Right-click on values â†’ "Show Values As" â†’ "% of Column Total" to see percentages instead of raw numbers.
                """,
                "key_points": ["Pivot tables summarize data without formulas", "Drag and drop fields to analyze different angles", "Use filters and slicers for interactive exploration"]
            },
            {
                "title": "Data Cleaning Techniques",
                "content": """
**Common Data Problems and Solutions:**

**1. Removing Duplicates**
- Excel: Data â†’ Remove Duplicates
- Sheets: Data â†’ Data Cleanup â†’ Remove Duplicates
- Formula: `=UNIQUE(range)` to create a clean list

**2. Handling Blank Cells**
```
=IF(A1="", "Missing", A1)
```
Or use Find & Replace: Find blank cells â†’ Replace with "N/A"

**3. Text Cleaning**
```
=TRIM(A1)           -- Remove extra spaces
=UPPER(A1)          -- Convert to uppercase
=PROPER(A1)         -- Capitalize first letters
=CLEAN(A1)          -- Remove non-printable characters
```

**4. Splitting Text**
```
=LEFT(A1, 5)        -- First 5 characters
=RIGHT(A1, 3)       -- Last 3 characters
=MID(A1, 2, 4)      -- 4 characters starting at position 2
=TEXTSPLIT(A1, ",") -- Split by delimiter (Excel 365)
```
Or: Data â†’ Text to Columns

**5. Date Standardization**
```
=DATEVALUE("2024-01-15")  -- Convert text to date
=TEXT(A1, "YYYY-MM-DD")   -- Format date consistently
```

**Real-World Scenario:**
You receive customer data with:
- Names in all caps: "JOHN SMITH"
- Extra spaces: "  Chicago  "
- Inconsistent dates: "01/15/24", "2024-01-15", "Jan 15, 2024"

**Cleaning Process:**
```
=PROPER(TRIM(A2))           -- Clean names
=TRIM(B2)                   -- Clean cities
=DATEVALUE(C2)              -- Standardize dates
```
                """,
                "key_points": ["Remove duplicates before analysis", "TRIM removes extra spaces", "Use Text to Columns to split data"]
            },
            {
                "title": "Automation with Macros and Scripts",
                "content": """
**Automating Repetitive Tasks**

**Excel: Recording Macros**
1. View â†’ Macros â†’ Record Macro
2. Give it a name and shortcut key
3. Perform your actions
4. Stop recording
5. Run the macro anytime with your shortcut

**Example Macro Tasks:**
- Format all reports the same way
- Copy data from one sheet to another
- Apply consistent styling to tables

**Google Sheets: Apps Script**
Google Sheets uses JavaScript-based Apps Script for automation.

**Example Script (Auto-email report):**
```javascript
function sendWeeklyReport() {
  var sheet = SpreadsheetApp.getActiveSheet();
  var data = sheet.getRange("A1:D10").getValues();
  
  MailApp.sendEmail({
    to: "manager@company.com",
    subject: "Weekly Sales Report",
    body: "See attached data..."
  });
}
```

**Common Automation Use Cases:**
1. **Data Import**: Pull data from external sources automatically
2. **Report Generation**: Create formatted reports with one click
3. **Email Alerts**: Send notifications when thresholds are met
4. **Data Validation**: Automatically check for errors in new entries

**Power Query (Excel):**
- Connect to databases, websites, or other files
- Transform data with a visual interface
- Refresh data automatically

**Pro Tip:** Start with recording macros to learn the basics, then graduate to VBA (Excel) or Apps Script (Sheets) for more complex automation.
                """,
                "key_points": ["Macros record repetitive actions", "Google Sheets uses Apps Script for automation", "Power Query connects to external data sources"]
            }
        ],
        "exercises": [
            {
                "title": "Calculate Regional Sales",
                "type": "practical",
                "question": "You have sales data with columns: Region (A), Product (B), Amount (C). Write a formula to sum all sales for the 'East' region.",
                "answer": "=SUMIF(A:A, \"East\", C:C) - This formula checks column A for 'East' and sums the corresponding values in column C.",
                "hint": "Use SUMIF with the region column as the range and 'East' as the criteria"
            },
            {
                "title": "Create a Pivot Table Summary",
                "type": "scenario",
                "question": "Your manager wants a report showing total sales by month for each product category. Describe how you would set up a pivot table to show this.",
                "answer": "1. Select all data including headers. 2. Insert Pivot Table. 3. Drag 'Product Category' to Rows. 4. Drag 'Date' to Columns and group by Month. 5. Drag 'Sales Amount' to Values (set to SUM). This creates a matrix with products as rows, months as columns, and sales totals in each cell.",
                "hint": "Think about what goes in rows vs columns, and what value you're summarizing"
            },
            {
                "title": "Clean Messy Data",
                "type": "practical",
                "question": "Cell A1 contains '  JOHN DOE  ' (with extra spaces and all caps). Write a formula to clean this to 'John Doe'.",
                "answer": "=PROPER(TRIM(A1)) - TRIM removes the extra spaces, then PROPER capitalizes only the first letter of each word, resulting in 'John Doe'.",
                "hint": "Combine TRIM to remove spaces and PROPER for proper capitalization"
            }
        ],
        "quiz": [
            {
                "question": "Which formula finds a value in the first column and returns a value from another column?",
                "options": ["SUMIF", "VLOOKUP", "COUNTIF", "TRIM"],
                "correct": 1,
                "explanation": "VLOOKUP (Vertical Lookup) searches the first column of a range for a value and returns a value from a specified column."
            },
            {
                "question": "What does =SUMIF(A:A, \"North\", B:B) do?",
                "options": ["Counts cells containing 'North'", "Sums column B where column A equals 'North'", "Finds 'North' in column A", "Averages column B"],
                "correct": 1,
                "explanation": "SUMIF adds up values in the sum_range (B:B) where the corresponding cells in the criteria_range (A:A) match the criteria ('North')."
            },
            {
                "question": "Which feature summarizes large datasets without formulas?",
                "options": ["VLOOKUP", "Conditional Formatting", "Pivot Table", "Data Validation"],
                "correct": 2,
                "explanation": "Pivot Tables allow you to summarize, analyze, and explore large datasets by dragging and dropping fields - no formulas required."
            },
            {
                "question": "=PROPER(TRIM(\"  HELLO WORLD  \")) returns:",
                "options": ["HELLO WORLD", "hello world", "Hello World", "  Hello World  "],
                "correct": 2,
                "explanation": "TRIM removes extra spaces, then PROPER capitalizes the first letter of each word, resulting in 'Hello World'."
            }
        ]
    },
    "Tool: Python Programming": {
        "course": "Programming Fundamentals",
        "tool_type": "programming",
        "description": "Learn Python for data analysis - pandas for data manipulation, data cleaning scripts, and automated analysis pipelines.",
        "lessons": [
            {
                "title": "Introduction to Pandas DataFrames",
                "content": """
**Pandas: The Data Analyst's Best Friend**

Pandas is a Python library that makes working with data easy and intuitive.

**Creating a DataFrame:**
```python
import pandas as pd

# From a dictionary
data = {
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'Salary': [50000, 60000, 70000]
}
df = pd.DataFrame(data)

# From a CSV file
df = pd.read_csv('sales_data.csv')

# From an Excel file
df = pd.read_excel('report.xlsx')
```

**Exploring Your Data:**
```python
df.head()           # First 5 rows
df.tail()           # Last 5 rows
df.info()           # Column types and non-null counts
df.describe()       # Statistical summary
df.shape            # (rows, columns)
df.columns          # List of column names
```

**Selecting Data:**
```python
df['Name']                    # Single column
df[['Name', 'Age']]          # Multiple columns
df.loc[0]                    # Row by label/index
df.iloc[0:5]                 # Rows by position
df[df['Age'] > 30]           # Filter rows by condition
```

**Real-World Example:**
```python
# Load sales data
sales = pd.read_csv('sales.csv')

# Quick exploration
print(f"Total records: {len(sales)}")
print(f"Columns: {sales.columns.tolist()}")
print(f"Date range: {sales['date'].min()} to {sales['date'].max()}")
```
                """,
                "key_points": ["pandas uses DataFrames to organize data in rows and columns", "read_csv() and read_excel() load external data", "Use head(), info(), describe() to explore data quickly"]
            },
            {
                "title": "Data Manipulation with Pandas",
                "content": """
**Common Data Operations:**

**1. Adding and Modifying Columns:**
```python
# New column from calculation
df['Bonus'] = df['Salary'] * 0.10

# New column from condition
df['Senior'] = df['Age'] > 30

# Modify existing column
df['Name'] = df['Name'].str.upper()
```

**2. Aggregations:**
```python
df['Salary'].sum()      # Total
df['Salary'].mean()     # Average
df['Age'].max()         # Maximum
df['Name'].count()      # Count

# Group by and aggregate
df.groupby('Department')['Salary'].mean()
df.groupby('Region').agg({
    'Sales': 'sum',
    'Orders': 'count',
    'Amount': 'mean'
})
```

**3. Sorting:**
```python
df.sort_values('Salary', ascending=False)  # Sort by column
df.sort_values(['Dept', 'Salary'])         # Sort by multiple columns
```

**4. Merging DataFrames:**
```python
# Like VLOOKUP in Excel
merged = pd.merge(orders, customers, on='customer_id', how='left')

# Combine datasets vertically
all_data = pd.concat([jan_data, feb_data, mar_data])
```

**Real-World Scenario:**
```python
# Calculate sales metrics by region
summary = sales.groupby('Region').agg({
    'Amount': ['sum', 'mean', 'count'],
    'Profit': 'sum'
}).round(2)

# Find top 10 customers
top_customers = sales.groupby('Customer')['Amount'].sum().nlargest(10)
```
                """,
                "key_points": ["groupby() aggregates data like pivot tables", "merge() combines DataFrames like VLOOKUP", "Method chaining allows multiple operations in one line"]
            },
            {
                "title": "Data Cleaning in Python",
                "content": """
**Handling Common Data Issues:**

**1. Missing Values:**
```python
# Find missing values
df.isnull().sum()                    # Count per column

# Handle missing values
df.dropna()                          # Remove rows with any missing
df.dropna(subset=['Name'])           # Remove if Name is missing
df.fillna(0)                         # Fill with 0
df['Age'].fillna(df['Age'].mean())   # Fill with mean
```

**2. Duplicates:**
```python
# Find duplicates
df.duplicated().sum()                # Count duplicates
df[df.duplicated()]                  # Show duplicate rows

# Remove duplicates
df.drop_duplicates()                 # Remove exact duplicates
df.drop_duplicates(subset=['Email']) # Remove based on column
```

**3. Data Type Conversion:**
```python
df['Date'] = pd.to_datetime(df['Date'])        # Convert to datetime
df['Price'] = df['Price'].astype(float)        # Convert to float
df['ID'] = df['ID'].astype(str)                # Convert to string
```

**4. Text Cleaning:**
```python
df['Name'] = df['Name'].str.strip()            # Remove whitespace
df['Name'] = df['Name'].str.lower()            # Lowercase
df['Name'] = df['Name'].str.title()            # Title Case
df['Code'] = df['Code'].str.replace('-', '')   # Remove characters
```

**5. Outlier Detection:**
```python
# Using Z-score
from scipy import stats
df['z_score'] = stats.zscore(df['Value'])
outliers = df[abs(df['z_score']) > 3]

# Using IQR
Q1 = df['Value'].quantile(0.25)
Q3 = df['Value'].quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df['Value'] < Q1 - 1.5*IQR) | (df['Value'] > Q3 + 1.5*IQR)]
```
                """,
                "key_points": ["isnull().sum() shows missing value counts", "drop_duplicates() removes duplicate rows", "to_datetime() converts text to proper dates"]
            },
            {
                "title": "Analysis Scripts and Automation",
                "content": """
**Creating Reusable Analysis Scripts:**

**Basic Analysis Template:**
```python
import pandas as pd

def analyze_sales(filepath):
    # Load data
    df = pd.read_csv(filepath)
    
    # Clean data
    df = df.dropna(subset=['Amount'])
    df['Date'] = pd.to_datetime(df['Date'])
    
    # Calculate metrics
    results = {
        'total_sales': df['Amount'].sum(),
        'avg_order': df['Amount'].mean(),
        'num_orders': len(df),
        'top_product': df.groupby('Product')['Amount'].sum().idxmax()
    }
    
    return results

# Run analysis
metrics = analyze_sales('sales_2024.csv')
print(f"Total Sales: ${metrics['total_sales']:,.2f}")
```

**Scheduling Automated Reports:**
```python
import schedule
import time

def daily_report():
    data = pd.read_csv('daily_data.csv')
    summary = data.groupby('Category').sum()
    summary.to_csv(f'report_{date.today()}.csv')
    print("Report generated!")

# Run every day at 9 AM
schedule.every().day.at("09:00").do(daily_report)

while True:
    schedule.run_pending()
    time.sleep(60)
```

**Exporting Results:**
```python
# To CSV
df.to_csv('output.csv', index=False)

# To Excel with formatting
with pd.ExcelWriter('report.xlsx') as writer:
    summary.to_excel(writer, sheet_name='Summary')
    details.to_excel(writer, sheet_name='Details')

# To JSON
df.to_json('data.json', orient='records')
```
                """,
                "key_points": ["Functions make analysis reusable", "schedule library automates recurring tasks", "Export to CSV, Excel, or JSON formats"]
            }
        ],
        "exercises": [
            {
                "title": "Calculate Sales Summary",
                "type": "practical",
                "question": "Write Python code to load 'sales.csv' and calculate the total sales amount and number of unique customers.",
                "answer": "```python\nimport pandas as pd\ndf = pd.read_csv('sales.csv')\ntotal_sales = df['Amount'].sum()\nunique_customers = df['CustomerID'].nunique()\nprint(f'Total: ${total_sales:,.2f}')\nprint(f'Unique Customers: {unique_customers}')\n```\nThe .sum() method adds all values, and .nunique() counts unique values.",
                "hint": "Use pd.read_csv to load, .sum() for total, and .nunique() for unique count"
            },
            {
                "title": "Group By Analysis",
                "type": "practical",
                "question": "Write code to find the average order amount by product category from a DataFrame called 'orders'.",
                "answer": "```python\navg_by_category = orders.groupby('Category')['Amount'].mean()\nprint(avg_by_category.sort_values(ascending=False))\n```\nThis groups orders by Category, then calculates the mean Amount for each group.",
                "hint": "Use groupby('Category') followed by .mean() on the Amount column"
            },
            {
                "title": "Clean Missing Data",
                "type": "scenario",
                "question": "Your DataFrame 'customers' has missing values in the 'Email' column and duplicate rows. Describe the steps to clean this data.",
                "answer": "```python\n# Step 1: Check missing values\nprint(customers['Email'].isnull().sum())\n\n# Step 2: Remove rows with missing emails\ncustomers = customers.dropna(subset=['Email'])\n\n# Step 3: Remove duplicate rows\ncustomers = customers.drop_duplicates()\n\n# Step 4: Verify cleaning\nprint(f'Remaining rows: {len(customers)}')\n```",
                "hint": "Use dropna() for missing values and drop_duplicates() for duplicates"
            }
        ],
        "quiz": [
            {
                "question": "Which pandas function loads a CSV file?",
                "options": ["pd.load_csv()", "pd.read_csv()", "pd.import_csv()", "pd.open_csv()"],
                "correct": 1,
                "explanation": "pd.read_csv() is the standard function to load CSV files into a pandas DataFrame."
            },
            {
                "question": "df.groupby('Region')['Sales'].sum() does what?",
                "options": ["Counts rows by region", "Averages sales by region", "Sums sales by region", "Lists all regions"],
                "correct": 2,
                "explanation": "This groups data by Region, then sums the Sales column for each region - like a pivot table."
            },
            {
                "question": "How do you check for missing values in a DataFrame?",
                "options": ["df.missing()", "df.isnull().sum()", "df.check_null()", "df.find_empty()"],
                "correct": 1,
                "explanation": "df.isnull().sum() returns the count of missing (null) values for each column."
            },
            {
                "question": "What does df.drop_duplicates() do?",
                "options": ["Removes columns", "Removes null values", "Removes duplicate rows", "Removes the first row"],
                "correct": 2,
                "explanation": "drop_duplicates() removes rows that are exact copies of other rows in the DataFrame."
            }
        ]
    },
    "Tool: SQL & Databases": {
        "course": "Databases and Cloud Services",
        "tool_type": "database",
        "description": "Master SQL for querying databases - learn SELECT statements, JOINs, aggregations, and data extraction from relational databases.",
        "lessons": [
            {
                "title": "SQL Basics: SELECT Queries",
                "content": """
**SQL (Structured Query Language)** is the standard language for working with databases.

**Basic SELECT Statement:**
```sql
SELECT column1, column2
FROM table_name
WHERE condition
ORDER BY column1;
```

**Examples:**

**1. Select All Data:**
```sql
SELECT * FROM customers;
```

**2. Select Specific Columns:**
```sql
SELECT first_name, last_name, email
FROM customers;
```

**3. Filter with WHERE:**
```sql
SELECT * FROM orders
WHERE order_date > '2024-01-01';

SELECT * FROM products
WHERE price BETWEEN 10 AND 50;

SELECT * FROM customers
WHERE city IN ('New York', 'Los Angeles', 'Chicago');
```

**4. Sort Results:**
```sql
SELECT * FROM sales
ORDER BY amount DESC;  -- Highest first

SELECT * FROM employees
ORDER BY department, hire_date;
```

**5. Limit Results:**
```sql
SELECT * FROM products
ORDER BY sales DESC
LIMIT 10;  -- Top 10 best sellers
```

**Common WHERE Operators:**
| Operator | Example | Description |
|----------|---------|-------------|
| = | WHERE city = 'Oslo' | Exact match |
| > < >= <= | WHERE price > 100 | Comparisons |
| BETWEEN | WHERE age BETWEEN 20 AND 30 | Range |
| LIKE | WHERE name LIKE 'J%' | Pattern match |
| IN | WHERE status IN ('Active', 'Pending') | Multiple values |
| IS NULL | WHERE email IS NULL | Missing values |
                """,
                "key_points": ["SELECT retrieves data from tables", "WHERE filters rows based on conditions", "ORDER BY sorts results, LIMIT restricts row count"]
            },
            {
                "title": "JOINs: Combining Tables",
                "content": """
**JOINs connect data from multiple tables** based on related columns.

**Types of JOINs:**

**1. INNER JOIN (Most Common)**
Returns only matching rows from both tables.
```sql
SELECT orders.order_id, customers.name, orders.amount
FROM orders
INNER JOIN customers ON orders.customer_id = customers.id;
```

**2. LEFT JOIN**
Returns all rows from the left table, matched rows from the right.
```sql
SELECT customers.name, orders.order_id
FROM customers
LEFT JOIN orders ON customers.id = orders.customer_id;
-- Shows all customers, even those without orders (NULL for orders)
```

**3. RIGHT JOIN**
Returns all rows from the right table, matched rows from the left.
```sql
SELECT customers.name, orders.order_id
FROM customers
RIGHT JOIN orders ON customers.id = orders.customer_id;
```

**Visual Representation:**
```
INNER JOIN:     [A âˆ© B]     - Only matching records
LEFT JOIN:      [A + (Aâˆ©B)] - All from left + matches
RIGHT JOIN:     [(Aâˆ©B) + B] - All from right + matches
FULL OUTER JOIN:[A + B]     - All records from both
```

**Real-World Example:**
You have three tables: `orders`, `customers`, `products`

```sql
SELECT 
    o.order_id,
    c.customer_name,
    p.product_name,
    o.quantity,
    o.order_date
FROM orders o
INNER JOIN customers c ON o.customer_id = c.id
INNER JOIN products p ON o.product_id = p.id
WHERE o.order_date >= '2024-01-01'
ORDER BY o.order_date DESC;
```
                """,
                "key_points": ["INNER JOIN returns only matching rows", "LEFT JOIN keeps all rows from the first table", "Use aliases (o, c, p) to simplify queries"]
            },
            {
                "title": "Aggregations and GROUP BY",
                "content": """
**Aggregate Functions** calculate values across multiple rows.

**Common Aggregate Functions:**
```sql
COUNT(*)        -- Number of rows
COUNT(column)   -- Non-null values in column
SUM(column)     -- Total
AVG(column)     -- Average
MIN(column)     -- Minimum value
MAX(column)     -- Maximum value
```

**GROUP BY: Aggregate by Category**
```sql
SELECT category, SUM(amount) as total_sales
FROM sales
GROUP BY category;
```

**Example Results:**
| category | total_sales |
|----------|-------------|
| Electronics | 150000 |
| Clothing | 85000 |
| Home | 62000 |

**HAVING: Filter Grouped Results**
```sql
SELECT customer_id, COUNT(*) as order_count
FROM orders
GROUP BY customer_id
HAVING COUNT(*) >= 5;  -- Only customers with 5+ orders
```

**Difference: WHERE vs HAVING**
- WHERE filters rows BEFORE grouping
- HAVING filters groups AFTER aggregation

**Complex Example:**
```sql
SELECT 
    region,
    COUNT(*) as num_orders,
    SUM(amount) as total_revenue,
    AVG(amount) as avg_order,
    MAX(amount) as largest_order
FROM orders
WHERE order_date >= '2024-01-01'
GROUP BY region
HAVING SUM(amount) > 10000
ORDER BY total_revenue DESC;
```
                """,
                "key_points": ["SUM, COUNT, AVG, MIN, MAX are aggregate functions", "GROUP BY creates groups for aggregation", "HAVING filters after grouping, WHERE filters before"]
            },
            {
                "title": "Subqueries and Advanced Techniques",
                "content": """
**Subqueries: Queries Within Queries**

**1. Subquery in WHERE:**
```sql
-- Find customers who spent above average
SELECT customer_name, total_spent
FROM customers
WHERE total_spent > (SELECT AVG(total_spent) FROM customers);
```

**2. Subquery in FROM:**
```sql
-- Calculate metrics on aggregated data
SELECT region, avg_order
FROM (
    SELECT region, AVG(amount) as avg_order
    FROM orders
    GROUP BY region
) as region_summary
WHERE avg_order > 100;
```

**Common Table Expressions (CTEs):**
```sql
WITH monthly_sales AS (
    SELECT 
        DATE_TRUNC('month', order_date) as month,
        SUM(amount) as revenue
    FROM orders
    GROUP BY DATE_TRUNC('month', order_date)
)
SELECT month, revenue,
       revenue - LAG(revenue) OVER (ORDER BY month) as change
FROM monthly_sales;
```

**Window Functions:**
```sql
-- Rank salespeople by performance
SELECT 
    salesperson,
    total_sales,
    RANK() OVER (ORDER BY total_sales DESC) as rank
FROM sales_summary;

-- Running total
SELECT 
    order_date,
    amount,
    SUM(amount) OVER (ORDER BY order_date) as running_total
FROM orders;
```

**CASE Statements:**
```sql
SELECT 
    product_name,
    price,
    CASE 
        WHEN price < 10 THEN 'Budget'
        WHEN price < 50 THEN 'Mid-range'
        ELSE 'Premium'
    END as category
FROM products;
```
                """,
                "key_points": ["Subqueries can be used in WHERE, FROM, or SELECT", "CTEs make complex queries more readable", "Window functions calculate across rows without grouping"]
            }
        ],
        "exercises": [
            {
                "title": "Basic Query",
                "type": "practical",
                "question": "Write a SQL query to find all orders over $500 from 2024, sorted by amount (highest first).",
                "answer": "```sql\nSELECT *\nFROM orders\nWHERE amount > 500\n  AND order_date >= '2024-01-01'\nORDER BY amount DESC;\n```\nThis filters orders by amount AND date, then sorts descending.",
                "hint": "Use WHERE with two conditions (AND), ORDER BY with DESC"
            },
            {
                "title": "JOIN Two Tables",
                "type": "practical",
                "question": "Write a query to show order_id, customer_name, and amount by joining 'orders' and 'customers' tables on customer_id.",
                "answer": "```sql\nSELECT o.order_id, c.customer_name, o.amount\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.id;\n```\nINNER JOIN connects the tables where customer_id matches.",
                "hint": "Use INNER JOIN with ON clause to specify the matching columns"
            },
            {
                "title": "Aggregation Challenge",
                "type": "practical",
                "question": "Write a query to find total sales by product category, but only show categories with total sales over $10,000.",
                "answer": "```sql\nSELECT category, SUM(amount) as total_sales\nFROM sales\nGROUP BY category\nHAVING SUM(amount) > 10000\nORDER BY total_sales DESC;\n```\nGROUP BY aggregates by category, HAVING filters the grouped results.",
                "hint": "Use GROUP BY for categories, HAVING (not WHERE) to filter after aggregation"
            }
        ],
        "quiz": [
            {
                "question": "Which SQL clause filters rows BEFORE grouping?",
                "options": ["HAVING", "WHERE", "GROUP BY", "ORDER BY"],
                "correct": 1,
                "explanation": "WHERE filters individual rows before any grouping. HAVING filters after grouping."
            },
            {
                "question": "What does LEFT JOIN do?",
                "options": ["Returns only matching rows", "Returns all rows from the left table plus matches", "Returns all rows from both tables", "Joins tables side by side"],
                "correct": 1,
                "explanation": "LEFT JOIN returns all rows from the left (first) table, and matching rows from the right table. Non-matches show NULL."
            },
            {
                "question": "SELECT COUNT(*) FROM orders returns:",
                "options": ["All order details", "Number of columns", "Total order amount", "Number of rows"],
                "correct": 3,
                "explanation": "COUNT(*) counts all rows in the table. COUNT(column) would count non-null values in that column."
            },
            {
                "question": "To show only top 5 results, use:",
                "options": ["TOP 5", "FIRST 5", "LIMIT 5", "MAX 5"],
                "correct": 2,
                "explanation": "LIMIT 5 restricts the result to 5 rows. (Note: TOP is used in SQL Server, LIMIT in MySQL/PostgreSQL)"
            }
        ]
    },
    "Tool: Tableau & Power BI": {
        "course": "Data Visualisation",
        "tool_type": "visualization",
        "description": "Learn to create professional dashboards and data visualizations using Business Intelligence tools like Tableau and Power BI.",
        "lessons": [
            {
                "title": "Dashboard Design Principles",
                "content": """
**Creating Effective Dashboards**

**The 5-Second Rule:**
Users should understand the main message within 5 seconds of viewing your dashboard.

**Key Design Principles:**

**1. Hierarchy: Most Important First**
- Place key metrics at the top-left (where eyes naturally start)
- Use size to indicate importance
- Group related information together

**2. Layout Patterns:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     KPI Cards (Top Metrics)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Main Chart   â”‚  Supporting     â”‚
â”‚  (Trend/Map)  â”‚  Charts         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Detail Table / Filters      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3. Color Best Practices:**
- Use a consistent color palette (3-5 colors max)
- Reserve red for negative/alerts
- Reserve green for positive/good
- Use gray for context/secondary info
- Ensure color-blind accessibility

**4. White Space:**
- Don't overcrowd - let visualizations breathe
- Group related elements with subtle borders or backgrounds
- Consistent margins and padding

**5. Interactivity:**
- Add filters for user exploration
- Use drill-down for details on demand
- Cross-filter charts when one is clicked
- Include tooltips for additional context

**Common Mistakes to Avoid:**
- Too many charts (aim for 4-6 per dashboard)
- 3D charts (distort perception)
- Pie charts with many slices (use bar charts instead)
- Inconsistent formatting across charts
                """,
                "key_points": ["Key metrics go top-left", "Limit to 3-5 colors", "Less is more - avoid clutter"]
            },
            {
                "title": "Choosing the Right Visualization",
                "content": """
**Match Your Chart to Your Data Story**

**1. Comparisons:**
| Chart Type | Best For |
|------------|----------|
| Bar Chart | Comparing categories |
| Column Chart | Comparing over time (few periods) |
| Bullet Chart | Comparing to a target |
| Lollipop Chart | Many categories, cleaner than bars |

**2. Trends Over Time:**
| Chart Type | Best For |
|------------|----------|
| Line Chart | Continuous time series |
| Area Chart | Showing volume over time |
| Sparklines | Compact trends in tables |

**3. Parts of a Whole:**
| Chart Type | Best For |
|------------|----------|
| Stacked Bar | Composition with few categories |
| Treemap | Hierarchical part-to-whole |
| Donut Chart | Simple 2-3 part breakdowns |

**4. Relationships:**
| Chart Type | Best For |
|------------|----------|
| Scatter Plot | Correlation between variables |
| Bubble Chart | Three variables (x, y, size) |
| Heat Map | Patterns across two dimensions |

**5. Geographic:**
| Chart Type | Best For |
|------------|----------|
| Choropleth Map | Values by region |
| Symbol Map | Point locations with values |

**Decision Framework:**
1. What question are you answering?
2. How many variables?
3. What type of data? (categories, time, geography)
4. Who is the audience?

**Example:**
"How have sales changed by region over the past year?"
- Time + Categories = Line chart with one line per region
- Or: Small multiples (one chart per region)
                """,
                "key_points": ["Bar charts for comparisons, line charts for trends", "Avoid pie charts with many slices", "Consider your audience when choosing complexity"]
            },
            {
                "title": "Building in Tableau",
                "content": """
**Tableau Workflow:**

**1. Connect to Data:**
- File â†’ New â†’ Connect to data source
- Supports: Excel, CSV, databases, cloud services
- Use "Data Interpreter" to clean messy files

**2. Understand the Interface:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Pane â”‚ Cards & Shelves  â”‚ Worksheet â”‚
â”‚ (Fields)  â”‚ (Rows, Columns,  â”‚ (Canvas)  â”‚
â”‚           â”‚  Filters, Marks) â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3. Building Visualizations:**
- Drag DIMENSIONS (categorical) to Rows/Columns
- Drag MEASURES (numerical) to create charts
- Tableau automatically suggests chart types

**4. Key Actions:**
```
Show Me Panel     â†’ Quick chart suggestions
Marks Card        â†’ Color, Size, Label, Tooltip
Filters Shelf     â†’ Filter data
Pages Shelf       â†’ Animation through data
```

**5. Calculated Fields:**
```
// Example: Profit Margin
[Profit] / [Sales]

// Example: Sales Category
IF [Sales] > 10000 THEN "High"
ELSEIF [Sales] > 5000 THEN "Medium"
ELSE "Low"
END
```

**6. Creating Dashboards:**
- New â†’ Dashboard
- Drag worksheets onto the canvas
- Add filters, text boxes, images
- Set up actions (filter, highlight, URL)

**Pro Tips:**
- Use Ctrl+click to multi-select fields
- Right-click for format options
- Save as .twbx to include data with workbook
                """,
                "key_points": ["Dimensions are categories, Measures are numbers", "Drag fields to Rows/Columns to build charts", "Dashboards combine multiple worksheets"]
            },
            {
                "title": "Building in Power BI",
                "content": """
**Power BI Workflow:**

**1. Get Data:**
- Home â†’ Get Data â†’ Choose source
- Transform data in Power Query Editor
- Load to data model

**2. Interface Overview:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Report View â”‚ Data View â”‚ Model View       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Fields â”‚ Visualizations â”‚ Report Canvas    â”‚
â”‚ Pane   â”‚ Pane           â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3. Creating Visuals:**
1. Click visual type in Visualizations pane
2. Drag fields to Values, Axis, Legend
3. Format in Format pane (paint roller icon)

**4. DAX Formulas:**
```
// Total Sales
Total Sales = SUM(Sales[Amount])

// Year-over-Year Growth
YoY Growth = 
DIVIDE(
    [Total Sales] - CALCULATE([Total Sales], SAMEPERIODLASTYEAR('Date'[Date])),
    CALCULATE([Total Sales], SAMEPERIODLASTYEAR('Date'[Date]))
)

// Running Total
Running Total = 
CALCULATE(
    [Total Sales],
    FILTER(ALL('Date'), 'Date'[Date] <= MAX('Date'[Date]))
)
```

**5. Key Features:**
- **Slicers**: Interactive filters
- **Drill Through**: Right-click to see details
- **Bookmarks**: Save views/states
- **Q&A**: Natural language queries

**6. Publishing:**
- Publish â†’ Select workspace
- Share dashboards with colleagues
- Schedule data refresh
- Set up alerts on metrics

**Tableau vs Power BI:**
| Feature | Tableau | Power BI |
|---------|---------|----------|
| Strengths | Visualization | Microsoft integration |
| Learning | Steeper curve | Easier for Excel users |
| Cost | Higher | Lower (included in Microsoft 365) |
                """,
                "key_points": ["Power BI uses DAX for calculations", "Slicers are interactive dashboard filters", "Publish to Power BI Service to share online"]
            }
        ],
        "exercises": [
            {
                "title": "Choose the Right Chart",
                "type": "scenario",
                "question": "You need to show how market share has changed over 5 years for 4 companies. What chart type would you use and why?",
                "answer": "A stacked area chart or 100% stacked area chart would work best. It shows: 1) Time progression (x-axis), 2) Each company's share (different colors), 3) How shares changed relative to each other. Alternative: Line chart if you want to emphasize individual company trends.",
                "hint": "You're showing parts of a whole (market share) over time"
            },
            {
                "title": "Dashboard Layout",
                "type": "scenario",
                "question": "Design a sales dashboard layout for a manager who needs to see: Total Revenue, Monthly Trend, Sales by Region, and Top Products. Describe where each element should go.",
                "answer": "Layout: 1) TOP: KPI cards for Total Revenue, Orders, Avg Order Value (most important, immediate visibility). 2) MIDDLE-LEFT: Line chart showing Monthly Sales Trend (main story, largest space). 3) MIDDLE-RIGHT: Map or bar chart for Sales by Region. 4) BOTTOM: Table or horizontal bar chart for Top 10 Products with details. Add date filter at top-right.",
                "hint": "Think about visual hierarchy - what's most important goes where the eye looks first"
            },
            {
                "title": "Create a DAX Measure",
                "type": "practical",
                "question": "Write a DAX formula to calculate the percentage of sales that came from the 'Online' channel (where Sales[Channel] = 'Online').",
                "answer": "```\nOnline % = \nDIVIDE(\n    CALCULATE(SUM(Sales[Amount]), Sales[Channel] = \"Online\"),\n    SUM(Sales[Amount])\n)\n```\nThis calculates Online sales divided by Total sales. CALCULATE applies the filter for Online channel.",
                "hint": "Use CALCULATE to filter for Online, then DIVIDE by total"
            }
        ],
        "quiz": [
            {
                "question": "What's the best chart for showing sales trends over 12 months?",
                "options": ["Pie chart", "Bar chart", "Line chart", "Scatter plot"],
                "correct": 2,
                "explanation": "Line charts are ideal for showing continuous trends over time. They clearly show patterns, increases, and decreases."
            },
            {
                "question": "In dashboard design, key metrics should be placed:",
                "options": ["At the bottom", "In the center", "At the top-left", "Hidden behind filters"],
                "correct": 2,
                "explanation": "Eyes naturally start at the top-left, so key metrics should be placed there for immediate visibility."
            },
            {
                "question": "In Tableau, what are DIMENSIONS?",
                "options": ["Numerical values for calculations", "Categorical fields for grouping", "Chart types", "Color settings"],
                "correct": 1,
                "explanation": "Dimensions are categorical fields (like Region, Product Name) used to group and categorize data. Measures are numerical values."
            },
            {
                "question": "In Power BI, DAX is used for:",
                "options": ["Connecting to databases", "Creating calculated measures and columns", "Designing visual layouts", "Sharing dashboards"],
                "correct": 1,
                "explanation": "DAX (Data Analysis Expressions) is Power BI's formula language for creating calculated columns, measures, and tables."
            }
        ]
    },
    "Tool: Statistical Analysis": {
        "course": "Statistical Tools",
        "tool_type": "statistics",
        "description": "Learn statistical analysis techniques including hypothesis testing, confidence intervals, and when to use different statistical methods.",
        "lessons": [
            {
                "title": "Descriptive Statistics Fundamentals",
                "content": """
**Summarizing Data with Numbers**

**Measures of Central Tendency:**

**1. Mean (Average)**
```
Mean = Sum of all values / Number of values

Example: 10, 20, 30, 40, 50
Mean = 150 / 5 = 30
```
Use when: Data is symmetric, no extreme outliers

**2. Median (Middle Value)**
```
Sort values, find the middle
Example: 10, 20, 30, 40, 100
Median = 30 (middle value)
```
Use when: Data has outliers (median is resistant to extremes)

**3. Mode (Most Frequent)**
```
Example: 10, 20, 20, 30, 20
Mode = 20 (appears most often)
```
Use when: Working with categorical data

**Measures of Spread:**

**1. Range**
```
Range = Maximum - Minimum
Example: Max=100, Min=10 â†’ Range = 90
```

**2. Variance**
```
Average of squared deviations from mean
ÏƒÂ² = Î£(x - Î¼)Â² / n
```

**3. Standard Deviation**
```
Square root of variance
Ïƒ = âˆš(Variance)

Interpretation:
- 68% of data within Â±1 SD of mean
- 95% of data within Â±2 SD of mean
- 99.7% of data within Â±3 SD of mean
```

**4. Interquartile Range (IQR)**
```
IQR = Q3 - Q1 (75th percentile - 25th percentile)
Used for identifying outliers:
Outlier if value < Q1 - 1.5Ã—IQR or > Q3 + 1.5Ã—IQR
```

**Real-World Application:**
Salary data: [30K, 35K, 40K, 45K, 500K]
- Mean = 130K (misleading due to outlier)
- Median = 40K (more representative)
- Report BOTH with explanation of the outlier
                """,
                "key_points": ["Mean is affected by outliers, median is not", "Standard deviation shows data spread", "Always report both mean and median for skewed data"]
            },
            {
                "title": "Hypothesis Testing Basics",
                "content": """
**Making Data-Driven Decisions**

**What is Hypothesis Testing?**
A method to determine if observed results are statistically significant or due to chance.

**Key Concepts:**

**1. Null Hypothesis (Hâ‚€)**
The "nothing special" assumption
Example: "There is no difference in sales before and after the marketing campaign."

**2. Alternative Hypothesis (Hâ‚)**
What you're trying to prove
Example: "The marketing campaign increased sales."

**3. P-value**
Probability of seeing your results if Hâ‚€ is true
- p < 0.05: Statistically significant (reject Hâ‚€)
- p â‰¥ 0.05: Not significant (fail to reject Hâ‚€)

**4. Significance Level (Î±)**
Usually set at 0.05 (5%)
- Î± = 0.05 means 5% chance of false positive

**Common Tests:**

| Test | When to Use |
|------|-------------|
| T-test | Compare means of 2 groups |
| ANOVA | Compare means of 3+ groups |
| Chi-square | Test categorical relationships |
| Correlation | Test relationship strength |

**Example Workflow:**
1. **Question**: Did the new website design increase conversions?
2. **Hâ‚€**: Conversion rate is the same (no effect)
3. **Hâ‚**: New design has higher conversion rate
4. **Collect Data**: Old design: 3.2%, New design: 4.1%
5. **Run Test**: Calculate p-value
6. **Interpret**: p = 0.02 < 0.05 â†’ Significant!
7. **Conclude**: New design significantly improved conversions

**Caution:**
- Statistical significance â‰  practical importance
- p = 0.04 is not "more significant" than p = 0.03
- Always consider effect size alongside p-value
                """,
                "key_points": ["p < 0.05 typically means statistically significant", "Null hypothesis assumes no effect", "Statistical significance doesn't guarantee practical importance"]
            },
            {
                "title": "Choosing the Right Statistical Test",
                "content": """
**Decision Framework for Statistical Tests**

**Step 1: What Type of Data?**
- Continuous (numbers): Age, Sales, Temperature
- Categorical (groups): Gender, Region, Product Type

**Step 2: How Many Groups/Variables?**

**Comparing Groups:**
```
Comparing 2 groups:
â”œâ”€â”€ Continuous outcome â†’ T-test
â”‚   Example: Compare sales between Region A and B
â”‚
â””â”€â”€ Categorical outcome â†’ Chi-square test
    Example: Compare click rates between 2 designs

Comparing 3+ groups:
â”œâ”€â”€ Continuous outcome â†’ ANOVA
â”‚   Example: Compare satisfaction across 5 departments
â”‚
â””â”€â”€ Categorical outcome â†’ Chi-square test
    Example: Compare preferences across age groups
```

**Analyzing Relationships:**
```
Two continuous variables â†’ Correlation / Regression
Example: Relationship between ad spend and sales

One continuous + categories â†’ T-test / ANOVA
Example: Effect of training (yes/no) on performance

Two categorical variables â†’ Chi-square
Example: Relationship between gender and product preference
```

**Quick Reference Table:**

| Scenario | Test |
|----------|------|
| Compare 2 group means | Independent t-test |
| Compare same group before/after | Paired t-test |
| Compare 3+ group means | One-way ANOVA |
| Predict outcome from variables | Regression |
| Test category relationships | Chi-square |
| Measure relationship strength | Correlation |

**Sample Size Considerations:**
- Small samples (n < 30): Use non-parametric tests
- Large samples: Central Limit Theorem helps
- Rule of thumb: At least 30 per group for t-tests
                """,
                "key_points": ["T-test for 2 groups, ANOVA for 3+", "Chi-square for categorical data", "Always check sample size requirements"]
            },
            {
                "title": "Confidence Intervals and Effect Size",
                "content": """
**Beyond P-values: Practical Significance**

**Confidence Intervals:**
Range where the true value likely falls.

**Interpreting a 95% CI:**
```
Average order value: $45 (95% CI: $42 - $48)

Meaning: We're 95% confident the true average
is between $42 and $48.
```

**Key Insights from CIs:**
- Narrow CI = More precise estimate
- If CI includes 0 (for differences): Not significant
- CIs that don't overlap â†’ Significantly different

**Example:**
```
Group A mean: 50 (95% CI: 45-55)
Group B mean: 60 (95% CI: 56-64)

CIs don't overlap â†’ Significant difference!
```

**Effect Size:**
Measures the magnitude of an effect, independent of sample size.

**Common Effect Size Measures:**

**1. Cohen's d (for mean differences):**
```
d = (Mean1 - Mean2) / Pooled SD

Interpretation:
d = 0.2: Small effect
d = 0.5: Medium effect
d = 0.8: Large effect
```

**2. R-squared (for regression):**
```
RÂ² = Proportion of variance explained

Example: RÂ² = 0.64 means 64% of variation
in Y is explained by X
```

**3. Correlation Coefficient (r):**
```
r = 0.1-0.3: Weak
r = 0.3-0.5: Moderate
r = 0.5-0.7: Strong
r > 0.7: Very strong
```

**Real-World Example:**
"The new training program significantly improved test scores (p < 0.01), with a medium effect size (d = 0.52). Scores increased from 72 (Â±8) to 76 (Â±7)."

This tells the full story: significant, meaningful, and practical!
                """,
                "key_points": ["Confidence intervals show the range of plausible values", "Effect size shows practical importance", "Report both p-value AND effect size"]
            }
        ],
        "exercises": [
            {
                "title": "Choose Correct Test",
                "type": "scenario",
                "question": "You want to compare customer satisfaction scores (1-10) across 4 different store locations. Which statistical test should you use?",
                "answer": "Use One-Way ANOVA. Reasons: 1) You're comparing means (satisfaction scores are continuous), 2) You have more than 2 groups (4 stores), 3) Groups are independent. If ANOVA shows significance, follow up with post-hoc tests to see which specific stores differ.",
                "hint": "You have continuous data (scores) and 3+ independent groups"
            },
            {
                "title": "Interpret Results",
                "type": "practical",
                "question": "A study reports: 'Mean difference = 5 points, p = 0.03, 95% CI: [1.2, 8.8], Cohen's d = 0.45'. What does this tell you?",
                "answer": "This tells us: 1) The difference (5 points) is statistically significant (p < 0.05). 2) The true difference is likely between 1.2 and 8.8 points (CI doesn't include 0). 3) The effect size is 'small to medium' (d = 0.45). 4) The finding is statistically significant but the practical impact is moderate.",
                "hint": "Consider what each metric (p-value, CI, effect size) tells you about the finding"
            },
            {
                "title": "Descriptive Statistics",
                "type": "practical",
                "question": "Calculate the mean and median for this salary data (in thousands): 40, 45, 42, 48, 150, 44, 46. Which measure better represents the typical salary?",
                "answer": "Mean = (40+45+42+48+150+44+46)/7 = 415/7 = 59.3K. Median = 45K (middle value when sorted: 40,42,44,45,46,48,150). The MEDIAN (45K) better represents typical salary because the 150K outlier pulls the mean up significantly. Most employees earn around 45K, not 59K.",
                "hint": "Sort the values to find the median. Consider which measure is affected by the outlier"
            }
        ],
        "quiz": [
            {
                "question": "A p-value of 0.03 means:",
                "options": ["97% chance the result is true", "3% chance of seeing this result if null hypothesis is true", "The effect size is 0.03", "3% of data is significant"],
                "correct": 1,
                "explanation": "P-value is the probability of observing results at least as extreme as yours, assuming the null hypothesis is true. p=0.03 means 3% chance."
            },
            {
                "question": "To compare means across 5 different groups, use:",
                "options": ["T-test", "Chi-square", "ANOVA", "Correlation"],
                "correct": 2,
                "explanation": "ANOVA (Analysis of Variance) is designed to compare means across 3 or more groups. T-test is for 2 groups only."
            },
            {
                "question": "A 95% confidence interval of [2, 8] for a difference means:",
                "options": ["95% of data falls in this range", "We're 95% confident the true difference is between 2 and 8", "The p-value is 0.95", "The effect is 95% significant"],
                "correct": 1,
                "explanation": "A 95% CI means we're 95% confident the true population value falls within this range. Since it doesn't include 0, the difference is significant."
            },
            {
                "question": "Cohen's d = 0.8 indicates:",
                "options": ["Not significant", "Small effect", "Medium effect", "Large effect"],
                "correct": 3,
                "explanation": "Cohen's d of 0.8 or higher is considered a large effect size. Small=0.2, Medium=0.5, Large=0.8."
            }
        ]
    },
    "Four Data Analysis Philosophies": {
        "course": "Data Driven Decision-Making",
        "description": "Master the four fundamental approaches to data analysis: Descriptive, Diagnostic, Predictive, and Prescriptive analytics.",
        "lessons": [
            {
                "title": "Overview: The Analytics Continuum",
                "content": """
**The Four Types of Data Analytics**

Data analysis follows a continuum from understanding the past to shaping the future:

```
DESCRIPTIVE â†’ DIAGNOSTIC â†’ PREDICTIVE â†’ PRESCRIPTIVE
"What happened?" â†’ "Why?" â†’ "What will happen?" â†’ "What should we do?"
```

**Value and Complexity:**
| Type | Question | Complexity | Business Value |
|------|----------|------------|----------------|
| Descriptive | What happened? | Low | Foundation |
| Diagnostic | Why did it happen? | Medium | Understanding |
| Predictive | What will happen? | High | Foresight |
| Prescriptive | What should we do? | Highest | Optimization |

**Real-World Example - Retail Store:**
1. **Descriptive**: "Sales dropped 15% last month"
2. **Diagnostic**: "Sales dropped because competitor opened nearby"
3. **Predictive**: "Sales will drop another 10% next quarter"
4. **Prescriptive**: "Launch loyalty program and price match campaign"

Each level builds on the previous, creating a complete analytical picture.
                """,
                "key_points": ["Four types form a continuum of increasing value", "Each answers a different question", "Most organizations start with descriptive and mature toward prescriptive"]
            },
            {
                "title": "Descriptive Analytics: What Happened?",
                "content": """
**Descriptive Analytics** summarizes historical data to understand what has occurred.

**Key Characteristics:**
- Looks at **past data** only
- Provides **summaries and aggregations**
- Answers: "What happened?" and "How many?"
- Foundation for all other analytics types

**Common Techniques:**
- Calculating averages, totals, percentages
- Creating dashboards and reports
- Trend analysis over time
- Data visualization (charts, graphs)

**Tools Used:**
- Excel/Google Sheets
- Power BI / Tableau
- SQL queries (SELECT, GROUP BY, SUM)

**Business Examples:**

| Industry | Descriptive Question | Output |
|----------|---------------------|--------|
| Retail | "What were total sales last quarter?" | $2.5M revenue |
| Healthcare | "How many patients visited in 2024?" | 45,000 patients |
| Marketing | "What was our email open rate?" | 22% open rate |
| HR | "What is our employee turnover?" | 15% annual turnover |

**Sample Dashboard Metrics:**
- Total Revenue: $2.5M
- Orders: 12,500
- Average Order Value: $200
- Top Product: Widget A (3,000 units)

**Limitations:**
Descriptive analytics tells you WHAT happened but not WHY or what to do about it.
                """,
                "key_points": ["Summarizes historical data", "Foundation of all analytics", "Uses dashboards, reports, and KPIs", "Cannot explain causes or predict future"]
            },
            {
                "title": "Diagnostic Analytics: Why Did It Happen?",
                "content": """
**Diagnostic Analytics** examines data to understand the causes behind outcomes.

**Key Characteristics:**
- Investigates **root causes**
- Uses drill-down and data mining
- Answers: "Why did this happen?"
- Requires deeper analysis skills

**Common Techniques:**
- **Drill-down analysis**: Breaking data into smaller segments
- **Correlation analysis**: Finding relationships between variables
- **Root cause analysis (RCA)**: Systematic problem investigation
- **Data discovery**: Exploring data for unexpected patterns

**The 5 Whys Technique:**
1. Sales dropped 15% â†’ Why?
2. Fewer customers visited â†’ Why?
3. Negative reviews increased â†’ Why?
4. Wait times were too long â†’ Why?
5. Staff shortage due to turnover â†’ **ROOT CAUSE**

**Business Examples:**

| Descriptive Finding | Diagnostic Investigation | Root Cause |
|--------------------|-------------------------|------------|
| Sales dropped 15% | Analyze by region, product, time | New competitor opened |
| Website bounce rate up 20% | Check page load times, device types | Mobile site broken |
| Customer complaints doubled | Categorize complaint types | Shipping delays |
| Employee productivity down | Compare teams, projects, tools | Outdated software |

**Correlation Example:**
"We noticed sales spike when temperature exceeds 30Â°C. Correlation analysis shows r=0.85 between temperature and ice cream sales."

**Tools Used:**
- Excel Pivot Tables (drill-down)
- SQL (JOINs, subqueries)
- Statistical software
- BI tools with drill-through
                """,
                "key_points": ["Investigates root causes", "Uses drill-down and correlation", "5 Whys technique is powerful", "Correlation â‰  Causation"]
            },
            {
                "title": "Predictive Analytics: What Will Happen?",
                "content": """
**Predictive Analytics** uses historical data and statistical models to forecast future outcomes.

**Key Characteristics:**
- Uses **statistical models and machine learning**
- Provides **probabilities**, not certainties
- Answers: "What is likely to happen?"
- Requires historical data patterns

**Common Techniques:**
- **Regression analysis**: Predict numeric outcomes
- **Classification**: Predict categories (yes/no, high/medium/low)
- **Time series forecasting**: Predict trends over time
- **Machine learning models**: More complex predictions

**Business Examples:**

| Industry | Predictive Question | Model Type |
|----------|-------------------|------------|
| Retail | "What will next month's sales be?" | Time series forecast |
| Banking | "Will this customer default on loan?" | Classification (Yes/No) |
| Marketing | "Which customers will churn?" | Churn prediction model |
| Healthcare | "What's the patient readmission risk?" | Risk scoring |

**Example: Sales Forecasting**
```
Historical Data: Jan=100K, Feb=110K, Mar=105K, Apr=115K, May=120K
Model identifies: 3% monthly growth trend
Prediction: June = 124K, July = 128K, August = 132K
Confidence interval: Â±8%
```

**Key Concepts:**
- **Training Data**: Historical data used to build the model
- **Features**: Variables used to make predictions
- **Accuracy**: How often predictions are correct
- **Confidence Interval**: Range of likely outcomes

**Important Limitations:**
- Predictions are probabilistic, not certain
- Models are only as good as historical data
- Unexpected events (black swans) break predictions
- "Past performance doesn't guarantee future results"
                """,
                "key_points": ["Forecasts future using historical patterns", "Provides probabilities not certainties", "Requires quality historical data", "Common: regression, classification, time series"]
            },
            {
                "title": "Prescriptive Analytics: What Should We Do?",
                "content": """
**Prescriptive Analytics** recommends specific actions to achieve desired outcomes.

**Key Characteristics:**
- **Recommends actions**, not just predictions
- Uses optimization and simulation
- Answers: "What is the best course of action?"
- Most advanced and valuable analytics type

**Common Techniques:**
- **Optimization algorithms**: Find the best solution
- **Simulation**: Test scenarios without real-world risk
- **Decision trees**: Map out action paths
- **A/B testing**: Compare action effectiveness
- **What-if analysis**: Explore different scenarios

**Business Examples:**

| Predictive Insight | Prescriptive Recommendation |
|-------------------|----------------------------|
| "30% churn risk for customers" | "Offer 20% discount to high-risk customers; expected to reduce churn by 15% and save $50K" |
| "Demand will spike 40% in December" | "Increase inventory by 35%, hire 10 temporary staff, extend operating hours" |
| "Campaign A outperforms B by 25%" | "Reallocate 80% of budget to Campaign A, expected ROI increase of $120K" |
| "Equipment failure likely in 2 weeks" | "Schedule preventive maintenance on Day 10, estimated cost savings of $25K" |

**Real-World Scenario:**
A delivery company uses prescriptive analytics:

1. **Descriptive**: "We made 10,000 deliveries last week"
2. **Diagnostic**: "Late deliveries were 30% higher in Zone C due to traffic"
3. **Predictive**: "Tomorrow Zone C will have 45% late deliveries"
4. **Prescriptive**: "Route trucks through Highway 5 instead of Main Street. Start Zone C deliveries 1 hour earlier. Deploy 2 additional drivers. **Expected result: Reduce late deliveries to 10%**"

**Tools Used:**
- Optimization software (solver tools)
- Simulation platforms
- Decision support systems
- AI/ML recommendation engines
                """,
                "key_points": ["Recommends specific actions", "Uses optimization and simulation", "Highest value but most complex", "Combines prediction with decision-making"]
            }
        ],
        "exercises": [
            {
                "title": "Classify the Analytics Type",
                "type": "scenario",
                "question": "Your manager says: 'We need to know why customer satisfaction scores dropped from 85% to 72% last quarter.' Which type of analytics is needed?",
                "answer": "This requires DIAGNOSTIC analytics. The question 'why did it drop?' indicates root cause investigation. You would drill down by customer segment, product line, service channel, and time period to identify what changed and caused the decline.",
                "hint": "Which question is being asked: What happened? Why? What will happen? What should we do?"
            },
            {
                "title": "Match Insights to Actions",
                "type": "practical",
                "question": "Your predictive model shows that 200 customers (worth $500K annual revenue) have 80% probability of churning next month. Recommend specific prescriptive actions.",
                "answer": "Prescriptive recommendations: 1) Segment the 200 customers by value - focus on top 50 (80/20 rule), 2) Assign account managers to personally call top 20 customers, 3) Offer tailored retention discounts (15-25% based on customer lifetime value), 4) Send personalized email campaign highlighting new features, 5) Create loyalty rewards for continued subscription. Expected outcome: Reduce churn to 30%, save ~$250K in revenue.",
                "hint": "Think about specific actions with measurable expected outcomes"
            },
            {
                "title": "Complete Analytics Journey",
                "type": "scenario",
                "question": "A coffee shop chain notices sales are declining. Walk through all four analytics types to address this problem.",
                "answer": "DESCRIPTIVE: 'Sales are down 20% over 3 months. Morning sales stable, afternoon sales down 35%.' DIAGNOSTIC: 'New competitor (bubble tea shop) opened nearby targeting afternoon customers. Weather data shows no correlation.' PREDICTIVE: 'If trend continues, afternoon sales will drop another 25% next quarter, totaling $50K lost revenue.' PRESCRIPTIVE: 'Launch afternoon happy hour (buy-one-get-one from 2-5pm), introduce 3 new iced drinks, partner with local businesses for afternoon delivery. Expected to recover 60% of lost afternoon sales within 2 months.'",
                "hint": "Address each of the four questions in sequence: What? Why? What next? What to do?"
            }
        ],
        "quiz": [
            {
                "question": "Which analytics type answers 'What should we do?'",
                "options": ["Descriptive", "Diagnostic", "Predictive", "Prescriptive"],
                "correct": 3,
                "explanation": "Prescriptive analytics recommends specific actions to achieve desired outcomes. It goes beyond prediction to optimization."
            },
            {
                "question": "'Our website had 50,000 visitors last month, with 2% conversion rate' is an example of:",
                "options": ["Descriptive analytics", "Diagnostic analytics", "Predictive analytics", "Prescriptive analytics"],
                "correct": 0,
                "explanation": "This is descriptive analytics - it summarizes what happened (visitors, conversion rate) without explaining why or predicting future."
            },
            {
                "question": "Using drill-down analysis to find why sales dropped in a specific region is:",
                "options": ["Descriptive analytics", "Diagnostic analytics", "Predictive analytics", "Prescriptive analytics"],
                "correct": 1,
                "explanation": "Diagnostic analytics investigates root causes using techniques like drill-down analysis to understand WHY something happened."
            },
            {
                "question": "A model that forecasts '85% probability this customer will churn' is:",
                "options": ["Descriptive analytics", "Diagnostic analytics", "Predictive analytics", "Prescriptive analytics"],
                "correct": 2,
                "explanation": "Predictive analytics uses statistical models to forecast future outcomes with probabilities."
            },
            {
                "question": "Which analytics type is the foundation for all others?",
                "options": ["Descriptive", "Diagnostic", "Predictive", "Prescriptive"],
                "correct": 0,
                "explanation": "Descriptive analytics is the foundation - you must understand what happened before analyzing why, predicting future, or recommending actions."
            }
        ]
    },
    "Data Analysis Lifecycle": {
        "course": "Data Driven Decision-Making",
        "description": "Learn the complete data analysis lifecycle from data collection to actionable insights and continuous improvement.",
        "lessons": [
            {
                "title": "Overview: The Data Lifecycle",
                "content": """
**The Data Analysis Lifecycle**

The data analysis lifecycle is a structured process that transforms raw data into actionable insights.

**The 6 Stages:**
```
1. DEFINE â†’ 2. COLLECT â†’ 3. CLEAN â†’ 4. ANALYZE â†’ 5. INTERPRET â†’ 6. ACT
     â†‘                                                              |
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ITERATE â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Stage Overview:**

| Stage | Question | Output |
|-------|----------|--------|
| 1. Define | What problem are we solving? | Clear objectives, KPIs |
| 2. Collect | Where is the data? | Raw datasets |
| 3. Clean | Is the data reliable? | Clean, validated data |
| 4. Analyze | What patterns exist? | Statistical findings |
| 5. Interpret | What does it mean? | Insights and recommendations |
| 6. Act | What do we do? | Decisions and actions |

**Key Principle: ITERATION**
The lifecycle is not linear - you often need to go back to previous stages:
- Analysis reveals data quality issues â†’ Return to Clean
- Interpretation raises new questions â†’ Return to Define
- Actions create new data â†’ Restart the cycle

**Time Distribution (Typical):**
- Define: 10%
- Collect: 15%
- **Clean: 40-60%** â† Most time-consuming!
- Analyze: 15%
- Interpret: 10%
- Act: 5%
                """,
                "key_points": ["6 stages from Define to Act", "Cleaning takes most time (40-60%)", "Process is iterative, not linear", "Each stage has clear outputs"]
            },
            {
                "title": "Stage 1: Define the Problem",
                "content": """
**Define: Setting Clear Objectives**

The most critical stage - a poorly defined problem leads to useless analysis.

**Key Activities:**
1. **Identify the business problem**
2. **Define success metrics (KPIs)**
3. **Scope the project** (what's in/out)
4. **Identify stakeholders**
5. **Set timeline and resources**

**SMART Goals Framework:**
- **S**pecific: "Reduce customer churn" â†’ "Reduce monthly churn rate"
- **M**easurable: "Reduce from 5% to 3%"
- **A**chievable: Based on benchmarks and resources
- **R**elevant: Aligned with business strategy
- **T**ime-bound: "Within Q2 2025"

**Questions to Ask:**
| Category | Questions |
|----------|-----------|
| Problem | What exactly are we trying to solve? |
| Impact | Why does this matter? What's the cost of not solving it? |
| Success | How will we measure success? What KPIs? |
| Scope | What's included? What's explicitly excluded? |
| Data | What data do we need? Do we have access? |
| Timeline | When are results needed? |

**Example - Well-Defined Problem:**

âŒ **Vague**: "We need to understand our customers better"

âœ… **Well-defined**: "Identify the top 3 factors that predict customer churn for our subscription service, measured by monthly churn rate reduction from 5% to 3%, to be completed by end of Q2 with findings presented to the retention team."

**Output of Define Stage:**
- Problem statement document
- Success metrics and KPIs
- Project scope and timeline
- Data requirements list
                """,
                "key_points": ["Clear problem definition is critical", "Use SMART goals framework", "Define specific KPIs for success", "Document scope and constraints"]
            },
            {
                "title": "Stage 2: Collect Data",
                "content": """
**Collect: Gathering the Right Data**

Finding and acquiring data from various sources to answer your questions.

**Data Source Types:**

| Source Type | Examples | Pros | Cons |
|-------------|----------|------|------|
| **Internal** | CRM, ERP, databases | Trusted, accessible | May be incomplete |
| **External** | Government data, APIs | Broader context | Quality varies |
| **Primary** | Surveys, experiments | Tailored to needs | Time-consuming, costly |
| **Secondary** | Reports, research | Already analyzed | May not fit exactly |

**Common Data Collection Methods:**
1. **Database queries** (SQL)
2. **API connections** (automated data pulls)
3. **Web scraping** (public websites)
4. **Surveys/forms** (Google Forms, SurveyMonkey)
5. **Manual data entry** (observations, logs)
6. **File imports** (CSV, Excel from partners)

**Data Quality Considerations:**
When collecting, evaluate:
- **Completeness**: Are there missing values?
- **Accuracy**: Is the data correct?
- **Timeliness**: How recent is it?
- **Relevance**: Does it answer our questions?
- **Consistency**: Same format across sources?

**Ethical Considerations (GDPR):**
- âœ… Only collect data you need (data minimization)
- âœ… Get consent for personal data
- âœ… Document data sources and permissions
- âœ… Secure sensitive information
- âŒ Don't collect data "just in case"

**Example - Data Collection Plan:**

| Data Needed | Source | Method | Owner | Timeline |
|-------------|--------|--------|-------|----------|
| Customer info | CRM | SQL query | Data team | Day 1-2 |
| Transactions | ERP | API | IT | Day 2-3 |
| Satisfaction | Survey | Google Forms | Marketing | Day 3-7 |
| Industry benchmarks | External | Web research | Analyst | Day 5-7 |
                """,
                "key_points": ["Multiple source types: internal, external, primary, secondary", "Evaluate quality before using", "Follow GDPR and ethical guidelines", "Document sources and permissions"]
            },
            {
                "title": "Stage 3: Clean the Data",
                "content": """
**Clean: Ensuring Data Quality**

The most time-consuming stage - "garbage in, garbage out."

**Common Data Quality Issues:**

| Issue | Example | Solution |
|-------|---------|----------|
| Missing values | Empty cells | Delete, impute, or flag |
| Duplicates | Same record twice | Remove duplicates |
| Inconsistent formats | "USA", "U.S.A.", "United States" | Standardize |
| Typos | "Jonh" instead of "John" | Correct or fuzzy match |
| Outliers | Age = 200 | Investigate, cap, or remove |
| Wrong data types | Numbers stored as text | Convert types |
| Invalid values | Date = "32/13/2025" | Validate and correct |

**Data Cleaning Steps:**
1. **Explore** - Get overview of data structure and values
2. **Validate** - Check for invalid or impossible values
3. **Standardize** - Consistent formats, units, naming
4. **Deduplicate** - Remove repeated records
5. **Handle missing** - Delete, impute, or flag
6. **Handle outliers** - Investigate before removing
7. **Document** - Record all changes made

**Handling Missing Data:**

| Strategy | When to Use |
|----------|-------------|
| **Delete rows** | Few missing values, large dataset |
| **Delete columns** | >50% missing in column |
| **Impute with mean/median** | Numeric data, random missing |
| **Impute with mode** | Categorical data |
| **Flag as "Unknown"** | Missing has meaning |

**Tools for Data Cleaning:**
- Excel: Find/Replace, Remove Duplicates, TRIM(), CLEAN()
- Power Query: Transform steps, replace values
- Python: pandas library (dropna, fillna, replace)
- SQL: UPDATE, CASE WHEN, string functions

**Best Practice: Document Everything!**
```
Data Cleaning Log - Customer Data
- Removed 145 duplicate records (same email)
- Standardized country names (15 variations â†’ 3)
- Imputed 23 missing ages with median (34)
- Flagged 5 outliers (age > 100) for review
- Converted phone numbers to E.164 format
```
                """,
                "key_points": ["Cleaning takes 40-60% of project time", "Common issues: missing, duplicates, inconsistencies", "Document all cleaning decisions", "Never assume data is clean"]
            },
            {
                "title": "Stages 4-6: Analyze, Interpret, Act",
                "content": """
**Stage 4: ANALYZE - Finding Patterns**

Apply statistical and analytical techniques to discover insights.

**Analysis Techniques:**
| Technique | Purpose | Example |
|-----------|---------|---------|
| Descriptive stats | Summarize data | Mean order value = $85 |
| Correlation | Find relationships | Temperature â†” Sales |
| Regression | Predict outcomes | Price impact on demand |
| Segmentation | Group similar items | Customer clusters |
| Trend analysis | Track over time | Monthly growth rate |

**Key Questions:**
- What patterns emerge from the data?
- Are there unexpected findings?
- Do the numbers support or contradict our hypothesis?

---

**Stage 5: INTERPRET - Understanding Meaning**

Translate analytical findings into business insights.

**From Analysis to Insight:**
| Analysis Finding | Business Interpretation |
|-----------------|------------------------|
| "r = 0.85 between ads and sales" | "Ad spending strongly drives sales; $1 spent returns ~$3.40" |
| "Churn highest in month 3" | "Customers decide to stay/leave early; focus onboarding on first 90 days" |
| "Segment A has 3x LTV of Segment B" | "Prioritize acquiring more Segment A customers in marketing" |

**Avoiding Misinterpretation:**
- âŒ Correlation â‰  Causation
- âŒ Cherry-picking data that supports your view
- âŒ Ignoring confidence intervals
- âœ… Consider alternative explanations
- âœ… Validate with domain experts

---

**Stage 6: ACT - Taking Action**

Convert insights into decisions and monitor results.

**Action Planning:**
| Insight | Recommended Action | Owner | Timeline | Success Metric |
|---------|-------------------|-------|----------|----------------|
| 3-month churn spike | Redesign onboarding | Product | Q2 | -30% month-3 churn |
| Segment A high value | Target ads to lookalikes | Marketing | Q2 | +20% Segment A acquisition |

**After Acting:**
1. Monitor KPIs for impact
2. Compare actual vs. expected results
3. Document learnings
4. Start next iteration of the lifecycle
                """,
                "key_points": ["Analyze: Find patterns with statistics", "Interpret: Translate to business meaning", "Act: Make decisions and monitor", "Always iterate based on results"]
            }
        ],
        "exercises": [
            {
                "title": "Identify the Stage",
                "type": "scenario",
                "question": "You discover that 15% of your customer records have the country field as blank, while others have variations like 'USA', 'United States', and 'U.S.' What stage are you in and what should you do?",
                "answer": "You are in the CLEAN stage. Actions: 1) Standardize country variations to one format (e.g., all 'USA'), 2) For blank countries, decide whether to impute based on other fields (phone area code, city) or flag as 'Unknown', 3) Document these decisions in your cleaning log, 4) Consider adding validation rules to prevent this in future data collection.",
                "hint": "This involves data quality issues - which stage handles those?"
            },
            {
                "title": "Write a Problem Statement",
                "type": "practical",
                "question": "A retail manager says: 'Our online sales seem low.' Transform this into a well-defined problem statement with SMART goals.",
                "answer": "SMART Problem Statement: 'Identify the top 3 factors causing low online conversion rates (currently 1.2%, industry benchmark 2.5%) by analyzing website analytics, customer journey data, and competitor benchmarks. Success metric: Actionable recommendations that can increase conversion rate to 2.0% within 6 months. Timeline: Analysis complete within 4 weeks, presented to e-commerce team by [date].'",
                "hint": "Use the SMART framework: Specific, Measurable, Achievable, Relevant, Time-bound"
            },
            {
                "title": "Plan Data Collection",
                "type": "practical",
                "question": "You need to analyze why customer support calls have increased 40% this quarter. Create a data collection plan listing at least 4 data sources.",
                "answer": "Data Collection Plan: 1) INTERNAL - Support ticket system: Extract all tickets from this quarter vs. previous (categories, resolution time, customer info), 2) INTERNAL - CRM: Customer details, product ownership, account history, 3) INTERNAL - Product/Engineering: Recent releases, known bugs, feature changes, 4) PRIMARY - Customer survey: Post-call satisfaction survey with open-ended feedback, 5) EXTERNAL - Social media: Mentions and complaints on Twitter/Facebook for sentiment analysis. Method: SQL queries for internal data, survey tool for primary, social listening tool for external.",
                "hint": "Think about internal vs. external sources and what data would explain increased calls"
            }
        ],
        "quiz": [
            {
                "question": "Which stage typically takes the most time in the data analysis lifecycle?",
                "options": ["Define", "Collect", "Clean", "Analyze"],
                "correct": 2,
                "explanation": "Data cleaning typically takes 40-60% of project time. Real-world data is messy and requires significant effort to prepare for analysis."
            },
            {
                "question": "Which SMART criterion is missing? 'Increase sales next year'",
                "options": ["Specific", "Measurable", "Achievable", "All of the above"],
                "correct": 3,
                "explanation": "This goal is vague. It should be: Specific (which product line?), Measurable (by how much?), Achievable (based on what?), and more Time-bound (Q1? Q4?)."
            },
            {
                "question": "You find that 'correlation between ice cream sales and drowning deaths is 0.95.' The correct interpretation is:",
                "options": ["Ice cream causes drowning", "We should ban ice cream sales", "Both are likely caused by a third factor (summer heat)", "This is statistically impossible"],
                "correct": 2,
                "explanation": "Correlation does not imply causation. Both ice cream sales and drowning increase in summer due to hot weather - a confounding variable."
            },
            {
                "question": "When should you return to an earlier stage in the lifecycle?",
                "options": ["Never - the lifecycle is strictly linear", "Only if the project fails", "Whenever new information suggests it's needed", "Only at management request"],
                "correct": 2,
                "explanation": "The lifecycle is iterative. You might return to Clean if analysis reveals data issues, or to Define if interpretation raises new questions."
            }
        ]
    },
    "Data-Driven Case Studies": {
        "course": "Data Driven Decision-Making",
        "description": "Apply data-driven decision-making through real-world case studies with before-and-after scenarios.",
        "lessons": [
            {
                "title": "Case Study: Retail Inventory Optimization",
                "content": """
**Company: MegaMart Retail Chain**
**Problem: Stockouts and Overstock**

**Before (Data-Free Decision Making):**
- Store managers ordered inventory based on "gut feeling"
- Frequent stockouts of popular items (lost sales)
- Overstock of slow items (tied-up capital, markdowns)
- Annual loss: $2.5M in stockouts + $1.8M in excess inventory

**Data Analysis Approach:**

**1. DEFINE:**
"Optimize inventory levels to reduce stockouts by 50% and excess inventory by 40% within 12 months."

**2. COLLECT:**
- 3 years of sales data by product, store, date
- Supplier lead times
- Promotional calendars
- Weather data
- Local event schedules

**3. ANALYZE:**
| Finding | Insight |
|---------|---------|
| 80% of revenue from 15% of products | Focus on top SKUs first |
| Sales spike 300% during local events | Align inventory with event calendar |
| Rain increases umbrella sales 500% | Weather-based ordering |
| Lead time varies 2-14 days by supplier | Buffer stock for slow suppliers |

**4. PRESCRIPTIVE ACTION:**
- Implemented automated reorder points based on demand forecasting
- Created event-triggered ordering rules
- Integrated weather forecast into daily orders

**After (Data-Driven Results):**
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Stockout rate | 12% | 4% | -67% |
| Excess inventory | $1.8M | $950K | -47% |
| Lost sales | $2.5M | $800K | -68% |
| Inventory turnover | 4x/year | 7x/year | +75% |

**Total Annual Savings: $2.55M**
                """,
                "key_points": ["Data replaced gut-feeling decisions", "Combined multiple data sources", "Automated decision-making with rules", "Measurable ROI from analytics"]
            },
            {
                "title": "Case Study: Healthcare Readmission Reduction",
                "content": """
**Organization: Regional Hospital Network**
**Problem: High 30-Day Readmission Rates**

**Before:**
- 30-day readmission rate: 22% (national target: 15%)
- Medicare penalties: $3.2M annually
- Readmissions cost average $15,000 each
- No systematic way to identify at-risk patients

**Data Analysis Approach:**

**1. DEFINE:**
"Identify patients at high risk of 30-day readmission and create targeted interventions to reduce rate from 22% to 15% within 18 months."

**2. COLLECT:**
- 5 years of patient records (anonymized)
- Diagnosis codes, procedures, medications
- Length of stay, discharge disposition
- Social factors (living alone, transport access)
- Prior admissions history

**3. ANALYZE - Key Predictors Found:**

| Risk Factor | Impact on Readmission |
|-------------|----------------------|
| 3+ admissions in past year | 4.2x higher risk |
| Lives alone, age 75+ | 2.8x higher risk |
| Discharged Friday afternoon | 1.9x higher risk |
| Heart failure + diabetes combo | 2.5x higher risk |
| No follow-up scheduled | 2.1x higher risk |

**4. PREDICTIVE MODEL:**
Created a risk score (0-100) calculated at admission:
- Score 0-30: Low risk (standard care)
- Score 31-60: Medium risk (phone follow-up)
- Score 61-100: High risk (intensive interventions)

**5. PRESCRIPTIVE ACTIONS:**
For high-risk patients:
- Discharge planning starts Day 1
- Social worker consultation
- Schedule follow-up before discharge
- Home health visit within 48 hours
- Medication reconciliation call

**After (Data-Driven Results):**
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| 30-day readmission | 22% | 14.5% | -34% |
| Medicare penalties | $3.2M | $0 | -100% |
| Readmission costs | $8.2M | $5.1M | -38% |
| Patient satisfaction | 72% | 84% | +12pts |

**ROI:** Invested $800K in analytics program, saved $6.3M annually.
                """,
                "key_points": ["Predictive model identified at-risk patients", "Targeted interventions based on risk level", "Combined clinical and social data", "Massive ROI from prevented readmissions"]
            },
            {
                "title": "Case Study: Marketing Campaign Optimization",
                "content": """
**Company: E-Commerce Fashion Retailer**
**Problem: Inefficient Marketing Spend**

**Before:**
- Annual marketing budget: $5M
- Spent equally across all channels
- No attribution modeling
- Overall ROAS (Return on Ad Spend): 2.5x
- Many campaigns ran without performance tracking

**Data Analysis Approach:**

**1. DEFINE:**
"Optimize marketing spend allocation to achieve ROAS of 4.0x while maintaining revenue, using data-driven attribution."

**2. COLLECT:**
- All ad platform data (Google, Meta, TikTok)
- Website analytics (Google Analytics)
- Customer purchase history
- Email campaign performance
- Customer surveys (how did you hear about us?)

**3. ANALYZE - Channel Performance:**

| Channel | Spend | Revenue | ROAS | Customer Type |
|---------|-------|---------|------|---------------|
| Google Search | $1M | $4.5M | 4.5x | High intent |
| Meta Ads | $1.5M | $3M | 2.0x | Discovery |
| TikTok | $800K | $1.2M | 1.5x | Young demo |
| Email | $200K | $2.4M | 12x | Existing customers |
| Display | $1M | $1.5M | 1.5x | Brand awareness |
| Influencer | $500K | $800K | 1.6x | Mixed |

**Key Insights:**
- Email had highest ROAS but was underfunded
- Display ads often credited with conversions actually started by search
- TikTok reached new demographic not accessible elsewhere
- Influencer ROI varied wildly (2 out of 20 drove 80% of results)

**4. PRESCRIPTIVE RECOMMENDATIONS:**

| Channel | Before | After | Rationale |
|---------|--------|-------|-----------|
| Email | $200K | $600K | Highest ROAS, expand segments |
| Google Search | $1M | $1.5M | Proven high-intent performer |
| Meta | $1.5M | $1.2M | Focus on lookalike audiences only |
| Display | $1M | $400K | Reduce, use for retargeting only |
| TikTok | $800K | $800K | Maintain for new customer acquisition |
| Influencer | $500K | $500K | Focus on top 2 performers only |

**After (Data-Driven Results):**
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Total Spend | $5M | $5M | Same |
| Total Revenue | $12.5M | $19.8M | +58% |
| ROAS | 2.5x | 3.96x | +58% |
| New customers | 45K | 52K | +16% |
| Email subscribers | 80K | 145K | +81% |
                """,
                "key_points": ["Attribution modeling revealed true channel value", "Reallocated budget without increasing total spend", "58% revenue increase with same budget", "Focused influencer spend on top performers"]
            }
        ],
        "exercises": [
            {
                "title": "Analyze the Case",
                "type": "scenario",
                "question": "In the MegaMart case, the team discovered that 80% of revenue came from 15% of products. What is this principle called, and how should it influence their analysis priority?",
                "answer": "This is the Pareto Principle (80/20 rule). Influence on analysis: 1) Focus demand forecasting efforts on the top 15% of SKUs first - errors here have the biggest impact, 2) These products should have tighter safety stock and more frequent reorder monitoring, 3) The remaining 85% of products can use simpler forecasting methods, 4) When presenting to stakeholders, lead with insights about top products.",
                "hint": "Think about the famous 80/20 principle and how to prioritize limited analytical resources"
            },
            {
                "title": "Calculate ROI",
                "type": "practical",
                "question": "The hospital case invested $800K in their analytics program. They saved $6.3M annually. Calculate: 1) Simple ROI, 2) Payback period in months, 3) 3-year total value.",
                "answer": "1) Simple ROI = (Savings - Investment) / Investment = ($6.3M - $0.8M) / $0.8M = 687.5% ROI in year 1. 2) Payback period = $800K / ($6.3M/12 months) = $800K / $525K per month = 1.52 months (about 6-7 weeks). 3) 3-year value = ($6.3M Ã— 3) - $0.8M = $18.1M net value. Note: This assumes the $800K is a one-time investment; if there are annual operating costs, subtract those.",
                "hint": "ROI = (Gain - Cost) / Cost. Payback = Investment / Monthly Savings."
            },
            {
                "title": "Design Your Own Intervention",
                "type": "practical",
                "question": "A streaming service sees that 35% of free trial users cancel before converting to paid. Design a data-driven approach to reduce cancellations.",
                "answer": "DEFINE: 'Increase free trial to paid conversion from 65% to 80% within 6 months.' COLLECT: User behavior logs (features used, videos watched, engagement time), demographics, signup source, device type, time to first content. ANALYZE: Look for patterns - maybe users who watch 5+ shows convert at 90% vs 40% for those watching 1-2. PREDICTIVE: Build model to identify users at risk of not converting by Day 7. PRESCRIPTIVE: For at-risk users: personalized content recommendations, mid-trial email highlighting popular shows, offer extended trial or discounted first month, in-app tips highlighting premium features. Monitor conversion rate and adjust interventions.",
                "hint": "Follow the lifecycle: Define the goal with KPIs, collect relevant behavioral data, analyze patterns, then prescribe actions"
            }
        ],
        "quiz": [
            {
                "question": "In the MegaMart case, what was the total annual savings achieved?",
                "options": ["$1.8M", "$2.5M", "$2.55M", "$4.3M"],
                "correct": 2,
                "explanation": "Total savings = Reduced lost sales ($2.5M - $0.8M = $1.7M) + Reduced excess inventory ($1.8M - $0.95M = $0.85M) = $2.55M"
            },
            {
                "question": "The hospital's risk score model is an example of which analytics type?",
                "options": ["Descriptive", "Diagnostic", "Predictive", "All of the above"],
                "correct": 2,
                "explanation": "The risk score (0-100) predicts which patients are likely to be readmitted. This is predictive analytics - forecasting future outcomes based on current data."
            },
            {
                "question": "In the marketing case, which channel had the highest ROAS?",
                "options": ["Google Search", "Meta Ads", "Email", "TikTok"],
                "correct": 2,
                "explanation": "Email had ROAS of 12x ($200K spend â†’ $2.4M revenue), far exceeding other channels. This insight led to tripling the email budget."
            },
            {
                "question": "What common theme appears across all three case studies?",
                "options": ["They all used AI/machine learning", "They all measured ROI and documented improvements", "They all focused on customer acquisition", "They all had unlimited budgets"],
                "correct": 1,
                "explanation": "All cases defined clear metrics, measured before/after results, and documented improvements with specific numbers. This proves the value of data-driven approaches."
            }
        ]
    },
    "KPI Selection and Tracking": {
        "course": "Data Driven Decision-Making",
        "description": "Learn to select appropriate KPIs, set targets, and create tracking systems to guide decision-making.",
        "lessons": [
            {
                "title": "Choosing the Right KPIs",
                "content": """
**What Makes a Good KPI?**

Not all metrics are KPIs. Key Performance Indicators are the vital few metrics that truly matter for success.

**The SMART Framework for KPIs:**

| Criterion | Question | Example |
|-----------|----------|---------|
| **S**pecific | What exactly are we measuring? | "Monthly active users" not "engagement" |
| **M**easurable | Can we quantify it? | Number, percentage, ratio |
| **A**chievable | Is the target realistic? | Based on benchmarks and capability |
| **R**elevant | Does it connect to goals? | Aligned with business strategy |
| **T**ime-bound | What's the timeframe? | Monthly, quarterly, annually |

**Types of KPIs:**

| Type | Purpose | Examples |
|------|---------|----------|
| **Lagging** | Measure outcomes (past) | Revenue, profit, customer count |
| **Leading** | Predict outcomes (future) | Pipeline value, website traffic, leads |
| **Input** | Resources invested | Marketing spend, hiring, training hours |
| **Process** | Efficiency of activities | Cycle time, defect rate, response time |
| **Output** | Results produced | Units sold, tickets resolved, content published |

**KPI Selection Process:**
1. Start with strategic objectives
2. Ask: "What would tell us if we're succeeding?"
3. Limit to 5-7 KPIs per area (avoid dashboard overload)
4. Ensure a mix of leading and lagging indicators
5. Verify data is available and reliable

**Common Mistakes:**
- âŒ Too many KPIs (vanity metrics)
- âŒ Only lagging indicators (no early warning)
- âŒ KPIs that can't be influenced
- âŒ No clear connection to strategy
- âœ… Focus on 5-7 metrics that drive decisions
                """,
                "key_points": ["KPIs are vital few metrics, not all metrics", "Use SMART framework", "Balance leading and lagging indicators", "Limit to 5-7 KPIs to stay focused"]
            },
            {
                "title": "KPIs by Business Function",
                "content": """
**Industry-Standard KPIs by Department**

**SALES KPIs:**
| KPI | Formula | Target Example |
|-----|---------|----------------|
| Revenue | Sum of all sales | $1M/month |
| Conversion Rate | Deals Won / Deals Created | 25% |
| Average Deal Size | Revenue / # Deals | $10,000 |
| Sales Cycle Length | Avg days from lead to close | 45 days |
| Pipeline Coverage | Pipeline Value / Quota | 3x |

**MARKETING KPIs:**
| KPI | Formula | Target Example |
|-----|---------|----------------|
| Customer Acquisition Cost (CAC) | Marketing Spend / New Customers | $50 |
| Return on Ad Spend (ROAS) | Revenue from Ads / Ad Spend | 4x |
| Website Conversion Rate | Conversions / Visitors | 3% |
| Email Open Rate | Opens / Emails Sent | 25% |
| Lead-to-Customer Rate | Customers / Leads | 10% |

**CUSTOMER SUCCESS KPIs:**
| KPI | Formula | Target Example |
|-----|---------|----------------|
| Customer Churn Rate | Lost Customers / Total Customers | <5%/month |
| Net Promoter Score (NPS) | % Promoters - % Detractors | >50 |
| Customer Lifetime Value (CLV) | Avg Revenue Ã— Avg Lifespan | $500 |
| First Response Time | Avg time to first reply | <2 hours |
| Customer Satisfaction (CSAT) | Satisfied / Total Responses | >90% |

**OPERATIONS KPIs:**
| KPI | Formula | Target Example |
|-----|---------|----------------|
| On-Time Delivery Rate | On-time / Total Deliveries | >98% |
| Defect Rate | Defective Units / Total Units | <1% |
| Inventory Turnover | Cost of Goods Sold / Avg Inventory | 6x/year |
| Employee Productivity | Output / Employee Hours | Varies |
| Equipment Uptime | Operating Hours / Total Hours | >99% |

**Pro Tip:** The most valuable insight often comes from ratios and comparisons, not absolute numbers. "$1M revenue" means little without context like "vs. $800K last year" or "vs. $1.2M target."
                """,
                "key_points": ["Different functions have different KPIs", "Formulas make KPIs actionable", "Include targets for context", "Ratios and comparisons add meaning"]
            },
            {
                "title": "Building a KPI Dashboard",
                "content": """
**Creating Effective KPI Dashboards**

A dashboard should enable quick decisions, not just display numbers.

**Dashboard Design Principles:**

| Principle | Description |
|-----------|-------------|
| **5-second rule** | Key insight visible in 5 seconds |
| **Information hierarchy** | Most important KPIs at top |
| **Context always** | Show vs. target, vs. last period |
| **Actionable** | Each KPI links to a decision |
| **Real-time when needed** | Update frequency matches decision speed |

**Dashboard Layout Template:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXECUTIVE DASHBOARD - [Month/Year]     [Last Updated: Now] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [KPI Card 1]    [KPI Card 2]    [KPI Card 3]   [KPI Card 4]â”‚
â”‚   Revenue         New Users      Churn Rate     NPS Score   â”‚
â”‚   $1.2M â†‘12%     5,240 â†‘8%      3.2% â†“0.5%     72 â†‘5       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    [Main Trend Chart]                       â”‚
â”‚        Monthly Revenue Trend with Target Line               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Secondary      â”‚  [Supporting Detail Table]                â”‚
â”‚  Chart 1]       â”‚   Product    Revenue    Growth            â”‚
â”‚  Revenue by     â”‚   Product A  $500K      +15%              â”‚
â”‚  Region         â”‚   Product B  $400K      +8%               â”‚
â”‚                 â”‚   Product C  $300K      -3%               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**KPI Card Components:**
- Metric name
- Current value
- Comparison (â†‘â†“ vs target or previous)
- Color coding (Green/Yellow/Red)
- Sparkline trend (optional)

**Effective Visualizations by KPI Type:**

| KPI Type | Best Visualization |
|----------|-------------------|
| Single metric | Big number card |
| Trend over time | Line chart |
| Part of whole | Pie/donut chart |
| Comparison | Bar chart |
| Geographic | Map |
| Progress to goal | Gauge / progress bar |
| Multiple dimensions | Table with conditional formatting |
                """,
                "key_points": ["5-second rule - key insight immediately visible", "Always show context (vs target, vs previous)", "Use color coding for status", "Match visualization to KPI type"]
            },
            {
                "title": "Setting Targets and Monitoring",
                "content": """
**Setting Meaningful KPI Targets**

A KPI without a target is just a number. Targets make KPIs actionable.

**Methods for Setting Targets:**

| Method | Description | When to Use |
|--------|-------------|-------------|
| **Historical** | Based on past performance | Stable, mature processes |
| **Benchmark** | Based on industry standards | New metrics, competitive pressure |
| **Aspirational** | Stretch goals beyond current | Growth mode, transformation |
| **Bottoms-up** | Calculated from team capacity | Resource-constrained |
| **Negotiated** | Agreed with stakeholders | Political environments |

**Target-Setting Formula (Historical + Growth):**
```
Target = Last Year's Actual Ã— (1 + Growth Rate)

Example:
Last year revenue: $10M
Target growth: 20%
This year target: $10M Ã— 1.20 = $12M
```

**RAG Status (Red/Amber/Green):**

| Status | Definition | Action |
|--------|------------|--------|
| ðŸŸ¢ Green | On track (â‰¥95% of target) | Continue current approach |
| ðŸŸ¡ Amber | At risk (80-95% of target) | Investigate, adjust plans |
| ðŸ”´ Red | Off track (<80% of target) | Immediate intervention required |

**Monitoring Cadence:**

| KPI Type | Review Frequency | Action Threshold |
|----------|-----------------|------------------|
| Strategic (Revenue, Customers) | Weekly/Monthly | Amber for 2+ weeks |
| Operational (Efficiency, Quality) | Daily/Weekly | Red for 1 day |
| Leading (Pipeline, Traffic) | Daily | Any negative trend |
| Lagging (NPS, Retention) | Monthly/Quarterly | Compare to target |

**KPI Review Meeting Agenda:**
1. **Current Status** - Dashboard walkthrough (5 min)
2. **Reds and Ambers** - What's off track? (10 min)
3. **Root Cause** - Why? (10 min)
4. **Action Items** - What are we doing about it? (10 min)
5. **Greens at Risk** - Early warnings? (5 min)

**Document Decisions:**
Every KPI review should end with:
- What we learned
- Decisions made
- Action items with owners and deadlines
                """,
                "key_points": ["Use multiple methods to set targets", "RAG status enables quick decisions", "Review frequency matches KPI type", "Document decisions and actions from reviews"]
            }
        ],
        "exercises": [
            {
                "title": "Select KPIs for a Scenario",
                "type": "practical",
                "question": "You're the new data analyst at a subscription meal kit company. The CEO wants to 'grow the business.' Select 5 KPIs you would recommend tracking and explain why.",
                "answer": "Recommended KPIs: 1) Monthly Recurring Revenue (MRR) - Primary growth indicator, directly measures business size. 2) Customer Acquisition Cost (CAC) - Ensures growth is sustainable and profitable. 3) Monthly Churn Rate - Critical for subscription businesses; growth is pointless if customers leave faster than they join. 4) Customer Lifetime Value (CLV) - Combined with CAC, shows if unit economics work (CLV should be 3x+ CAC). 5) Active Subscriber Count - Simple, easy to communicate growth metric. BONUS LEADING: Website conversion rate - Predicts future subscriber growth.",
                "hint": "Think about what metrics truly indicate a subscription business is growing sustainably"
            },
            {
                "title": "Calculate Target",
                "type": "practical",
                "question": "Last year's quarterly revenue was: Q1=$2M, Q2=$2.3M, Q3=$2.1M, Q4=$2.8M. Calculate the targets for next year if the goal is 25% annual growth, distributed proportionally by quarter.",
                "answer": "Step 1: Last year's total = $2M + $2.3M + $2.1M + $2.8M = $9.2M. Step 2: Next year's total with 25% growth = $9.2M Ã— 1.25 = $11.5M. Step 3: Calculate each quarter's proportion of annual revenue: Q1=21.7%, Q2=25%, Q3=22.8%, Q4=30.4%. Step 4: Apply proportions to new total: Q1=$2.5M, Q2=$2.88M, Q3=$2.63M, Q4=$3.5M. These targets maintain the seasonal pattern (Q4 is peak) while achieving 25% growth.",
                "hint": "Calculate each quarter as a percentage of the year, then apply those percentages to the new annual target"
            },
            {
                "title": "Design a Dashboard",
                "type": "practical",
                "question": "An e-commerce manager wants a dashboard to monitor their holiday sales campaign (Nov-Dec). What KPIs would you include and how would you arrange them?",
                "answer": "Holiday Sales Dashboard: TOP ROW (Big Number Cards): 1) Daily Revenue vs Target (with â†‘â†“ and % of goal), 2) Conversion Rate (vs. campaign avg), 3) Average Order Value, 4) Cart Abandonment Rate. MIDDLE: Main line chart showing Daily Revenue with Target Line and Last Year comparison. Split into: LEFT - Revenue by Channel (pie or bar: Direct, Paid Search, Social, Email), RIGHT - Top Selling Products (table with stock status). BOTTOM: Hourly traffic heatmap to optimize staffing, and Promotional Code usage (which discounts are working). Update frequency: Real-time for revenue/traffic, hourly for others. Alert if daily revenue falls below 80% of target by 3pm.",
                "hint": "Think about what decisions need to be made during a high-stakes campaign period"
            }
        ],
        "quiz": [
            {
                "question": "Which of these is a LEADING indicator?",
                "options": ["Last month's revenue", "Customer satisfaction score", "Sales pipeline value", "Annual profit"],
                "correct": 2,
                "explanation": "Sales pipeline value is a leading indicator - it predicts future revenue. Revenue, satisfaction, and profit are lagging indicators that measure past outcomes."
            },
            {
                "question": "A KPI is at 87% of target. What RAG status should it show?",
                "options": ["Green - on track", "Amber - at risk", "Red - off track", "Depends on the KPI"],
                "correct": 1,
                "explanation": "87% falls in the Amber zone (80-95% of target), indicating the KPI is at risk and requires attention and possible plan adjustment."
            },
            {
                "question": "How many KPIs should each business function typically track?",
                "options": ["1-2", "5-7", "15-20", "As many as possible"],
                "correct": 1,
                "explanation": "Best practice is 5-7 KPIs per function. Too few miss important aspects; too many cause 'dashboard overload' and dilute focus."
            },
            {
                "question": "CLV/CAC ratio of 4:1 means:",
                "options": ["Company is losing money on each customer", "Each customer is worth 4Ã— what it costs to acquire them", "4% of customers are profitable", "Acquisition takes 4 months"],
                "correct": 1,
                "explanation": "CLV/CAC of 4:1 means Customer Lifetime Value is 4Ã— Customer Acquisition Cost. This is excellent - typically 3:1 or higher is considered healthy."
            },
            {
                "question": "For strategic KPIs like annual revenue, the typical review frequency is:",
                "options": ["Hourly", "Daily", "Weekly/Monthly", "Annually"],
                "correct": 2,
                "explanation": "Strategic KPIs are typically reviewed weekly or monthly. Daily is for operational metrics, and annual-only reviews don't allow time to course-correct."
            }
        ]
    },
    "Project Planning & Execution": {
        "course": "Semester Project 1",
        "description": "Learn to plan, scope, and execute data analysis projects from start to finish with professional standards.",
        "lessons": [
            {
                "title": "Defining Project Scope",
                "content": """
**Starting Your Data Analysis Project**

Every successful project begins with clear scope definition. Without it, projects fail or never end.

**Key Questions to Answer:**

| Question | Purpose |
|----------|---------|
| What problem are we solving? | Focus and direction |
| Who are the stakeholders? | Know your audience |
| What data do we need? | Resource planning |
| What are the deliverables? | Clear expectations |
| What's out of scope? | Prevent scope creep |
| When is it due? | Timeline planning |

**Project Scope Template:**
```
PROJECT: [Name]
PROBLEM STATEMENT: [1-2 sentences describing what you're solving]

OBJECTIVES:
1. [Specific goal 1]
2. [Specific goal 2]

IN SCOPE:
- [What's included]
- [Data sources to use]
- [Analysis methods]

OUT OF SCOPE:
- [What's NOT included]
- [Future phases]

DELIVERABLES:
- [ ] Data analysis report
- [ ] Cleaned dataset
- [ ] Visualization dashboard
- [ ] Presentation

TIMELINE: [Start] to [End]
STAKEHOLDERS: [Who needs to see results]
```

**Common Scope Mistakes:**
- âŒ "Analyze all the data" - Too vague
- âŒ No defined deliverables
- âŒ Unrealistic timeline
- âŒ Scope changes without documentation
- âœ… Specific, measurable objectives
- âœ… Clear list of what's IN and OUT
                """,
                "key_points": ["Define scope before starting work", "Use a scope template", "Document what's OUT of scope", "Get stakeholder agreement upfront"]
            },
            {
                "title": "Creating a Project Timeline",
                "content": """
**Building Your Project Schedule**

A timeline keeps you on track and helps manage stakeholder expectations.

**Data Analysis Project Phases:**

| Phase | % of Time | Activities |
|-------|-----------|------------|
| 1. Planning | 10% | Scope, data needs, approach |
| 2. Data Collection | 15% | Gather, access, import data |
| 3. Data Cleaning | 30-40% | Quality checks, standardization |
| 4. Analysis | 20% | Statistics, modeling, findings |
| 5. Documentation | 10% | Report writing, visualizations |
| 6. Presentation | 5% | Final delivery, Q&A |

**Sample 4-Week Project Timeline:**

```
WEEK 1: Foundation
â”œâ”€â”€ Day 1-2: Define scope, identify data sources
â”œâ”€â”€ Day 3-4: Collect and import data
â””â”€â”€ Day 5: Initial data exploration

WEEK 2: Data Preparation
â”œâ”€â”€ Day 1-2: Data cleaning and validation
â”œâ”€â”€ Day 3-4: Continue cleaning, handle missing values
â””â”€â”€ Day 5: Document data quality issues

WEEK 3: Analysis
â”œâ”€â”€ Day 1-2: Perform main analysis
â”œâ”€â”€ Day 3: Validate findings
â”œâ”€â”€ Day 4-5: Create visualizations
â””â”€â”€ Buffer day for unexpected issues

WEEK 4: Delivery
â”œâ”€â”€ Day 1-2: Write report/documentation
â”œâ”€â”€ Day 3: Create presentation
â”œâ”€â”€ Day 4: Review and refine
â””â”€â”€ Day 5: Final presentation
```

**Milestone Checkpoints:**
- âœ… Week 1: Data collected and accessible
- âœ… Week 2: Clean dataset ready for analysis
- âœ… Week 3: Key findings identified
- âœ… Week 4: Deliverables complete

**Buffer Time:**
Always add 20% buffer for unexpected issues:
- Data quality worse than expected
- New questions from stakeholders
- Technical problems
- Illness or other interruptions
                """,
                "key_points": ["Data cleaning takes most time (30-40%)", "Break project into weekly milestones", "Add 20% buffer for unexpected issues", "Track progress against timeline"]
            },
            {
                "title": "Managing Deliverables",
                "content": """
**Project Deliverables for Data Analysis**

Deliverables are the tangible outputs that prove your work is complete.

**Common Data Project Deliverables:**

| Deliverable | Purpose | Format |
|-------------|---------|--------|
| **Analysis Report** | Main findings and recommendations | PDF, Word doc |
| **Clean Dataset** | Prepared data for future use | CSV, Excel |
| **Dashboard** | Interactive visualization | Power BI, Tableau |
| **Presentation** | Summary for stakeholders | PowerPoint, Slides |
| **Code/Scripts** | Reproducible analysis | Python, SQL files |
| **Data Dictionary** | Explain data fields | Excel, PDF |

**Quality Checklist for Each Deliverable:**

**ðŸ“Š Analysis Report:**
- [ ] Executive summary (1 page)
- [ ] Clear problem statement
- [ ] Methodology explained
- [ ] Key findings with evidence
- [ ] Visualizations to support points
- [ ] Recommendations with rationale
- [ ] Limitations and next steps
- [ ] Proofread for errors

**ðŸ“ Clean Dataset:**
- [ ] All values validated
- [ ] Consistent formatting
- [ ] Missing values handled
- [ ] Outliers addressed
- [ ] Column names clear
- [ ] Data dictionary included

**ðŸ“ˆ Dashboard/Visualizations:**
- [ ] Clear titles and labels
- [ ] Appropriate chart types
- [ ] Consistent colors and fonts
- [ ] Interactive filters work
- [ ] Data source documented

**ðŸŽ¤ Presentation:**
- [ ] 10-15 slides maximum
- [ ] One key message per slide
- [ ] Visual-heavy, text-light
- [ ] Practiced timing (aim for 15-20 min)
- [ ] Prepared for Q&A
                """,
                "key_points": ["Define deliverables at project start", "Use checklists for quality", "Each deliverable serves a purpose", "Document everything for reproducibility"]
            },
            {
                "title": "Working Independently and in Teams",
                "content": """
**Project Execution: Solo and Team Work**

Data projects can be individual or collaborative - each requires different skills.

**Individual Project Best Practices:**

| Practice | Why It Matters |
|----------|---------------|
| **Daily progress log** | Track what you did, decisions made |
| **Version control** | Save versions (v1, v2, v3 or use Git) |
| **Regular breaks** | Avoid burnout, fresh eyes catch errors |
| **Self-review** | Check your own work before submission |
| **Ask for feedback** | Don't wait until the end |

**Sample Daily Log Entry:**
```
Date: 2025-02-15
Hours worked: 4

COMPLETED:
- Cleaned customer dataset (removed 145 duplicates)
- Created pivot table for sales by region
- Identified outlier in March data

BLOCKED:
- Need access to inventory data (emailed IT)

NEXT:
- Follow up on data access
- Start correlation analysis

DECISIONS MADE:
- Used median imputation for 23 missing ages
- Excluded records before 2023 (incomplete)
```

**Team Project Best Practices:**

| Practice | How to Do It |
|----------|-------------|
| **Clear role division** | Who does what (data, analysis, report) |
| **Regular check-ins** | Daily standup or weekly sync |
| **Shared workspace** | Cloud storage, shared drives |
| **Communication channel** | Slack, Teams, email thread |
| **Integration points** | When to merge work together |

**Team Role Examples:**
- **Data Lead**: Collection, cleaning, validation
- **Analysis Lead**: Statistics, modeling, insights
- **Visualization Lead**: Charts, dashboards
- **Documentation Lead**: Report, presentation

**Conflict Resolution:**
1. Discuss disagreements early, not at deadline
2. Focus on the project goal, not personal preferences
3. Use data to settle debates when possible
4. Escalate to mentor/supervisor if stuck
                """,
                "key_points": ["Keep a daily progress log", "Use version control for all files", "In teams: define clear roles", "Communicate early about problems"]
            }
        ],
        "exercises": [
            {
                "title": "Write a Scope Statement",
                "type": "practical",
                "question": "Your manager asks you to 'look into why sales are down.' Write a proper scope statement with objectives, in-scope, out-of-scope, and deliverables.",
                "answer": "PROJECT: Q1 2025 Sales Decline Analysis. PROBLEM: Identify root causes for 15% sales decline in Q1 2025 vs Q1 2024. OBJECTIVES: 1) Quantify decline by product category and region, 2) Identify top 3 contributing factors, 3) Recommend corrective actions. IN SCOPE: Sales data 2024-2025, customer feedback Q1 2025, competitor pricing data. OUT OF SCOPE: Marketing campaign effectiveness (separate project), international markets, product development recommendations. DELIVERABLES: Analysis report with findings and recommendations, executive presentation (10 slides), cleaned sales dataset. TIMELINE: 3 weeks. STAKEHOLDERS: Sales VP, Regional Managers.",
                "hint": "Transform the vague request into specific, measurable objectives with clear boundaries"
            },
            {
                "title": "Create a Project Timeline",
                "type": "practical",
                "question": "You have 2 weeks to analyze customer churn data and present findings. Create a day-by-day timeline with milestones.",
                "answer": "WEEK 1 - Data & Cleaning: Day 1: Define scope, success metrics, stakeholder alignment. Day 2: Collect customer data, subscription history, support tickets. Day 3-4: Data cleaning (expect 2 days - data quality usually worse than expected). Day 5: Exploratory analysis, initial patterns. MILESTONE: Clean dataset ready, initial insights documented. WEEK 2 - Analysis & Delivery: Day 1-2: Deep analysis (churn predictors, segment analysis, correlation). Day 3: Create visualizations, validate findings. Day 4: Write report, build presentation. Day 5: Review, practice presentation, buffer for refinements. MILESTONE: Final deliverables complete. BUFFER: Day 5 of each week reserved for overruns.",
                "hint": "Remember: cleaning takes 30-40% of time, always add buffer"
            },
            {
                "title": "Deliverable Checklist",
                "type": "scenario",
                "question": "You've finished your analysis and are about to submit. Your report shows that Product A has declining sales. What 5 things should you verify before submitting?",
                "answer": "Before submitting, verify: 1) DATA ACCURACY: Double-check the numbers - did you filter correctly? Are calculations right? Run the analysis again to confirm. 2) EVIDENCE: Does your visualization clearly show the decline? Are axes labeled correctly? Is the time period clear? 3) CONTEXT: Did you compare to benchmarks or previous periods? Is the decline significant or normal variation? 4) RECOMMENDATIONS: Are your suggestions actionable and tied to findings? 5) PROOFREADING: Check for typos, formatting issues, missing sections. Have someone else review if possible. BONUS: Check that your data source is documented so findings can be reproduced.",
                "hint": "Think about what would embarrass you if it were wrong in front of stakeholders"
            }
        ],
        "quiz": [
            {
                "question": "What percentage of a data project is typically spent on data cleaning?",
                "options": ["5-10%", "15-20%", "30-40%", "60-70%"],
                "correct": 2,
                "explanation": "Data cleaning typically takes 30-40% of project time. Real-world data is messy, and underestimating cleaning time is a common mistake."
            },
            {
                "question": "What is 'scope creep'?",
                "options": ["When the project finishes early", "When requirements keep expanding beyond original plan", "When team members leave", "When data is insufficient"],
                "correct": 1,
                "explanation": "Scope creep occurs when project requirements continuously expand beyond the original plan, often causing delays and budget overruns."
            },
            {
                "question": "Which deliverable provides explanations of data fields and their meanings?",
                "options": ["Executive Summary", "Data Dictionary", "Dashboard", "Analysis Report"],
                "correct": 1,
                "explanation": "A Data Dictionary documents what each field means, its data type, allowed values, and any transformations applied."
            },
            {
                "question": "When should you document decisions made during data cleaning?",
                "options": ["At the end of the project", "During the cleaning process", "Only if asked", "Never - it's not important"],
                "correct": 1,
                "explanation": "Document decisions as you make them. This ensures reproducibility, helps explain your methodology, and protects you if questions arise later."
            }
        ]
    },
    "Data Ethics & GDPR": {
        "course": "Semester Project 1",
        "description": "Understand ethical principles for data collection, storage, and use, including GDPR compliance.",
        "lessons": [
            {
                "title": "Why Data Ethics Matters",
                "content": """
**The Importance of Ethical Data Practice**

As a data analyst, you have access to sensitive information. With this access comes responsibility.

**Real-World Consequences of Poor Data Ethics:**

| Incident | What Happened | Consequence |
|----------|--------------|-------------|
| Cambridge Analytica | Used Facebook data without consent for political targeting | $5 billion fine, company dissolved |
| Equifax Breach | Poor security exposed 147 million people's data | $700 million settlement, reputation destroyed |
| Target Pregnancy | Predicted pregnancy from purchase data, exposed to family | Public backlash, privacy concerns |

**Core Ethical Principles:**

1. **Transparency**: Be honest about what data you collect and why
2. **Consent**: Get permission before collecting personal data
3. **Purpose Limitation**: Only use data for stated purposes
4. **Data Minimization**: Collect only what you need
5. **Accuracy**: Keep data correct and up-to-date
6. **Security**: Protect data from unauthorized access
7. **Accountability**: Take responsibility for data handling

**Questions to Ask Yourself:**
- Would I be comfortable if my data was used this way?
- Would this be acceptable if it became public?
- Am I respecting people's privacy and autonomy?
- Could this analysis harm individuals or groups?
- Is this use of data legal and compliant?

**The "Front Page Test":**
Before doing something with data, ask: "Would I be comfortable if this appeared on the front page of a newspaper?"
                """,
                "key_points": ["Data access comes with responsibility", "Poor ethics has real consequences", "Always consider the human impact", "Use the 'front page test'"]
            },
            {
                "title": "GDPR Fundamentals",
                "content": """
**Understanding GDPR (General Data Protection Regulation)**

GDPR is EU law that protects personal data. It applies to ANY organization handling EU residents' data.

**Key GDPR Principles:**

| Principle | Meaning | Your Action |
|-----------|---------|-------------|
| **Lawfulness** | Must have legal basis to process data | Document your legal basis |
| **Purpose Limitation** | Only use for specified purposes | State purpose before collecting |
| **Data Minimization** | Collect only what's necessary | Remove unnecessary fields |
| **Accuracy** | Data must be correct | Validate and update regularly |
| **Storage Limitation** | Don't keep longer than needed | Define retention periods |
| **Security** | Protect against unauthorized access | Encrypt, secure access |
| **Accountability** | Must prove compliance | Document everything |

**What is Personal Data?**
Any information relating to an identified or identifiable person:
- âœ… Name, email, phone number
- âœ… ID numbers (SSN, passport)
- âœ… Location data
- âœ… IP address
- âœ… Photos, videos
- âœ… Health, financial, genetic data (SPECIAL category - extra protection)

**Legal Bases for Processing:**
1. **Consent**: Person explicitly agrees
2. **Contract**: Necessary to fulfill a contract
3. **Legal Obligation**: Required by law
4. **Vital Interests**: To protect someone's life
5. **Public Task**: Official authority function
6. **Legitimate Interest**: Balanced business need (most complex)

**Individual Rights Under GDPR:**
- Right to be informed (what data, why, how long)
- Right of access (get copy of their data)
- Right to rectification (correct errors)
- Right to erasure ("right to be forgotten")
- Right to restrict processing
- Right to data portability
- Right to object
                """,
                "key_points": ["GDPR applies to EU resident data globally", "Need legal basis to process personal data", "Individuals have strong rights over their data", "Document your compliance"]
            },
            {
                "title": "Ethical Data Collection",
                "content": """
**Collecting Data the Right Way**

Before collecting any data, follow these guidelines:

**Before Collection Checklist:**
- [ ] Do I have a clear, specific purpose?
- [ ] Am I collecting only what I need? (minimization)
- [ ] Do I have legal basis/consent?
- [ ] Have I informed people about the collection?
- [ ] Is my collection method secure?
- [ ] Have I documented my approach?

**Sources and Their Ethical Considerations:**

| Source | Ethical Concerns | Best Practice |
|--------|-----------------|---------------|
| **Company databases** | Access rights, purpose limitation | Only access what you need for the task |
| **Surveys** | Informed consent, voluntary participation | Clear purpose statement, opt-out option |
| **Web scraping** | Terms of service, robots.txt | Check legality, respect restrictions |
| **Social media** | Public vs private, context collapse | Consider user expectations |
| **Third-party data** | Original consent, data quality | Verify source legitimacy and consent |
| **Public datasets** | Anonymization, re-identification risk | Check license and privacy safeguards |

**Informed Consent Essentials:**
When collecting directly from people:
1. Who you are (organization)
2. What data you're collecting
3. Why you need it (purpose)
4. How long you'll keep it
5. Who you'll share it with
6. Their rights (access, delete, etc.)
7. How to withdraw consent

**Red Flags - When to Stop and Ask:**
ðŸš© You don't have explicit permission
ðŸš© Data seems too personal for the purpose
ðŸš© You're not sure about the legal basis
ðŸš© The collection method feels sneaky
ðŸš© You can't explain why you need it
                """,
                "key_points": ["Always have a clear purpose before collecting", "Collect minimum necessary data", "Informed consent is essential", "When in doubt, stop and ask"]
            },
            {
                "title": "Handling Sensitive Data",
                "content": """
**Working with Sensitive Information**

Some data requires extra care due to potential for discrimination or harm.

**Special Category Data (GDPR):**
Requires explicit consent or special legal basis:
- Racial or ethnic origin
- Political opinions
- Religious beliefs
- Trade union membership
- Genetic data
- Biometric data
- Health data
- Sexual orientation

**Sensitive Data Best Practices:**

| Practice | How to Implement |
|----------|-----------------|
| **Anonymization** | Remove all identifying information |
| **Pseudonymization** | Replace IDs with codes (can be reversed with key) |
| **Encryption** | Encrypt data at rest and in transit |
| **Access Control** | Limit who can see sensitive fields |
| **Audit Logging** | Track who accessed what and when |
| **Secure Deletion** | Properly destroy when no longer needed |

**Anonymization Techniques:**
- **Data masking**: Replace with fake but realistic values
- **Generalization**: Use ranges instead of exact values (Age: 30-35)
- **Suppression**: Remove fields entirely
- **Noise addition**: Add random variation to values
- **Aggregation**: Only report group-level statistics

**Re-identification Risk:**
Even "anonymized" data can sometimes identify individuals:
- Netflix dataset + IMDB = identified users
- Zip code + birth date + gender = 87% of US uniquely identified

**Questions to Ask:**
- Can this data identify anyone if combined with other sources?
- Would individuals be surprised by this use of their data?
- Could this analysis enable discrimination?
- Is the minimum necessary data being used?
                """,
                "key_points": ["Special category data needs extra protection", "Anonymization is not always enough", "Consider re-identification risk", "Use encryption and access controls"]
            }
        ],
        "exercises": [
            {
                "title": "Identify Ethical Issues",
                "type": "scenario",
                "question": "Your company wants to analyze employee emails to predict which employees might quit. What ethical concerns should you raise?",
                "answer": "Ethical concerns: 1) PRIVACY: Employees may not know their emails are monitored for this purpose - lack of transparency. 2) CONSENT: Did employees consent to this specific use? Agreeing to email monitoring for security is different from predictive analysis. 3) DISCRIMINATION: Model might unfairly flag people based on protected characteristics (parental status, health discussions). 4) TRUST: Even if legal, this erodes employee trust and workplace culture. 5) PURPOSE LIMITATION: Email data collected for communication, not retention prediction. RECOMMENDATION: If proceeding, use voluntary surveys with clear consent, aggregate data only, and involve HR/legal. Consider the 'front page test' - how would this look publicly?",
                "hint": "Consider privacy, consent, potential for harm, and how employees would feel if they knew"
            },
            {
                "title": "GDPR Compliance Check",
                "type": "practical",
                "question": "You're building a customer survey for a European company. List 5 things you must include to comply with GDPR.",
                "answer": "GDPR compliance for survey: 1) IDENTITY: State who is collecting the data (company name and contact). 2) PURPOSE: Clearly explain why you're collecting responses and how they'll be used. 3) LEGAL BASIS: State your legal basis (likely consent for a survey). 4) RETENTION: Specify how long responses will be kept. 5) RIGHTS: Inform respondents of their rights (access, deletion, withdrawal). 6) CONSENT MECHANISM: Clear opt-in (no pre-ticked boxes), separate consent for different uses. 7) CONTACT: Provide way to contact data protection officer or make requests. 8) THIRD PARTIES: Disclose if data will be shared (analytics tools, etc.).",
                "hint": "Think about what individuals need to know to make an informed choice about participating"
            },
            {
                "title": "Anonymization Decision",
                "type": "scenario",
                "question": "You have customer data with: Name, Email, Age, City, Purchase History. You need to share analysis with a partner company. How would you anonymize it?",
                "answer": "Anonymization approach: 1) REMOVE: Name and Email (direct identifiers) - delete entirely. 2) GENERALIZE: Age â†’ Age bands (18-25, 26-35, etc.) to prevent exact matching. City â†’ Region level if small city populations. 3) AGGREGATE: Purchase history â†’ Categories and totals, not individual transactions. 4) ASSESS: Can someone re-identify with remaining data? If city is small and age range narrow, might need to generalize further. 5) TEST: Try to identify yourself in the anonymized data - if you can, others might too. 6) DOCUMENT: Record what was removed/changed and why. Consider if pseudonymization (using a code that maps back) is sufficient instead, with the mapping kept secure.",
                "hint": "Consider both direct identifiers and combinations of fields that could identify someone"
            }
        ],
        "quiz": [
            {
                "question": "Which is NOT a GDPR principle?",
                "options": ["Data minimization", "Purpose limitation", "Maximum data collection", "Storage limitation"],
                "correct": 2,
                "explanation": "GDPR requires data MINIMIZATION - collecting only what's necessary. Maximum data collection is the opposite of what GDPR requires."
            },
            {
                "question": "Under GDPR, health data is classified as:",
                "options": ["Standard personal data", "Special category data requiring extra protection", "Public data", "Not regulated"],
                "correct": 1,
                "explanation": "Health data is 'special category data' under GDPR, requiring explicit consent and additional protections due to its sensitive nature."
            },
            {
                "question": "The 'right to be forgotten' means:",
                "options": ["Forgetting user passwords", "Individuals can request deletion of their data", "Companies can forget compliance", "Data expires automatically"],
                "correct": 1,
                "explanation": "Under GDPR, individuals have the right to request erasure of their personal data ('right to be forgotten') when certain conditions apply."
            },
            {
                "question": "Which technique replaces identifiers with codes that can be reversed with a key?",
                "options": ["Anonymization", "Pseudonymization", "Encryption", "Aggregation"],
                "correct": 1,
                "explanation": "Pseudonymization replaces identifiers with artificial identifiers (codes). Unlike anonymization, the link can be restored with the key, so it's still personal data under GDPR."
            }
        ]
    },
    "Documentation & Presentation": {
        "course": "Semester Project 1",
        "description": "Master professional documentation and presentation skills for data analysis projects.",
        "lessons": [
            {
                "title": "Writing the Analysis Report",
                "content": """
**Structure of a Professional Data Analysis Report**

Your report tells the story of your analysis and provides a record for future reference.

**Standard Report Structure:**

```
1. EXECUTIVE SUMMARY (1 page max)
   - Key findings in bullet points
   - Main recommendations
   - Bottom line impact

2. INTRODUCTION
   - Problem statement
   - Objectives
   - Scope and limitations

3. METHODOLOGY
   - Data sources
   - Cleaning steps taken
   - Analysis techniques used

4. FINDINGS
   - Results with visualizations
   - Key insights
   - Statistical evidence

5. RECOMMENDATIONS
   - Actions to take
   - Expected outcomes
   - Implementation considerations

6. APPENDIX
   - Technical details
   - Additional charts
   - Data dictionary
```

**Writing Tips:**

| Do | Don't |
|----|-------|
| Lead with the conclusion | Bury insights at the end |
| Use bullet points for lists | Write long paragraphs |
| Support claims with data | Make unsupported assertions |
| Explain technical terms | Assume reader expertise |
| Include visualizations | Use only text and tables |
| Acknowledge limitations | Overstate certainty |

**Executive Summary Formula:**
1. **Situation**: What was the problem/question?
2. **Findings**: What did we discover? (2-3 key points)
3. **Implications**: Why does it matter?
4. **Recommendations**: What should we do?

**Example Executive Summary:**
"This analysis investigated the 15% sales decline in Q1 2025. **Key findings:** (1) 80% of decline concentrated in the North region, (2) main driver was lost accounts to new competitor, (3) remaining customers are spending more per order. **Recommendation:** Launch targeted retention campaign in North region with estimated $200K recovery potential."
                """,
                "key_points": ["Lead with conclusions, not process", "Executive summary is most-read section", "Support every claim with evidence", "Keep it concise - respect reader's time"]
            },
            {
                "title": "Creating Effective Visualizations",
                "content": """
**Visualization Best Practices for Reports**

Charts should clarify, not confuse. Choose the right type for your message.

**Chart Selection Guide:**

| Message | Best Chart Type |
|---------|-----------------|
| Comparison across categories | Bar chart (horizontal for many categories) |
| Trend over time | Line chart |
| Part of whole | Pie chart (max 5-6 segments) or stacked bar |
| Relationship between variables | Scatter plot |
| Distribution | Histogram |
| Geographic data | Map |
| Multiple dimensions | Combo chart, small multiples |

**Visual Design Rules:**

**1. Declutter:**
- Remove gridlines or make very light
- Eliminate unnecessary borders
- Don't use 3D effects (ever!)
- Remove chart junk and decorations

**2. Focus Attention:**
- Highlight the key data point
- Use color purposefully (gray for context, color for focus)
- Add clear titles that state the insight

**3. Make It Readable:**
- Label axes clearly with units
- Font size minimum 10pt for print, 18pt for presentations
- Legend close to data (or label directly)
- Sort bars by value (usually largest to smallest)

**Title Examples:**

âŒ **Bad**: "Sales by Region"
âœ… **Good**: "North Region Sales Declined 20% While Others Grew"

âŒ **Bad**: "Customer Satisfaction Survey Results"
âœ… **Good**: "85% of Customers Rate Service 'Good' or 'Excellent'"

**Color Guidelines:**
- Use color consistently (same meaning throughout)
- Avoid red/green only (colorblind accessibility)
- Gray for context, 1-2 accent colors for key data
- Match company brand colors when appropriate
                """,
                "key_points": ["Choose chart type based on message", "Declutter ruthlessly", "Title should state the insight", "Use color with purpose"]
            },
            {
                "title": "Building Your Presentation",
                "content": """
**Presenting Data Findings to Stakeholders**

Your presentation is often the only thing stakeholders see. Make it count.

**Presentation Structure (15-20 minutes):**

```
1. OPENING (2 min)
   - The question we answered
   - Why it matters
   
2. KEY FINDINGS (8-10 min)
   - Finding 1 + supporting visual
   - Finding 2 + supporting visual
   - Finding 3 + supporting visual
   
3. RECOMMENDATIONS (3-4 min)
   - What should we do?
   - Expected impact
   
4. NEXT STEPS (2 min)
   - Immediate actions
   - Open questions for discussion
```

**Slide Design Rules:**

| Rule | Why |
|------|-----|
| One idea per slide | Prevents overwhelm |
| Maximum 5 bullet points | Easy to scan |
| Maximum 5 words per bullet | Prevents reading slides |
| Large font (24pt minimum) | Readability from back of room |
| High contrast | Visibility on projectors |
| Image/chart > text | Visual memory is stronger |

**Handling Data in Presentations:**

**âŒ Don't:**
- Show complex tables with many rows
- Include all the data
- Use tiny fonts to fit everything
- Read numbers aloud from slides

**âœ… Do:**
- Highlight key numbers in large font
- Round numbers (say "about 85%" not "84.7%")
- Use visuals to show patterns
- Provide detailed tables in appendix/handout

**Preparing for Q&A:**
1. Anticipate questions (especially "why?" and "how confident?")
2. Prepare backup slides with details
3. Know your data source and methodology
4. It's OK to say "I'll follow up on that"
5. Listen fully before answering
                """,
                "key_points": ["Structure: Opening, Findings, Recommendations, Next Steps", "One idea per slide", "Round numbers for speaking", "Prepare for likely questions"]
            },
            {
                "title": "The Reflection Report",
                "content": """
**Writing Your Individual Reflection**

A reflection report documents your learning journey and personal growth during the project.

**Purpose of Reflection:**
- Consolidate what you learned
- Identify areas for improvement
- Demonstrate critical thinking
- Build portfolio of experiences

**Reflection Structure:**

```
1. PROJECT OVERVIEW
   - Brief description of what you did
   - Your role and responsibilities

2. WHAT WENT WELL
   - Successful approaches
   - Skills you applied effectively
   - Challenges you overcame

3. WHAT COULD BE IMPROVED
   - Difficulties encountered
   - Mistakes made and lessons learned
   - What you would do differently

4. SKILLS DEVELOPED
   - Technical skills gained
   - Soft skills practiced
   - Knowledge areas expanded

5. FUTURE APPLICATION
   - How you'll apply learnings
   - Goals for continued development
```

**Reflection Prompts:**

| Area | Questions to Consider |
|------|----------------------|
| Process | What worked? What didn't? Why? |
| Technical | What new tools/techniques did you learn? |
| Collaboration | How did you work with others? |
| Time Management | Did you meet deadlines? Why/why not? |
| Problem-Solving | What unexpected issues arose? How did you handle them? |
| Growth | How have you improved from the start? |

**Writing Tips:**
- Be honest about challenges (this shows maturity)
- Give specific examples, not vague statements
- Connect learning to future goals
- Show self-awareness and critical thinking

**Example Excerpt:**
"I initially underestimated data cleaning time, allocating only 2 days when it ultimately took 5. This compressed my analysis phase and forced me to simplify my approach. In future projects, I'll add 50% buffer to cleaning estimates and do a quick data quality assessment before planning the timeline."
                """,
                "key_points": ["Reflection shows learning and maturity", "Be honest about challenges", "Give specific examples", "Connect to future improvement"]
            }
        ],
        "exercises": [
            {
                "title": "Write an Executive Summary",
                "type": "practical",
                "question": "Your analysis found: 1) Customer churn increased from 5% to 8% this quarter, 2) Main cause is price increase, 3) Competitor is offering 15% lower prices, 4) Loyal customers (2+ years) churned less. Write a 4-sentence executive summary.",
                "answer": "This analysis investigated the 60% increase in customer churn (5% to 8%) this quarter. Key findings: (1) Price-sensitive customers are leaving for a competitor offering 15% lower rates, while (2) long-term customers (2+ years) remain loyal with only 3% churn. The estimated revenue impact is $120K annually if trends continue. Recommendation: Implement tiered pricing with loyalty discounts for customers approaching the 2-year mark, and consider a targeted win-back campaign for recently churned price-sensitive segments.",
                "hint": "Follow: Situation â†’ Findings â†’ Implication â†’ Recommendation"
            },
            {
                "title": "Fix the Slide",
                "type": "scenario",
                "question": "A slide has: Title 'Q1 Results', 12 bullet points of text in 10pt font, a complex table with 15 rows and 8 columns, and no visualization. What's wrong and how would you fix it?",
                "answer": "Problems: 1) Vague title that doesn't state insight, 2) Too many bullets (max 5), 3) Font too small (need 24pt+), 4) Complex table unreadable in presentation, 5) No visualization to tell the story. Fixes: 1) Retitle to the main finding: 'Q1 Revenue Up 12%, Driven by New Product Line', 2) Keep only 3-4 key bullet points, 3) Increase font to minimum 24pt, 4) Replace table with a bar or line chart showing the key comparison, 5) Move detailed table to appendix or handout, 6) Add one clear visualization that supports the title claim.",
                "hint": "Think about what someone can actually read and absorb during a presentation"
            },
            {
                "title": "Reflect on a Challenge",
                "type": "practical",
                "question": "Write a reflection paragraph about a time you struggled with messy data. Include: what happened, how you felt, what you did, and what you learned.",
                "answer": "Sample reflection: During my sales analysis project, I received customer data with over 40% missing values in key fields. Initially, I felt frustrated and considered asking for better data, which would have delayed the project by weeks. Instead, I researched imputation techniques and decided to use median imputation for numeric fields and 'Unknown' categories for text fields, documenting each decision. I learned that perfect data rarely exists in the real world, and developing strategies to work with imperfect data is a core analyst skill. I also learned to do a data quality assessment at the project start so I can plan appropriately. Going forward, I'll always request a sample of data before committing to timelines.",
                "hint": "Be specific about the situation, your response, and the concrete lesson learned"
            }
        ],
        "quiz": [
            {
                "question": "What should the first page of an analysis report typically be?",
                "options": ["Methodology", "Data sources", "Executive summary", "Table of contents"],
                "correct": 2,
                "explanation": "The executive summary should be first - it's the most-read section and gives busy stakeholders the key takeaways immediately."
            },
            {
                "question": "A good chart title should:",
                "options": ["Describe the chart type", "State the key insight", "List the data source", "Be as short as possible"],
                "correct": 1,
                "explanation": "A good chart title states the insight ('Sales Grew 20% in Q4') rather than just describing what the chart shows ('Quarterly Sales')."
            },
            {
                "question": "What is the maximum recommended number of bullet points per slide?",
                "options": ["3", "5", "8", "10"],
                "correct": 1,
                "explanation": "Maximum 5 bullet points per slide keeps content scannable. More than that overwhelms the audience."
            },
            {
                "question": "In a reflection report, you should:",
                "options": ["Only discuss successes", "Be honest about challenges and mistakes", "Blame external factors for problems", "Keep it as short as possible"],
                "correct": 1,
                "explanation": "Being honest about challenges and mistakes shows maturity and self-awareness. The goal is to demonstrate learning, not perfection."
            }
        ]
    },
    "Soft Skills for Data Analysts": {
        "course": "Semester Project 1",
        "description": "Develop essential soft skills for working effectively with stakeholders and team members.",
        "lessons": [
            {
                "title": "Communicating with Non-Technical Stakeholders",
                "content": """
**Translating Data Into Business Language**

Your analysis is only valuable if stakeholders understand and act on it.

**Common Communication Gaps:**

| Analyst Says | Stakeholder Hears |
|-------------|-------------------|
| "R-squared is 0.85" | "...what?" |
| "Statistically significant" | "Must be important" (not always true) |
| "Correlation of 0.7" | "Something about numbers" |
| "We need more data" | "They're stalling" |

**Translation Guide:**

| Technical Term | Plain Language |
|---------------|----------------|
| Correlation | "These things tend to move together" |
| Statistical significance | "This pattern is unlikely to be random chance" |
| Regression | "A formula to predict one thing from another" |
| Outlier | "An unusual value that doesn't fit the pattern" |
| Confidence interval | "We're 95% sure the true number is in this range" |
| P-value < 0.05 | "Less than 5% chance this is random luck" |

**The "So What?" Test:**
After every finding, ask yourself: "So what? Why should they care?"

âŒ "Average order value increased by $12"
âœ… "Average order value increased by $12, which means $2M additional annual revenue"

**Storytelling Framework:**
1. **Hook**: Start with the business problem
2. **Context**: What did we look at?
3. **Findings**: What did we discover?
4. **Impact**: What does it mean for the business?
5. **Action**: What should we do?

**Tips for Non-Technical Audiences:**
- Lead with the business impact, not the methodology
- Use analogies they understand
- Show, don't tell (visualizations > statistics)
- Avoid acronyms and jargon
- Check for understanding ("Does that make sense?")
                """,
                "key_points": ["Translate technical terms to plain language", "Always answer 'So what?' ", "Lead with business impact", "Use the storytelling framework"]
            },
            {
                "title": "Active Listening & Asking Questions",
                "content": """
**Understanding What Stakeholders Really Need**

Often what stakeholders ask for isn't what they actually need. Listening helps you find the real problem.

**Levels of Listening:**

| Level | Description | You're Doing This When... |
|-------|-------------|--------------------------|
| 1. Not listening | Waiting for your turn to speak | Formulating response while they talk |
| 2. Selective | Hearing only what you expect | Missing context and nuance |
| 3. Active | Fully focused, seeking to understand | Asking clarifying questions |
| 4. Empathic | Understanding feelings and perspective | Recognizing underlying concerns |

**Active Listening Techniques:**

1. **Paraphrase**: "So what I'm hearing is..."
2. **Clarify**: "When you say X, do you mean...?"
3. **Summarize**: "Let me make sure I understand..."
4. **Probe**: "Can you tell me more about...?"
5. **Reflect**: "It sounds like you're concerned about..."

**Powerful Questions to Ask:**

| Situation | Question |
|-----------|----------|
| Vague request | "What decision will this analysis support?" |
| Understanding goals | "What does success look like for this project?" |
| Finding constraints | "Are there any limitations I should know about?" |
| Clarifying priority | "If you could only know one thing, what would it be?" |
| Understanding context | "What have you already tried?" |
| Timeline | "When do you need this, and why that date?" |

**The "5 Whys" for Requirements:**
When someone asks for something, dig deeper:
- "I need sales data" â†’ Why?
- "To see performance" â†’ Why performance?
- "We're missing targets" â†’ Why do you think?
- "Certain products are down" â†’ Which ones? Why those?
- "New competitor in that segment" â†’ So you need competitive analysis?

**Discovered Real Need**: Compare your products to competitor in that segment, not just "sales data"
                """,
                "key_points": ["Active listening finds the real problem", "Paraphrase to confirm understanding", "Ask 'why' to get to root needs", "What they ask for â‰  what they need"]
            },
            {
                "title": "Working with Different Roles",
                "content": """
**Collaborating Across Data and Business Teams**

Data analysts work with many different roles. Understanding their perspectives helps collaboration.

**Common Collaborators:**

| Role | Their Priority | What They Need From You |
|------|---------------|------------------------|
| **Executives** | Strategic decisions, ROI | High-level insights, recommendations |
| **Marketing** | Customer behavior, campaign performance | Segmentation, conversion metrics |
| **Sales** | Revenue, targets, forecasts | Pipeline analysis, trends |
| **Operations** | Efficiency, cost reduction | Process metrics, bottlenecks |
| **Product** | User experience, feature usage | Usage data, feedback analysis |
| **Finance** | Profitability, budgets | Cost analysis, forecasting |
| **IT/Engineering** | Data infrastructure, security | Data requirements, quality issues |

**Adapting Your Communication:**

**To Executives:**
- Bottom line first
- Focus on decisions, not details
- Quantify business impact
- 1-page max, with backup available

**To Technical Teams:**
- Can go deeper on methodology
- Discuss data quality openly
- Collaborate on implementation
- Share code/queries if helpful

**To Business Teams:**
- Focus on actionability
- Use their terminology
- Show how data supports their goals
- Be available for questions

**Building Relationships:**

1. **Understand their world**: Ask about their challenges and goals
2. **Speak their language**: Learn their terminology and metrics
3. **Be reliable**: Deliver on time, follow up promptly
4. **Educate gently**: Help them understand data without condescension
5. **Celebrate together**: Share wins as team achievements
6. **Accept feedback**: Be open to their domain expertise
                """,
                "key_points": ["Different roles have different priorities", "Adapt communication to audience", "Build relationships proactively", "Learn their terminology and goals"]
            },
            {
                "title": "Handling Feedback and Conflict",
                "content": """
**Receiving Feedback Professionally**

Feedback helps you grow. How you receive it matters as much as the feedback itself.

**Receiving Critical Feedback:**

| Step | Action |
|------|--------|
| 1. Listen | Let them finish without interrupting |
| 2. Clarify | Ask questions to understand fully |
| 3. Acknowledge | Thank them for the feedback |
| 4. Reflect | Consider it honestly before reacting |
| 5. Act | Make changes or explain your reasoning |

**What NOT to Do:**
- âŒ Get defensive ("But I...")
- âŒ Make excuses ("I didn't have time")
- âŒ Dismiss it ("They don't understand")
- âŒ Take it personally
- âŒ Argue immediately

**What TO Do:**
- âœ… "Thank you for pointing that out"
- âœ… "I hadn't considered that perspective"
- âœ… "Can you help me understand what you expected?"
- âœ… "I'll revisit that section and update it"

**Handling Disagreements:**

When someone disagrees with your analysis:

1. **Stay calm**: Take a breath, don't react emotionally
2. **Seek to understand**: "Can you tell me more about your concern?"
3. **Find common ground**: "We both want the best decision..."
4. **Use data**: Offer to investigate their hypothesis
5. **Know when to yield**: You might be wrong!
6. **Escalate thoughtfully**: If stuck, involve a neutral third party

**Common Conflict Scenarios:**

| Situation | Response |
|-----------|----------|
| "Your numbers are wrong" | "Let me verify the source and methodology" |
| "This isn't what I asked for" | "Help me understand what you were expecting" |
| "I don't believe this finding" | "What would convince you? Let me investigate" |
| "This analysis is useless" | "What would make it useful? Let's discuss" |

**Growth Mindset:**
Every piece of feedback is an opportunity to improve. Even harsh feedback usually contains some truth worth considering.
                """,
                "key_points": ["Listen fully before responding", "Thank people for feedback, even negative", "Stay calm in disagreements", "Use data to resolve conflicts"]
            }
        ],
        "exercises": [
            {
                "title": "Translate Technical to Plain Language",
                "type": "practical",
                "question": "Translate this to non-technical language: 'The regression analysis shows a statistically significant positive correlation (r=0.72, p<0.01) between marketing spend and customer acquisition.'",
                "answer": "Plain language: 'When we spend more on marketing, we get more customers - and this pattern is very consistent. For every $1 extra we spend on marketing, we typically gain [X] new customers. We're confident this is a real relationship, not just coincidence.' Note: Would be even better to add the actual business impact, e.g., 'Based on this, if we increased marketing budget by $50K, we'd expect to gain approximately 500 new customers.'",
                "hint": "Avoid statistical terms, focus on what it means for the business, and quantify the impact"
            },
            {
                "title": "Uncover the Real Need",
                "type": "scenario",
                "question": "A sales manager says: 'I need a report of all our customers sorted by revenue.' What questions would you ask to understand what they really need?",
                "answer": "Questions to ask: 1) 'What decision will this report help you make?' (Might reveal: identify at-risk accounts, find upsell opportunities, allocate account manager time). 2) 'What will you do with the top customers on this list?' (Reveals intended action). 3) 'Are you looking at a specific time period or trend?' (They might actually need trend data). 4) 'Should I include any other information like last contact date or satisfaction score?' (Might need a more complete view). 5) 'Is this a one-time analysis or something you'd need regularly?' (Informs automation). The real need might be: 'Help me identify my top 20 accounts at risk of churning so I can prioritize retention calls this week.'",
                "hint": "Focus on the decision they're trying to make and what action they'll take with the information"
            },
            {
                "title": "Handle Negative Feedback",
                "type": "scenario",
                "question": "Your stakeholder says: 'This analysis doesn't make sense. The conclusions don't match what we see on the ground.' How do you respond?",
                "answer": "Response approach: 1) STAY CALM - Don't get defensive. 2) ACKNOWLEDGE: 'I appreciate you sharing that concern. It's important that the analysis reflects reality.' 3) SEEK TO UNDERSTAND: 'Can you help me understand what you're seeing on the ground that doesn't match?' 4) COLLABORATE: 'Let's look at the data together - you might spot something I missed, or there might be context I need to understand.' 5) INVESTIGATE: 'Would it help if I checked [specific aspect they mention] to see if something was filtered incorrectly?' 6) COMMIT: 'I'll review the analysis with your feedback in mind and get back to you by [date].' Never say: 'You don't understand the data' or 'The numbers are correct, so you must be wrong.'",
                "hint": "Their ground-level experience is valuable data too - don't dismiss it"
            }
        ],
        "quiz": [
            {
                "question": "When a stakeholder asks for 'sales data,' you should:",
                "options": ["Send them the raw sales database", "Ask what decision they're trying to make", "Create every possible sales report", "Tell them to be more specific"],
                "correct": 1,
                "explanation": "Asking what decision they're trying to make helps uncover the real need. 'Sales data' could mean many things - understanding the purpose ensures you deliver what's actually useful."
            },
            {
                "question": "When translating 'statistically significant' for non-technical audiences, say:",
                "options": ["The p-value is below 0.05", "This is very important", "This pattern is unlikely to be random chance", "The correlation is strong"],
                "correct": 2,
                "explanation": "'Unlikely to be random chance' captures the meaning without jargon. 'Very important' is a common misunderstanding - statistical significance doesn't mean practical importance."
            },
            {
                "question": "When someone disagrees with your analysis, the first thing to do is:",
                "options": ["Defend your methodology", "Seek to understand their perspective", "Escalate to management", "Redo the entire analysis"],
                "correct": 1,
                "explanation": "First seek to understand their perspective - they may have valid information you don't have, or there may be a miscommunication you can resolve."
            },
            {
                "question": "Active listening involves:",
                "options": ["Waiting for your turn to speak", "Formulating your response while they talk", "Paraphrasing and asking clarifying questions", "Taking detailed notes"],
                "correct": 2,
                "explanation": "Active listening means fully focusing on understanding, demonstrated through paraphrasing ('So you're saying...') and asking clarifying questions."
            }
        ]
    },
    "KPIs & Decision Heuristics": {
        "course": "Evaluation of Outcomes",
        "description": "Learn to use Key Performance Indicators as heuristics for evaluating model outcomes and guiding decision-making.",
        "lessons": [
            {
                "title": "KPIs as Evaluation Heuristics",
                "content": """
**Using KPIs to Evaluate Model Outcomes**

KPIs serve as heuristics - mental shortcuts that help us quickly assess whether our analysis and models are performing well.

**What is a Heuristic?**
A heuristic is a practical rule of thumb that helps make decisions without analyzing every detail.

| Traditional Analysis | Heuristic Approach |
|---------------------|-------------------|
| Examine all metrics in detail | Focus on 3-5 key indicators |
| Time-consuming deep dives | Quick assessments |
| Risk of analysis paralysis | Faster decisions |
| Complete picture | "Good enough" picture |

**KPIs as Evaluation Heuristics:**

When evaluating model outcomes, KPIs help answer:
- Is the model working as expected?
- Are we achieving our objectives?
- Do we need to intervene?

**Example: Sales Forecasting Model**

| KPI Heuristic | Threshold | Action |
|---------------|-----------|--------|
| Forecast Error (MAPE) | < 10% = Good | > 15% = Investigate |
| Directional Accuracy | > 80% = Good | < 70% = Retrain model |
| Bias | Â±5% = Acceptable | > 10% = Check data sources |

**Building Your KPI Dashboard:**

```
MODEL HEALTH DASHBOARD
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Accuracy: 92% (target: >85%)
âš ï¸  False Positives: 12% (target: <10%)
âœ… Processing Time: 2.3s (target: <5s)
âŒ Data Freshness: 48h (target: <24h)

Overall Status: ATTENTION NEEDED
Priority Action: Update data pipeline
```

**The 80/20 Rule for KPIs:**
Focus on the vital few metrics that indicate 80% of the outcomes:
- Don't track 50 metrics - track 5-7 that matter most
- Each KPI should drive a specific action
- If a KPI doesn't change decisions, remove it
                """,
                "key_points": ["KPIs are heuristics for quick assessment", "Focus on 5-7 key metrics", "Each KPI should drive an action", "Use thresholds to trigger interventions"]
            },
            {
                "title": "Selecting KPIs for Model Evaluation",
                "content": """
**Choosing the Right KPIs for Your Models**

Different models require different evaluation KPIs.

**Model Types and Their Primary KPIs:**

| Model Type | Primary KPIs | Why These Matter |
|------------|--------------|------------------|
| **Classification** | Accuracy, Precision, Recall, F1 | Balance of correct predictions |
| **Regression** | MAE, RMSE, R-squared, MAPE | Prediction error magnitude |
| **Clustering** | Silhouette Score, Inertia | Cluster quality |
| **Time Series** | MAPE, Directional Accuracy | Forecast reliability |
| **Recommendation** | Hit Rate, Coverage, Diversity | User satisfaction |

**Classification KPIs Explained:**

```
CONFUSION MATRIX
                 Predicted
                 Pos    Neg
Actual  Pos     [TP]   [FN]
        Neg     [FP]   [TN]

Accuracy = (TP + TN) / Total
Precision = TP / (TP + FP)  â† "Of predicted positives, how many correct?"
Recall = TP / (TP + FN)     â† "Of actual positives, how many found?"
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**Which KPI to Prioritize?**

| Scenario | Priority KPI | Reasoning |
|----------|-------------|-----------|
| Medical diagnosis | Recall | Don't miss sick patients |
| Spam detection | Precision | Don't block legitimate emails |
| Fraud detection | F1 Score | Balance both errors |
| Customer churn | Recall | Catch at-risk customers |

**Regression KPIs Explained:**

```
MAE (Mean Absolute Error):
Average of |actual - predicted|
Interpretation: "On average, off by X units"

RMSE (Root Mean Square Error):
âˆš(average of (actual - predicted)Â²)
Interpretation: Penalizes large errors more

MAPE (Mean Absolute Percentage Error):
Average of |actual - predicted| / actual Ã— 100
Interpretation: "On average, off by X%"

R-squared:
1 - (Sum of squared errors / Total variance)
Interpretation: "Model explains X% of variance"
```

**Setting Realistic Targets:**
- Benchmark against industry standards
- Compare to baseline (naive) models
- Consider business impact of errors
                """,
                "key_points": ["Match KPIs to model type", "Prioritize based on business impact", "Understand precision vs recall tradeoff", "Set targets based on benchmarks"]
            },
            {
                "title": "KPI Thresholds and Alert Systems",
                "content": """
**Setting Thresholds and Automated Alerts**

Thresholds transform KPIs into actionable signals.

**Threshold Types:**

| Type | Example | Use Case |
|------|---------|----------|
| **Absolute** | Error rate < 5% | Fixed performance standard |
| **Relative** | 10% better than baseline | Improvement tracking |
| **Statistical** | Â±2 standard deviations | Anomaly detection |
| **Time-based** | Same as last quarter | Trend maintenance |

**RAG Status System:**

```
ðŸŸ¢ GREEN: On target, no action needed
ðŸŸ¡ AMBER: At risk, monitor closely
ðŸ”´ RED: Off target, immediate action required

Example Thresholds:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KPI             â”‚ Green  â”‚ Amber  â”‚ Red    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Accuracy        â”‚ >90%   â”‚ 80-90% â”‚ <80%   â”‚
â”‚ Processing Time â”‚ <2s    â”‚ 2-5s   â”‚ >5s    â”‚
â”‚ Data Freshness  â”‚ <1hr   â”‚ 1-4hr  â”‚ >4hr   â”‚
â”‚ Error Rate      â”‚ <2%    â”‚ 2-5%   â”‚ >5%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Alert Escalation Pyramid:**

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  CRITICAL   â”‚ â†’ Immediate escalation
         â”‚   (Red)     â”‚   Page on-call team
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚   WARNING   â”‚ â†’ Review within hours
         â”‚  (Amber)    â”‚   Email notification
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚    INFO     â”‚ â†’ Review in daily standup
         â”‚  (Green)    â”‚   Dashboard only
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avoiding Alert Fatigue:**

| Problem | Solution |
|---------|----------|
| Too many alerts | Prioritize by impact |
| False positives | Tune thresholds over time |
| Ignored alerts | Track response metrics |
| Duplicate alerts | Group related issues |

**Best Practices:**
1. Start with conservative thresholds, adjust over time
2. Every alert should have a clear response action
3. Review and refine thresholds quarterly
4. Include context in alerts (trend, comparison)
                """,
                "key_points": ["Use RAG status for quick assessment", "Every alert needs a response action", "Avoid alert fatigue with prioritization", "Review thresholds quarterly"]
            },
            {
                "title": "From KPIs to Decisions",
                "content": """
**Translating KPI Results into Business Decisions**

KPIs are only valuable if they drive decisions.

**Decision Framework:**

```
KPI RESULT â†’ INTERPRETATION â†’ OPTIONS â†’ DECISION â†’ ACTION
     â†“              â†“            â†“          â†“          â†“
  "What?"       "So what?"    "Now what?"  "Which?"   "How?"
```

**Example Decision Flow:**

| Stage | Example |
|-------|---------|
| **KPI Result** | Model accuracy dropped from 92% to 84% |
| **Interpretation** | 8% drop over 2 weeks, below 85% target |
| **Options** | 1) Retrain model 2) Check data quality 3) Accept degradation |
| **Decision** | Investigate data quality first (lowest cost) |
| **Action** | Run data validation checks by Friday |

**Decision Matrix for Model Issues:**

| Symptom | Possible Causes | First Action |
|---------|-----------------|--------------|
| Sudden accuracy drop | Data pipeline issue | Check data freshness/quality |
| Gradual accuracy decline | Concept drift | Evaluate retraining |
| High variance in predictions | Model instability | Review feature inputs |
| Consistent bias | Systematic error | Audit training data |

**Communicating Decisions:**

When presenting KPI-driven decisions:

1. **State the KPI**: "Customer churn prediction accuracy is at 78%"
2. **Provide context**: "Down from 85% last month, below our 80% threshold"
3. **Explain impact**: "We're missing 22% of at-risk customers"
4. **Propose action**: "Recommend retraining with Q4 data"
5. **Quantify benefit**: "Expected to recover 5% accuracy, saving ~$50K"

**Document Your Decisions:**
Keep a decision log linking KPIs to actions for:
- Learning from past decisions
- Justifying future investments
- Building institutional knowledge
                """,
                "key_points": ["KPIs must drive decisions, not just reports", "Use decision framework: What â†’ So What â†’ Now What", "Always quantify business impact", "Document decisions for future learning"]
            }
        ],
        "exercises": [
            {
                "title": "Design a KPI Dashboard",
                "type": "practical",
                "question": "You've built a customer churn prediction model. Design a KPI dashboard with 5 metrics, thresholds, and what action to take when each threshold is breached.",
                "answer": "Churn Model KPI Dashboard: 1) RECALL: Target >80%, Amber 70-80%, Red <70%. Action if Red: Review false negatives, possibly lower prediction threshold. 2) PRECISION: Target >60%, Amber 50-60%, Red <50%. Action if Red: Check for data quality issues causing false positives. 3) AUC-ROC: Target >0.75, Amber 0.65-0.75, Red <0.65. Action if Red: Retrain model with updated features. 4) PREDICTION TIMELINESS: Target <24hr before churn, Amber 24-48hr, Red >48hr. Action if Red: Increase scoring frequency. 5) INTERVENTION SUCCESS RATE: Target >30% saved, Amber 20-30%, Red <20%. Action if Red: Review intervention strategy, not just model.",
                "hint": "Include both model performance metrics and business outcome metrics"
            },
            {
                "title": "Interpret KPI Changes",
                "type": "scenario",
                "question": "Your sales forecast model shows: MAPE increased from 8% to 14%, but R-squared remained at 0.85. What does this combination tell you, and what should you investigate?",
                "answer": "Interpretation: MAPE measures average percentage error while R-squared measures explained variance. If R-squared stayed high but MAPE increased significantly, this suggests: 1) The model still captures the overall pattern (explains variance), 2) BUT individual predictions have larger errors (higher MAPE), 3) Likely cause: OUTLIERS or unusual data points are affecting predictions without distorting overall correlation. Investigation steps: 1) Check for outliers in recent data, 2) Look for specific segments with high errors, 3) Compare prediction errors by product/region/time, 4) Check if data distribution has shifted. This pattern often indicates concept drift in specific segments while overall relationships hold.",
                "hint": "Think about what each metric measures differently and what could affect one but not the other"
            }
        ],
        "quiz": [
            {
                "question": "A heuristic in decision-making is:",
                "options": ["A perfect algorithm", "A mental shortcut for quick decisions", "A detailed analysis method", "A type of database"],
                "correct": 1,
                "explanation": "A heuristic is a mental shortcut or rule of thumb that allows for quick decisions without analyzing every detail."
            },
            {
                "question": "For a medical diagnosis model, which KPI should be prioritized?",
                "options": ["Precision", "Recall", "Accuracy", "F1 Score"],
                "correct": 1,
                "explanation": "Recall is prioritized in medical diagnosis because missing a sick patient (false negative) is more dangerous than a false alarm (false positive)."
            },
            {
                "question": "What color in RAG status indicates 'at risk, monitor closely'?",
                "options": ["Red", "Amber", "Green", "Blue"],
                "correct": 1,
                "explanation": "Amber indicates 'at risk' status - not critical yet, but needs monitoring and may require action soon."
            },
            {
                "question": "When MAPE increases but R-squared stays the same, this likely indicates:",
                "options": ["Model is perfect", "Outliers affecting predictions", "Data is missing", "Model needs no changes"],
                "correct": 1,
                "explanation": "This pattern suggests outliers are causing larger individual prediction errors (MAPE) while overall patterns remain captured (R-squared)."
            }
        ]
    },
    "Statistical Result Analysis": {
        "course": "Evaluation of Outcomes",
        "description": "Master statistical inference techniques to evaluate and interpret model results including regression, variance, and z-testing.",
        "lessons": [
            {
                "title": "Interpreting Regression Results",
                "content": """
**Evaluating Linear Regression Outcomes**

Linear regression is fundamental for understanding relationships and making predictions.

**Key Regression Output Components:**

```
REGRESSION SUMMARY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependent Variable: Sales
R-squared: 0.847
Adjusted R-squared: 0.831

Coefficients:
                  Estimate    Std Error   t-value   p-value
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
(Intercept)       1250.00      125.50      9.96    <0.001 ***
Marketing_Spend      2.35        0.42      5.60    <0.001 ***
Price              -15.80        3.25     -4.86    <0.001 ***
Seasonality         45.20       12.30      3.67     0.002 **
```

**Interpreting Each Component:**

| Component | What It Tells You |
|-----------|------------------|
| **R-squared** | % of variance explained (0.847 = 84.7%) |
| **Adjusted RÂ²** | RÂ² adjusted for number of predictors |
| **Coefficient** | Change in Y for 1-unit change in X |
| **Std Error** | Uncertainty in coefficient estimate |
| **t-value** | Coefficient / Std Error (larger = more significant) |
| **p-value** | Probability result is due to chance |

**Reading the Example:**
- Marketing: $1 more spend â†’ $2.35 more sales
- Price: $1 higher price â†’ $15.80 less sales
- Model explains 84.7% of sales variation

**Common Interpretation Mistakes:**

| Mistake | Reality |
|---------|---------|
| High RÂ² = good model | Could be overfit |
| Low p-value = important | Statistical â‰  practical significance |
| Coefficient shows causation | Correlation only without experiments |
| Larger coefficient = more important | Depends on scale of variables |

**Practical Significance vs Statistical Significance:**

```
Example:
Coefficient = 0.002, p-value = 0.001

Statistically significant? YES (p < 0.05)
Practically significant? Maybe not - 
  $1000 increase in marketing â†’ only $2 more sales
```
                """,
                "key_points": ["R-squared shows variance explained", "Coefficients show relationship magnitude", "Low p-value â‰  practical importance", "Watch for overfitting with high RÂ²"]
            },
            {
                "title": "Analyzing Variance and Spread",
                "content": """
**Understanding Data Variance in Model Evaluation**

Variance tells you how spread out your data and predictions are.

**Measures of Variance:**

```
VARIANCE (ÏƒÂ²)
Average of squared deviations from mean

Standard Deviation (Ïƒ)
Square root of variance - same units as data

Coefficient of Variation (CV)
Standard deviation / Mean Ã— 100%
Use when comparing different scales
```

**Five-Point Summary (Box Plot Values):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Min   Q1   Median   Q3   Max           â”‚
â”‚   â†“     â†“      â†“      â†“    â†“            â”‚
â”‚  10    25     35     45   90            â”‚
â”‚                                          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”¤                 â”‚
â”‚     IQR = Q3 - Q1 = 20                   â”‚
â”‚                                          â”‚
â”‚  Outlier threshold:                      â”‚
â”‚  < Q1 - 1.5Ã—IQR = 25 - 30 = -5          â”‚
â”‚  > Q3 + 1.5Ã—IQR = 45 + 30 = 75          â”‚
â”‚  â†’ 90 is an outlier!                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Using Variance in Model Evaluation:**

| High Variance | Low Variance |
|---------------|--------------|
| Predictions spread widely | Predictions clustered |
| Model may be unreliable | Model more consistent |
| Check for missing features | May be too conservative |
| Could indicate concept drift | Good for stable domains |

**Variance in Prediction Errors:**

```
RESIDUAL ANALYSIS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Mean Residual: -0.5 (bias)
Std Dev Residuals: 12.3
Min: -45.2
Max: +38.7

Check:
âœ“ Mean near 0? (no systematic bias)
âœ“ Constant variance? (homoscedasticity)
âœ“ Normally distributed? (valid confidence intervals)
```

**ANOVA for Comparing Groups:**

When comparing model performance across groups:
- Hâ‚€: All group means are equal
- Hâ‚: At least one group differs
- Use F-statistic and p-value to decide
                """,
                "key_points": ["Five-point summary identifies outliers", "High variance may indicate model issues", "Residual variance should be constant", "Use ANOVA to compare group performance"]
            },
            {
                "title": "Z-Testing for Outcome Evaluation",
                "content": """
**Using Z-Tests to Evaluate Outcomes**

Z-tests help determine if observed results are statistically significant.

**When to Use Z-Test:**
- Large sample size (n > 30)
- Population standard deviation known
- Comparing sample mean to population mean
- Comparing two sample proportions

**Z-Test Formula:**

```
          Sample Mean - Population Mean
Z = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Population SD / âˆš(Sample Size)

          xÌ„ - Î¼
Z = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Ïƒ / âˆšn
```

**Interpreting Z-Scores:**

```
Z-Score Distribution:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”‚                    â”‚
    -3   -2   -1   0   +1   +2   +3
         â”‚                    â”‚
    â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€
         2.5%              97.5%
         
Common thresholds:
|Z| > 1.96 â†’ p < 0.05 (95% confidence)
|Z| > 2.58 â†’ p < 0.01 (99% confidence)
|Z| > 3.29 â†’ p < 0.001 (99.9% confidence)
```

**Example: Evaluating Model Performance**

```
Question: Is our new model significantly better?

Old model accuracy: 85% (population mean Î¼)
New model accuracy: 88% (sample mean xÌ„)
Standard deviation: 3%
Sample size: 100 tests

Z = (88 - 85) / (3 / âˆš100)
Z = 3 / 0.3
Z = 10

Interpretation:
Z = 10 >> 1.96
p << 0.001
Conclusion: New model is significantly better
```

**Two-Sample Z-Test for Proportions:**

Comparing conversion rates:
```
Group A: 250/1000 = 25% converted
Group B: 280/1000 = 28% converted

Pooled proportion p = 530/2000 = 26.5%
Standard Error = âˆš[p(1-p)(1/nâ‚ + 1/nâ‚‚)]
               = âˆš[0.265 Ã— 0.735 Ã— 0.002]
               = 0.0197

Z = (0.28 - 0.25) / 0.0197 = 1.52

|Z| < 1.96 â†’ NOT statistically significant
```
                """,
                "key_points": ["Z > 1.96 indicates statistical significance at 95%", "Use for large samples (n > 30)", "Compare means or proportions", "Statistical significance â‰  practical importance"]
            },
            {
                "title": "Sampled Sets and Inference",
                "content": """
**Drawing Conclusions from Sample Data**

In practice, we evaluate models on sample data and infer population performance.

**Key Sampling Concepts:**

| Term | Meaning |
|------|---------|
| **Population** | The entire group you want to understand |
| **Sample** | Subset you actually measure |
| **Sampling Error** | Difference between sample and population |
| **Confidence Interval** | Range likely containing true value |

**Sample Size and Reliability:**

```
MARGIN OF ERROR
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           z Ã— Ïƒ
MOE = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          âˆšn

For 95% confidence (z = 1.96):
n = 100  â†’ MOE = 0.196Ïƒ
n = 400  â†’ MOE = 0.098Ïƒ
n = 1000 â†’ MOE = 0.062Ïƒ

Quadrupling sample size halves the margin of error!
```

**Confidence Intervals:**

```
INTERPRETING CONFIDENCE INTERVALS

Model accuracy: 87% Â± 3% (95% CI)

This means:
âœ“ Sample accuracy is 87%
âœ“ 95% confident true accuracy is between 84-90%
âœ— Does NOT mean 95% probability the true value is in range
   (it either is or isn't!)
```

**Comparing Two Models with Confidence Intervals:**

```
Model A: 85% (82-88)
Model B: 88% (84-92)
                    â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
                 â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
         80  82  84  86  88  90  92

Intervals overlap!
â†’ Cannot conclude B is significantly better
â†’ Need more data or different test
```

**Cross-Validation for Robust Estimates:**

```
K-FOLD CROSS-VALIDATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Split data into K folds (e.g., K=5)

Fold 1: [Test] [Train] [Train] [Train] [Train] â†’ Acc: 86%
Fold 2: [Train] [Test] [Train] [Train] [Train] â†’ Acc: 88%
Fold 3: [Train] [Train] [Test] [Train] [Train] â†’ Acc: 84%
Fold 4: [Train] [Train] [Train] [Test] [Train] â†’ Acc: 87%
Fold 5: [Train] [Train] [Train] [Train] [Test] â†’ Acc: 85%

Mean Accuracy: 86%
Std Dev: 1.6%
CI: 86% Â± 1.4% (based on std error)
```

**Avoiding Sampling Bias:**
- Random sampling for representative data
- Stratified sampling for balanced groups
- Time-based splits for time series
- Never test on training data!
                """,
                "key_points": ["Larger samples reduce margin of error", "Confidence intervals show uncertainty", "Overlapping CIs suggest no significant difference", "Use cross-validation for robust estimates"]
            }
        ],
        "exercises": [
            {
                "title": "Interpret Regression Output",
                "type": "practical",
                "question": "A regression shows: RÂ² = 0.72, Marketing coefficient = 1.8 (p=0.003), Price coefficient = -12.5 (p=0.41). What can you conclude about each variable's impact on sales?",
                "answer": "Interpretation: 1) RÂ² = 0.72: The model explains 72% of sales variation - reasonably good but 28% unexplained. 2) Marketing (coef=1.8, p=0.003): Statistically significant (p<0.05). Each $1 in marketing increases sales by $1.80. Strong evidence of positive relationship. 3) Price (coef=-12.5, p=0.41): NOT statistically significant (p>0.05). We cannot confidently say price affects sales based on this data. The negative coefficient suggests higher price = lower sales, but the relationship could be due to chance. Recommendation: Keep marketing in model, consider removing price or collecting more data to detect effect.",
                "hint": "Look at both coefficient direction/magnitude AND statistical significance (p-value)"
            },
            {
                "title": "Calculate and Interpret Z-Score",
                "type": "practical",
                "question": "Your A/B test shows: Control group 1000 users, 12% conversion. Test group 1000 users, 15% conversion. Is the difference statistically significant at 95% confidence?",
                "answer": "Calculation: Pooled proportion p = (120 + 150) / 2000 = 0.135. Standard Error SE = âˆš[0.135 Ã— 0.865 Ã— (1/1000 + 1/1000)] = âˆš[0.1167 Ã— 0.002] = âˆš0.000234 = 0.0153. Z = (0.15 - 0.12) / 0.0153 = 0.03 / 0.0153 = 1.96. Interpretation: Z = 1.96 is exactly at the 95% confidence threshold. This is borderline significant. Technically, |Z| â‰¥ 1.96 means p â‰¤ 0.05, so this JUST reaches significance. However, I would recommend: 1) Running the test longer for more certainty, 2) Calculating exact p-value (it's about 0.05), 3) Considering practical significance: 3% lift = 30 extra conversions per 1000 users.",
                "hint": "Use the two-proportion z-test formula with pooled proportion"
            }
        ],
        "quiz": [
            {
                "question": "In regression, a high R-squared value means:",
                "options": ["The model is perfect", "The model explains most of the variance", "All coefficients are significant", "The model cannot be overfit"],
                "correct": 1,
                "explanation": "R-squared indicates the percentage of variance explained by the model. High RÂ² means the model captures most of the variation, but doesn't guarantee it's not overfit."
            },
            {
                "question": "A coefficient is statistically significant when:",
                "options": ["It is large", "The p-value is less than 0.05", "R-squared is high", "The t-value is less than 1"],
                "correct": 1,
                "explanation": "Statistical significance is determined by p-value. P < 0.05 (at 95% confidence) means the relationship is unlikely due to chance."
            },
            {
                "question": "The IQR in a five-point summary is:",
                "options": ["Maximum minus minimum", "Q3 minus Q1", "Mean minus median", "Standard deviation times 2"],
                "correct": 1,
                "explanation": "IQR (Interquartile Range) = Q3 - Q1. It represents the middle 50% of the data and is used to identify outliers."
            },
            {
                "question": "What does |Z| > 1.96 indicate at 95% confidence?",
                "options": ["Result is not significant", "Result is statistically significant", "Sample is too small", "Data is normally distributed"],
                "correct": 1,
                "explanation": "Z-score magnitude greater than 1.96 indicates statistical significance at the 95% confidence level (p < 0.05)."
            }
        ]
    },
    "Confidence Levels & Scenarios": {
        "course": "Evaluation of Outcomes",
        "description": "Learn to work with confidence levels and create multiple outcome scenarios for data-driven decision making.",
        "lessons": [
            {
                "title": "Understanding Confidence Levels",
                "content": """
**Confidence Levels in Statistical Analysis**

Confidence levels quantify our certainty about results and predictions.

**What is a Confidence Level?**

The probability that a confidence interval contains the true population value.

| Confidence Level | Z-Score | Interpretation |
|-----------------|---------|----------------|
| 90% | 1.645 | 90% of similar intervals contain true value |
| 95% | 1.960 | 95% of similar intervals contain true value |
| 99% | 2.576 | 99% of similar intervals contain true value |

**The Tradeoff:**

```
HIGHER CONFIDENCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           Certainty
              â†‘
           99%|  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  Wide interval
              |                        (less precise)
           95%|  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
              |
           90%|  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        Narrow interval
              |                        (more precise)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
                      Width
```

**Calculating Confidence Intervals:**

```
CI = Point Estimate Â± (Z Ã— Standard Error)

Example: Survey results
Sample mean: 72%
Standard error: 2%

90% CI: 72% Â± (1.645 Ã— 2%) = 68.7% to 75.3%
95% CI: 72% Â± (1.960 Ã— 2%) = 68.1% to 75.9%
99% CI: 72% Â± (2.576 Ã— 2%) = 66.8% to 77.2%
```

**Choosing the Right Confidence Level:**

| Situation | Recommended Level | Why |
|-----------|------------------|-----|
| **High-stakes decisions** | 99% | Minimize risk of wrong conclusion |
| **Standard analysis** | 95% | Industry convention, good balance |
| **Exploratory research** | 90% | Acceptable for initial findings |
| **Life-critical** | 99.9%+ | Medical, safety applications |

**Common Misinterpretation:**

âŒ "There's a 95% chance the true value is in this range"
âœ… "If we repeated this study 100 times, ~95 intervals would contain the true value"
                """,
                "key_points": ["Higher confidence = wider intervals", "95% is the standard convention", "Choose level based on decision stakes", "CI interpretation is about the method, not this specific interval"]
            },
            {
                "title": "Building Probability Scenarios",
                "content": """
**Creating Multiple Outcome Scenarios**

Scenarios help decision-makers understand the range of possible outcomes.

**Scenario Framework:**

| Scenario | Probability | Characteristics |
|----------|-------------|-----------------|
| **Best Case** | 10-15% | Everything goes right |
| **Optimistic** | 20-25% | Most things go well |
| **Base Case** | 50% | Expected outcome |
| **Pessimistic** | 20-25% | Some challenges |
| **Worst Case** | 10-15% | Major problems |

**Building Scenarios from Data:**

```
REVENUE FORECAST SCENARIOS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Based on historical data and model predictions:

                    Revenue    Probability
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Worst Case          $800K        10%
Pessimistic         $950K        20%
Base Case         $1,100K        40%
Optimistic        $1,250K        20%
Best Case         $1,400K        10%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Expected Value = Î£(Probability Ã— Outcome)
             = (0.1Ã—800) + (0.2Ã—950) + (0.4Ã—1100) + (0.2Ã—1250) + (0.1Ã—1400)
             = $1,090K
```

**Monte Carlo Simulation:**

For complex scenarios with many variables:

```
STEPS:
1. Define probability distributions for each variable
2. Randomly sample from each distribution
3. Calculate outcome for this combination
4. Repeat 1000+ times
5. Analyze distribution of outcomes

Example: Profit Forecast
â”œâ”€â”€ Sales: Normal(1000, 100)
â”œâ”€â”€ Cost: Uniform(400, 500)
â””â”€â”€ Price: Triangular(45, 50, 60)

After 10,000 simulations:
Mean Profit: $52,000
5th percentile: $38,000 (95% confidence lower bound)
95th percentile: $67,000 (95% confidence upper bound)
```

**Sensitivity Analysis:**

Which variables most impact outcomes?

```
TORNADO DIAGRAM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Variable        Impact on Profit
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sales Volume    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  Â±$25K
Price           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        Â±$18K
Material Cost   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             Â±$12K
Labor Cost      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               Â±$9K
Marketing       â–ˆâ–ˆâ–ˆâ–ˆ                  Â±$5K
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Focus on: Sales Volume and Price
```
                """,
                "key_points": ["Use scenarios to show range of possibilities", "Expected value weights outcomes by probability", "Monte Carlo handles complex uncertainty", "Sensitivity analysis identifies key variables"]
            },
            {
                "title": "Decision Making Under Uncertainty",
                "content": """
**Making Decisions with Incomplete Information**

Most business decisions involve uncertainty. Here's how to navigate it.

**Decision Framework with Confidence:**

```
1. IDENTIFY OPTIONS
   What actions can we take?

2. ESTIMATE OUTCOMES
   What might happen with each option?
   What's the confidence level?

3. ASSESS PROBABILITIES
   How likely is each outcome?

4. EVALUATE EXPECTED VALUES
   Expected Value = Î£(Probability Ã— Outcome)

5. CONSIDER RISK TOLERANCE
   Can we afford the worst case?

6. DECIDE AND DOCUMENT
   Make choice, record reasoning
```

**Expected Value vs Risk Tolerance:**

```
Option A: 100% chance of $50,000
Option B: 50% chance of $120,000, 50% chance of $0

Expected Values:
A: $50,000
B: 0.5 Ã— $120,000 + 0.5 Ã— $0 = $60,000

B has higher expected value, BUT:
- If you can't afford to lose, choose A
- If you can absorb the loss, B might be better
```

**Decision Trees with Probabilities:**

```
                    â”Œâ”€â”€ Success (70%): +$100K
Launch New Model â”€â”€â”€â”¤
        â”‚           â””â”€â”€ Failure (30%): -$40K
        â”‚           EV = 0.7Ã—100 + 0.3Ã—(-40) = $58K
        â”‚
        â”‚           â”Œâ”€â”€ Market Grows (40%): +$30K
Wait and See â”€â”€â”€â”€â”€â”€â”€â”¤
                    â””â”€â”€ Market Flat (60%): +$10K
                    EV = 0.4Ã—30 + 0.6Ã—10 = $18K

Decision: Launch (higher expected value)
```

**Communicating Uncertainty:**

| Bad Communication | Better Communication |
|------------------|---------------------|
| "Revenue will be $1M" | "Revenue expected $900K-1.1M (95% CI)" |
| "The model is accurate" | "Model accuracy: 87% Â± 3%" |
| "This will definitely work" | "70% confidence in success" |
| "We'll probably hit target" | "65% probability of hitting Q4 target" |

**Document Your Assumptions:**
Always record:
- What confidence level you used
- What scenarios you considered
- What you assumed about probabilities
- What would change your decision
                """,
                "key_points": ["Expected value helps compare options", "Consider risk tolerance, not just expected value", "Decision trees visualize choices and outcomes", "Communicate uncertainty explicitly"]
            },
            {
                "title": "Probability Bounds and Ranges",
                "content": """
**Setting Probability Bounds for Model Outcomes**

Providing ranges instead of point estimates improves decision quality.

**Types of Uncertainty Bounds:**

| Type | Use Case | Example |
|------|----------|---------|
| **Confidence Interval** | Statistical uncertainty | Mean Â± margin of error |
| **Prediction Interval** | Individual prediction | Wider than CI |
| **Credible Interval** | Bayesian analysis | Probability of parameter |
| **Tolerance Interval** | Process variation | Captures X% of population |

**Confidence vs Prediction Intervals:**

```
REGRESSION EXAMPLE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Data: Marketing spend vs Sales

Confidence Interval (95%):
"Average sales at $50K marketing = $120K Â± $8K"
â†’ Uncertainty about the MEAN

Prediction Interval (95%):
"A specific company spending $50K = $120K Â± $25K"
â†’ Uncertainty about INDIVIDUAL outcome

Prediction intervals are ALWAYS wider!
```

**Percentile Ranges:**

```
SALES FORECAST DISTRIBUTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    P10     P25    P50    P75    P90
   (10%)   (25%)  (50%)  (75%)  (90%)
     â”‚       â”‚      â”‚      â”‚      â”‚
    $80K   $95K  $110K  $125K  $140K
     
"50% chance sales between $95K-$125K (P25-P75)"
"90% chance sales between $80K-$140K (P10-P90)"
```

**Communicating Ranges to Stakeholders:**

**For Executives:**
"We expect Q4 revenue of $2.1M, with 80% confidence between $1.9M and $2.3M"

**For Operations:**
"Daily transactions: expect 450, plan capacity for up to 600 (95th percentile)"

**For Risk Management:**
"Value at Risk (95%): Maximum single-day loss of $125K"

**When to Use Each:**

| Audience Needs | Provide |
|---------------|---------|
| Planning for average | Point estimate + confidence interval |
| Planning for specific case | Prediction interval |
| Capacity planning | Upper percentile (90th, 95th) |
| Risk management | Lower percentile for upside, VaR for downside |
| Scenario planning | Multiple percentile ranges |
                """,
                "key_points": ["Prediction intervals are wider than confidence intervals", "Use percentiles for capacity and risk planning", "Match the bound type to the decision need", "Always communicate the confidence level"]
            }
        ],
        "exercises": [
            {
                "title": "Create Outcome Scenarios",
                "type": "practical",
                "question": "Your sales forecast model predicts $500K revenue with standard error of $50K. Create 5 scenarios with probabilities and calculate expected value.",
                "answer": "Using normal distribution principles: Worst Case ($350K, -3Ïƒ): 2% probability. Pessimistic ($400K, -2Ïƒ): 13% probability. Base Case ($500K, mean): 50% probability. Optimistic ($600K, +2Ïƒ): 13% probability. Best Case ($650K, +3Ïƒ): 2% probability. Note: Remaining 20% distributed around other outcomes. Expected Value calculation: (0.02 Ã— 350) + (0.13 Ã— 400) + (0.50 Ã— 500) + (0.13 Ã— 600) + (0.02 Ã— 650) + (0.20 Ã— 500) = 7 + 52 + 250 + 78 + 13 + 100 = $500K. The expected value equals the mean, as expected for a symmetric distribution. For planning, budget conservatively at $400K (pessimistic) while targeting $600K (optimistic).",
                "hint": "Use standard deviations to define scenarios and normal distribution probabilities"
            },
            {
                "title": "Choose Confidence Level",
                "type": "scenario",
                "question": "You're recommending a drug dosage algorithm for a hospital. What confidence level should you use and why? What if you were recommending a marketing email subject line instead?",
                "answer": "Drug Dosage Algorithm: Use 99.9% or higher confidence level. Reasoning: 1) Patient safety is paramount - wrong dosage could be fatal. 2) False positives/negatives have severe consequences. 3) Medical standards require extremely high certainty. 4) Would need extensive clinical validation. 5) Consider prediction intervals, not just confidence intervals. Marketing Email Subject Line: 90-95% confidence is appropriate. Reasoning: 1) Low-stakes decision - worst case is slightly lower open rate. 2) Can easily A/B test and iterate. 3) Cost of being wrong is minimal. 4) Speed of decision may be more valuable than precision. 5) 90% allows faster conclusions with smaller sample sizes. The key principle: match confidence level to the consequences of being wrong.",
                "hint": "Consider the consequences of being wrong in each situation"
            }
        ],
        "quiz": [
            {
                "question": "A 99% confidence interval is wider than a 95% interval because:",
                "options": ["It uses more data", "Higher certainty requires more range", "The sample is larger", "The calculation is different"],
                "correct": 1,
                "explanation": "To be more confident that the true value is captured, the interval must be wider. There's a tradeoff between precision and confidence."
            },
            {
                "question": "Expected value is calculated by:",
                "options": ["Taking the average outcome", "Multiplying each outcome by its probability and summing", "Choosing the most likely outcome", "Subtracting worst from best case"],
                "correct": 1,
                "explanation": "Expected Value = Î£(Probability Ã— Outcome). It weights each possible outcome by its likelihood."
            },
            {
                "question": "A prediction interval is wider than a confidence interval because:",
                "options": ["It uses less data", "It accounts for individual variation, not just mean uncertainty", "The formula is different", "It has lower confidence"],
                "correct": 1,
                "explanation": "Prediction intervals account for both uncertainty about the mean AND variation of individual observations around that mean."
            },
            {
                "question": "For a life-critical medical decision, you should use:",
                "options": ["90% confidence", "95% confidence", "99%+ confidence", "No confidence interval needed"],
                "correct": 2,
                "explanation": "Life-critical decisions require very high confidence levels (99% or higher) because the cost of being wrong is extremely high."
            }
        ]
    },
    "Iterative Error Elimination": {
        "course": "Evaluation of Outcomes",
        "description": "Master systematic approaches to identify, debug, and eliminate errors in data models and analysis results.",
        "lessons": [
            {
                "title": "The Error Elimination Process",
                "content": """
**Systematic Debugging of Data Models**

Iterative error elimination is a structured approach to finding and fixing problems.

**The Debugging Cycle:**

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                      â”‚
    â†“                                      â”‚
DETECT â†’ DIAGNOSE â†’ FIX â†’ VERIFY â†’ PREVENT
   â”‚                                   â”‚
   â”‚   â†â”€â”€â”€â”€ If new errors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â””â†’ Document and close
```

**Step 1: DETECT - Finding Errors**

| Detection Method | What It Catches |
|-----------------|-----------------|
| **Automated tests** | Known failure patterns |
| **Monitoring/alerts** | Performance degradation |
| **Manual review** | Logic and interpretation errors |
| **User feedback** | Real-world failures |
| **Comparison to baseline** | Unexpected deviations |

**Step 2: DIAGNOSE - Root Cause Analysis**

**The 5 Whys Technique:**
```
Problem: Model accuracy dropped 10%

Why #1: Predictions are wrong for new customers
Why #2: New customer features have missing values
Why #3: Data pipeline doesn't handle new data source
Why #4: New data source format wasn't documented
Why #5: No validation check for source changes

Root Cause: Missing data validation in pipeline
```

**Step 3: FIX - Implementing Solutions**

| Fix Type | When to Use | Risk Level |
|----------|-------------|------------|
| **Hotfix** | Critical production issue | High - test quickly |
| **Standard fix** | Normal bugs | Medium - full testing |
| **Refactor** | Systemic issues | Low - plan carefully |

**Step 4: VERIFY - Confirming the Fix**

```
VERIFICATION CHECKLIST
â˜‘ Error no longer occurs
â˜‘ Related functionality still works
â˜‘ Performance not degraded
â˜‘ Edge cases tested
â˜‘ Rollback plan ready
```

**Step 5: PREVENT - Stopping Recurrence**

- Add automated test for this error
- Update documentation
- Share learnings with team
- Consider systemic improvements
                """,
                "key_points": ["Follow systematic cycle: Detect â†’ Diagnose â†’ Fix â†’ Verify â†’ Prevent", "Use 5 Whys to find root cause", "Always verify fix and add prevention", "Document for future reference"]
            },
            {
                "title": "Common Error Types in Data Models",
                "content": """
**Recognizing and Fixing Model Errors**

Understanding error types helps you diagnose problems faster.

**Data Errors:**

| Error Type | Symptoms | Fix |
|-----------|----------|-----|
| **Missing values** | NaN in predictions | Imputation or filtering |
| **Outliers** | Extreme predictions | Detection and handling |
| **Data leakage** | Too-good validation scores | Review feature engineering |
| **Stale data** | Accuracy degradation | Update data pipeline |
| **Labeling errors** | Inconsistent patterns | Audit training labels |

**Model Errors:**

| Error Type | Symptoms | Fix |
|-----------|----------|-----|
| **Overfitting** | High train, low test accuracy | Regularization, more data |
| **Underfitting** | Low accuracy everywhere | More features, complex model |
| **Bias** | Systematic over/under prediction | Calibration, balanced training |
| **Concept drift** | Accuracy drops over time | Retrain, monitor distributions |
| **Feature importance shift** | Unexpected predictions | Review feature correlations |

**Process Errors:**

```
COMMON PROCESS MISTAKES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Training on test data (data leakage)
   â†’ Use proper train/test splits

2. Not versioning data and models
   â†’ Implement version control

3. Ignoring edge cases
   â†’ Add boundary testing

4. Hardcoding thresholds
   â†’ Make configurable

5. No monitoring in production
   â†’ Set up alerts and dashboards
```

**Error Severity Classification:**

| Severity | Impact | Response Time | Example |
|----------|--------|---------------|---------|
| **Critical** | System down | < 1 hour | Model returns no predictions |
| **High** | Major incorrect results | < 4 hours | 50% accuracy drop |
| **Medium** | Partial degradation | < 1 day | Some features not working |
| **Low** | Minor issues | < 1 week | Cosmetic or edge cases |

**Building Error Intuition:**
- Most errors are in data, not code
- Recent changes are usually the cause
- Simple errors are most common
- Edge cases reveal hidden bugs
                """,
                "key_points": ["Data errors are most common", "Overfitting = high train, low test accuracy", "Concept drift causes gradual degradation", "Classify severity to prioritize fixes"]
            },
            {
                "title": "Debugging Techniques for Analysts",
                "content": """
**Practical Debugging Methods**

When your model isn't working, use these techniques to find the problem.

**Technique 1: Sanity Checks**

```
SANITY CHECK SEQUENCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Check data shapes and types
   - Expected rows/columns?
   - Data types correct?

2. Check for nulls and infinities
   - df.isnull().sum()
   - np.isinf(values).sum()

3. Check value ranges
   - Are values within expected bounds?
   - Any impossible values?

4. Check distributions
   - Did the distribution change?
   - Compare to historical data

5. Check predictions
   - Are predictions in valid range?
   - Do they pass business logic?
```

**Technique 2: Bisection (Divide and Conquer)**

```
If the problem is somewhere in your pipeline:

1. Test at midpoint
   Data â†’ [Process A] â†’ [Process B] â†’ [Model] â†’ Output
                          â†‘
                     Check here first

2. If good at midpoint, problem is in second half
   If bad at midpoint, problem is in first half

3. Repeat until you find the exact step
```

**Technique 3: Minimal Reproducible Example**

```
Reduce the problem to its smallest form:

Instead of:
"The model doesn't work on the full dataset"

Create:
"Row 47 with these exact values produces error X"

Steps:
1. Start with the failing case
2. Remove data/features one by one
3. Find the minimal set that still fails
4. Debug that specific case
```

**Technique 4: Comparison Testing**

| Compare | To Find |
|---------|---------|
| Current vs previous version | What changed |
| Production vs development | Environment issues |
| Sample vs full data | Data-specific bugs |
| Simple vs complex model | Model-related issues |

**Technique 5: Logging and Tracing**

```
Add strategic logging:

def predict(data):
    log("Input shape: " + str(data.shape))
    log("Input sample: " + str(data.head()))
    
    processed = preprocess(data)
    log("After preprocessing: " + str(processed.shape))
    
    result = model.predict(processed)
    log("Output range: " + str(result.min(), result.max()))
    
    return result
```
                """,
                "key_points": ["Start with sanity checks on data", "Use bisection to narrow down the problem", "Create minimal reproducible examples", "Add logging to trace issues"]
            },
            {
                "title": "Building Robust Error Prevention",
                "content": """
**Preventing Errors Before They Happen**

The best error is one that never reaches production.

**Defensive Programming Principles:**

| Principle | Implementation |
|-----------|---------------|
| **Validate inputs** | Check all data before processing |
| **Fail fast** | Error early rather than propagate |
| **Explicit defaults** | Don't rely on implicit behavior |
| **Boundary testing** | Test edge cases explicitly |
| **Assertion checks** | Verify assumptions in code |

**Data Validation Framework:**

```
INPUT VALIDATION CHECKLIST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â˜ Schema validation (correct columns/types)
â˜ Value range checks (min/max bounds)
â˜ Null/missing checks (acceptable levels)
â˜ Uniqueness checks (duplicate handling)
â˜ Referential integrity (foreign keys valid)
â˜ Business logic (domain-specific rules)

Example checks:
assert df['age'].between(0, 120).all()
assert df['email'].str.contains('@').all()
assert df['revenue'] >= 0
```

**Automated Testing Layers:**

```
TEST PYRAMID
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          /\\
         /  \\     E2E Tests
        /    \\    (few, slow, expensive)
       /â”€â”€â”€â”€â”€â”€\\
      /        \\  Integration Tests
     /          \\ (some, medium speed)
    /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\
   /              \\ Unit Tests
  /                \\ (many, fast, cheap)
 /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\
```

**Continuous Monitoring:**

```
PRODUCTION MONITORING DASHBOARD
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ“Š Model Performance
   â””â”€â”€ Accuracy: 87% âœ“ (threshold: >85%)
   â””â”€â”€ Latency: 120ms âœ“ (threshold: <200ms)
   â””â”€â”€ Error Rate: 0.1% âœ“ (threshold: <1%)

ðŸ“ˆ Data Quality
   â””â”€â”€ Null Rate: 0.5% âœ“ (threshold: <2%)
   â””â”€â”€ Distribution Drift: 0.02 âœ“ (threshold: <0.05)
   â””â”€â”€ Volume: 10,234 records âœ“ (expected: 8K-12K)

ðŸš¨ Alerts (last 24h)
   â””â”€â”€ No critical alerts
   â””â”€â”€ 1 warning: Latency spike at 14:00
```

**Post-Mortem Process:**

After any significant error:
1. **Timeline**: What happened when?
2. **Root cause**: Why did it happen?
3. **Impact**: Who/what was affected?
4. **Resolution**: How was it fixed?
5. **Prevention**: How do we prevent recurrence?
6. **Action items**: Specific tasks with owners
                """,
                "key_points": ["Validate inputs before processing", "Build automated testing pyramid", "Monitor continuously in production", "Conduct post-mortems to learn and prevent"]
            },
            {
                "title": "Ethical Model Evaluation",
                "content": """
**Responsible Assessment and Ethical Critique of Models**

Evaluating outcomes isn't just technical - it requires ethical judgment and responsibility.

**Ethical Evaluation Framework:**

| Dimension | Questions to Ask |
|-----------|-----------------|
| **Fairness** | Does the model treat all groups equitably? |
| **Transparency** | Can we explain how decisions are made? |
| **Accountability** | Who is responsible for outcomes? |
| **Privacy** | Is personal data protected appropriately? |
| **Harm** | Could this model cause harm to individuals? |

**Bias Detection in Model Outcomes:**

```
BIAS AUDIT CHECKLIST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â˜ Performance by demographic group
   â””â”€â”€ Accuracy, precision, recall by age, gender, location

â˜ Outcome distribution fairness
   â””â”€â”€ Are approval/rejection rates proportional?

â˜ Feature importance review
   â””â”€â”€ Are protected characteristics influencing decisions?

â˜ Historical bias in training data
   â””â”€â”€ Does past data encode unfair practices?

â˜ Proxy variable detection
   â””â”€â”€ Do features correlate with protected groups?
```

**Types of Algorithmic Bias:**

| Bias Type | Example | Detection |
|-----------|---------|-----------|
| **Selection bias** | Training data not representative | Compare data vs population |
| **Historical bias** | Past discrimination encoded | Audit historical decisions |
| **Measurement bias** | Errors differ by group | Error analysis by segment |
| **Aggregation bias** | One model for diverse groups | Subgroup performance testing |

**Ethical Decision-Making Process:**

```
1. ASSESS THE IMPACT
   Who is affected by this model's decisions?
   What are the consequences of errors?

2. CHECK FOR FAIRNESS
   Analyze performance across demographic groups
   Identify any disparate impact

3. ENSURE TRANSPARENCY
   Can we explain decisions to affected individuals?
   Is the logic understandable?

4. CONSIDER ALTERNATIVES
   If the model has issues, what are the options?
   Is using a model appropriate for this decision?

5. DOCUMENT AND COMMUNICATE
   Record ethical considerations
   Share concerns with stakeholders
```

**Presenting Outcomes Responsibly:**

| Bad Practice | Ethical Practice |
|-------------|------------------|
| Hide limitations | Acknowledge model limitations clearly |
| Ignore edge cases | Report performance on minority groups |
| Overstate confidence | Present uncertainty honestly |
| Blame the data | Take responsibility for model choices |
| Rush to production | Allow time for ethical review |

**When to Raise Concerns:**

You have a professional obligation to speak up when:
- Model could cause significant harm
- Bias is detected but not addressed
- Stakeholders ignore ethical risks
- Regulations or policies are violated
- You're asked to suppress unfavorable findings

**Escalation Path:**
1. Raise with immediate team/manager
2. Document concerns in writing
3. Escalate to ethics/compliance if unresolved
4. Consider professional obligations (varies by industry)
                """,
                "key_points": ["Evaluate models for fairness across groups", "Check for bias in training data and outcomes", "Present limitations and uncertainties honestly", "Raise ethical concerns when you see them"]
            }
        ],
        "exercises": [
            {
                "title": "Apply 5 Whys Analysis",
                "type": "practical",
                "question": "Your customer churn model suddenly started predicting everyone as 'will churn.' Apply the 5 Whys technique to find the root cause.",
                "answer": "5 Whys Analysis: Why #1: Model predicts 'churn' for all customers â†’ Because all prediction probabilities are above threshold. Why #2: Why are all probabilities high? â†’ Because one feature has extreme values for all records. Why #3: Why does that feature have extreme values? â†’ The feature is customer tenure, and all values show 0 days. Why #4: Why is tenure showing 0 for everyone? â†’ The data pipeline is calculating tenure from an empty 'signup_date' field. Why #5: Why is signup_date empty? â†’ A recent database migration removed the date format, and the ETL couldn't parse it. ROOT CAUSE: Database migration broke date parsing in ETL pipeline. FIX: 1) Immediate: Restore from backup or fix date format. 2) Prevention: Add data validation to check for reasonable tenure values before model runs.",
                "hint": "Start with what's directly wrong, then ask 'why' at each level until you reach a systemic cause"
            },
            {
                "title": "Design Validation Checks",
                "type": "practical",
                "question": "Design 5 validation checks for a model that predicts loan approval (features: income, credit_score, debt_ratio, employment_years, loan_amount).",
                "answer": "Validation checks: 1) INCOME: Assert income >= 0 and income < 10,000,000 (reasonable bounds). Flag if null or negative. 2) CREDIT_SCORE: Assert credit_score between 300 and 850 (valid FICO range). Reject if outside range - this is a data error. 3) DEBT_RATIO: Assert debt_ratio between 0 and 1 (or 0-100%). If > 1, check if percentage vs decimal format. 4) EMPLOYMENT_YEARS: Assert employment_years >= 0 and < 60. If > person's age minus 16, flag as likely error. 5) LOAN_AMOUNT: Assert loan_amount > 0 and < 10 Ã— income (reasonable debt-to-income). Also check loan_amount / income ratio is within lending policy limits. ADDITIONAL: Check for nulls in any required field. Validate referential integrity of customer_id. Log any records that fail validation for review.",
                "hint": "Consider valid ranges, business logic rules, and relationships between fields"
            },
            {
                "title": "Ethical Bias Assessment",
                "type": "scenario",
                "question": "Your hiring recommendation model has 85% overall accuracy, but you discover it rejects 60% of applicants from one demographic group compared to 30% for others. What ethical issues does this raise and how should you proceed?",
                "answer": "Ethical Issues: 1) DISPARATE IMPACT: The 2x rejection rate for one group (60% vs 30%) suggests unfair treatment, even if unintentional. 2) HISTORICAL BIAS: Training data may encode past hiring discrimination. 3) PROXY DISCRIMINATION: Features like zip code, school, or name may correlate with protected characteristics. 4) HARM POTENTIAL: Affected candidates lose job opportunities unfairly. Proceed: 1) PAUSE DEPLOYMENT: Do not use this model for decisions until resolved. 2) ANALYZE: Investigate which features drive the disparity. 3) AUDIT: Check if model relies on proxies for protected groups. 4) REPORT: Escalate to management/HR/legal with findings. 5) REMEDIATE: Consider removing biased features, retraining with balanced data, or applying fairness constraints. 6) DOCUMENT: Record findings and decisions made. The 85% accuracy is irrelevant if the model discriminates - fairness is a separate, critical requirement.",
                "hint": "Consider the impact on affected individuals and your professional responsibility to address bias"
            }
        ],
        "quiz": [
            {
                "question": "The 5 Whys technique is used to:",
                "options": ["Count errors", "Find root causes", "Measure accuracy", "Test performance"],
                "correct": 1,
                "explanation": "The 5 Whys is a root cause analysis technique that repeatedly asks 'why' to drill down from symptoms to underlying causes."
            },
            {
                "question": "If your model has high training accuracy but low test accuracy, the problem is likely:",
                "options": ["Underfitting", "Overfitting", "Data leakage", "Concept drift"],
                "correct": 1,
                "explanation": "High train/low test accuracy is the classic sign of overfitting - the model memorized training data but doesn't generalize."
            },
            {
                "question": "The bisection debugging technique works by:",
                "options": ["Testing random points", "Dividing the problem in half repeatedly", "Testing all points", "Skipping to the end"],
                "correct": 1,
                "explanation": "Bisection (divide and conquer) tests at midpoints to efficiently narrow down where the problem occurs in a pipeline."
            },
            {
                "question": "In the test pyramid, which tests are at the bottom (most numerous)?",
                "options": ["End-to-end tests", "Integration tests", "Unit tests", "Manual tests"],
                "correct": 2,
                "explanation": "Unit tests form the base of the pyramid - they're fast, cheap, and you should have many of them testing individual components."
            },
            {
                "question": "When you discover your model has significantly different error rates for different demographic groups, you should:",
                "options": ["Ignore it if overall accuracy is good", "Investigate for bias and consider pausing deployment", "Report only the overall accuracy", "Assume the difference is natural"],
                "correct": 1,
                "explanation": "Significant performance differences across demographic groups may indicate bias. You have an ethical obligation to investigate and address this before deployment, even if overall metrics look good."
            }
        ]
    },
    "Data Ensembling & Reliability": {
        "course": "Evaluation of Outcomes",
        "description": "Learn ensemble techniques to improve model reliability and result consistency.",
        "lessons": [
            {
                "title": "Introduction to Ensemble Methods",
                "content": """
**Combining Models for Better Results**

Ensemble methods combine multiple models to produce better predictions than any single model.

**Why Ensembles Work:**

| Problem | How Ensembles Help |
|---------|-------------------|
| Single model overfits | Different models overfit differently, averaging reduces it |
| High variance | Averaging reduces variance |
| Model uncertainty | Multiple models provide confidence bounds |
| Missing patterns | Different models capture different patterns |

**Main Ensemble Strategies:**

```
1. BAGGING (Bootstrap Aggregating)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Train multiple models on random subsets
   Combine by averaging (regression) or voting (classification)
   
   Example: Random Forest
   
   Data â†’ [Subset 1] â†’ Model 1 â”€â”
       â†’ [Subset 2] â†’ Model 2 â”€â”€â”¼â†’ Average/Vote â†’ Prediction
       â†’ [Subset 3] â†’ Model 3 â”€â”˜


2. BOOSTING
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Train models sequentially, each fixing previous errors
   
   Example: XGBoost, AdaBoost
   
   Data â†’ Model 1 â†’ Errors â†’ Model 2 â†’ Errors â†’ Model 3
                                                    â†“
                                        Combined Prediction


3. STACKING
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Train diverse base models, then train a meta-model on their predictions
   
   Data â†’ [Model A] â†’ Pred A â”€â”
       â†’ [Model B] â†’ Pred B â”€â”€â”¼â†’ Meta-Model â†’ Final Prediction
       â†’ [Model C] â†’ Pred C â”€â”˜
```

**When to Use Each:**

| Method | Best For | Caution |
|--------|----------|---------|
| **Bagging** | High-variance models (decision trees) | Less effective for biased models |
| **Boosting** | Weak learners, tabular data | Can overfit, slower training |
| **Stacking** | Diverse model types | Complexity, risk of overfitting |
                """,
                "key_points": ["Ensembles combine multiple models for better results", "Bagging reduces variance through averaging", "Boosting reduces bias by focusing on errors", "Stacking uses a meta-model on base predictions"]
            },
            {
                "title": "Improving Reliability with Ensembles",
                "content": """
**Using Ensembles for More Reliable Predictions**

Beyond accuracy, ensembles improve prediction reliability and confidence.

**Reliability Metrics:**

| Metric | What It Measures |
|--------|-----------------|
| **Prediction variance** | How much predictions vary across models |
| **Calibration** | Do probabilities match actual frequencies? |
| **Consistency** | Same input â†’ similar outputs over time |
| **Robustness** | Performance on noisy or adversarial data |

**Measuring Prediction Confidence:**

```
ENSEMBLE CONFIDENCE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For a single prediction, get outputs from all models:

Model 1: 0.82  â”€â”
Model 2: 0.78   â”œâ†’ Mean: 0.81, Std: 0.04
Model 3: 0.85  â”€â”˜

High agreement (low std) = High confidence
Low agreement (high std) = Low confidence, flag for review
```

**Confidence-Based Actions:**

| Prediction Std | Confidence | Action |
|---------------|------------|--------|
| < 0.05 | High | Automate decision |
| 0.05 - 0.15 | Medium | Apply with monitoring |
| > 0.15 | Low | Human review required |

**Calibration Improvement:**

```
BEFORE CALIBRATION:
Model says 70% â†’ Actually 85% of the time
Model says 90% â†’ Actually 75% of the time

CALIBRATION TECHNIQUES:
1. Platt Scaling: Fit logistic regression on probabilities
2. Isotonic Regression: Non-parametric calibration
3. Temperature Scaling: Scale logits by learned temperature

AFTER CALIBRATION:
Model says 70% â†’ Actually ~70% of the time
Model says 90% â†’ Actually ~90% of the time
```

**Reliability vs Accuracy Tradeoff:**

Sometimes you can increase reliability by:
- Abstaining on uncertain predictions
- Using simpler, more stable models
- Accepting slightly lower accuracy for more consistent results

```
ABSTENTION STRATEGY:
If prediction confidence < threshold:
    Return "Uncertain - requires human review"
Else:
    Return prediction

This improves reliability of accepted predictions
but reduces coverage (some cases not automated)
```
                """,
                "key_points": ["Ensemble variance indicates prediction confidence", "Calibration aligns probabilities with reality", "Consider abstaining on low-confidence predictions", "Reliability sometimes trades off with accuracy"]
            },
            {
                "title": "Practical Ensemble Implementation",
                "content": """
**Building Ensembles in Practice**

Step-by-step approach to implementing ensemble methods.

**Step 1: Select Diverse Base Models**

```
GOOD DIVERSITY (different error patterns):
â”œâ”€â”€ Linear model (captures linear relationships)
â”œâ”€â”€ Decision tree (captures non-linear, rules)
â”œâ”€â”€ Neural network (captures complex patterns)
â””â”€â”€ Nearest neighbors (captures local patterns)

BAD DIVERSITY (similar errors):
â”œâ”€â”€ Random Forest
â”œâ”€â”€ Extra Trees
â”œâ”€â”€ Another Random Forest variant
â””â”€â”€ Decision Tree
(All tree-based, similar biases)
```

**Step 2: Train Base Models**

```
# Pseudocode for bagging
base_models = []
for i in range(n_models):
    sample = bootstrap_sample(training_data)
    model = train_model(sample)
    base_models.append(model)
```

**Step 3: Combine Predictions**

| Task Type | Combination Method |
|-----------|-------------------|
| **Regression** | Mean, median, or weighted average |
| **Classification** | Majority vote or average probabilities |
| **Ranking** | Rank averaging or Borda count |

```
WEIGHTED AVERAGING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weight models by their validation performance:

Model 1: Accuracy 0.85 â†’ Weight 0.85/2.45 = 0.35
Model 2: Accuracy 0.80 â†’ Weight 0.80/2.45 = 0.33
Model 3: Accuracy 0.80 â†’ Weight 0.80/2.45 = 0.33

Final = 0.35Ã—Predâ‚ + 0.33Ã—Predâ‚‚ + 0.33Ã—Predâ‚ƒ
```

**Step 4: Validate the Ensemble**

```
VALIDATION CHECKLIST
â˜ Ensemble beats best single model?
â˜ Ensemble variance is lower than individual?
â˜ Ensemble is well-calibrated?
â˜ Performance consistent across data splits?
â˜ Computational cost acceptable?
```

**Step 5: Monitor in Production**

```
ENSEMBLE MONITORING DASHBOARD
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Base Model Agreement:
â””â”€â”€ High agreement (>80%): 92% of predictions âœ“
â””â”€â”€ Low agreement (<50%): 3% of predictions (flagged)

Individual Model Health:
â”œâ”€â”€ Model 1: Active, 86% accuracy âœ“
â”œâ”€â”€ Model 2: Active, 84% accuracy âœ“
â””â”€â”€ Model 3: Degraded, 78% accuracy âš  (investigate)

Ensemble Performance:
â””â”€â”€ Combined: 89% accuracy âœ“
```
                """,
                "key_points": ["Choose diverse model types for best ensembles", "Weight models by performance", "Validate ensemble against single models", "Monitor individual model health"]
            },
            {
                "title": "Ensemble Techniques for Data Quality",
                "content": """
**Using Ensembles to Detect and Handle Data Issues**

Ensembles can improve data reliability, not just model predictions.

**Data Ensembling for Quality:**

| Technique | Purpose |
|-----------|---------|
| **Multiple sources** | Cross-validate information |
| **Imputation ensemble** | Better missing value estimates |
| **Anomaly consensus** | More reliable outlier detection |
| **Label aggregation** | Handle noisy labels |

**Multi-Source Data Fusion:**

```
DATA SOURCE AGREEMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Customer revenue from 3 sources:

CRM: $125,000
ERP: $127,500
Reports: $124,800

Agreement score: High (within 2%)
â†’ Use average: $125,767

If sources disagreed significantly:
â†’ Flag for manual review
â†’ Apply business rules to resolve
```

**Ensemble Imputation:**

```
MISSING VALUE: Customer age

Method 1 (Mean): 42
Method 2 (Median): 39
Method 3 (KNN): 44
Method 4 (Regression): 41

Ensemble estimate: 41.5 (average)
Uncertainty: Â±2.2 (std dev)

Flag if uncertainty > threshold
```

**Consensus Outlier Detection:**

```
OUTLIER DETECTION ENSEMBLE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each data point, run multiple detectors:

Point X:
â”œâ”€â”€ Z-score: Normal
â”œâ”€â”€ IQR method: Normal
â”œâ”€â”€ Isolation Forest: Outlier
â””â”€â”€ LOF: Normal

Consensus: 1/4 flagged â†’ Likely normal

Point Y:
â”œâ”€â”€ Z-score: Outlier
â”œâ”€â”€ IQR method: Outlier
â”œâ”€â”€ Isolation Forest: Outlier
â””â”€â”€ LOF: Normal

Consensus: 3/4 flagged â†’ Likely outlier, investigate
```

**Label Aggregation (Crowdsourcing):**

When multiple labelers annotate data:

```
Sample #42 labels:
â”œâ”€â”€ Labeler A: Positive
â”œâ”€â”€ Labeler B: Positive
â”œâ”€â”€ Labeler C: Negative
â”œâ”€â”€ Labeler D: Positive
â””â”€â”€ Labeler E: Positive

Majority vote: Positive (4/5)
Agreement: 80% â†’ High confidence

Methods:
1. Simple majority vote
2. Weighted by labeler quality
3. Dawid-Skene model (accounts for labeler bias)
```
                """,
                "key_points": ["Combine multiple data sources for reliability", "Ensemble imputation provides uncertainty estimates", "Consensus improves outlier detection", "Aggregate labels from multiple annotators"]
            }
        ],
        "exercises": [
            {
                "title": "Design an Ensemble Strategy",
                "type": "practical",
                "question": "You're predicting customer lifetime value (CLV). Design an ensemble with 3 diverse models and explain how you'd combine them.",
                "answer": "Ensemble Design: BASE MODELS: 1) Linear Regression - Captures linear relationships between features (recency, frequency, monetary) and CLV. Simple, interpretable baseline. 2) XGBoost - Captures non-linear relationships and feature interactions. Good for tabular data, handles missing values. 3) Neural Network (MLP) - Captures complex patterns, can learn from large datasets. Different optimization approach. COMBINATION: Use weighted averaging based on validation RMSE. Calculate weights as inverse of error: Weight_i = (1/RMSE_i) / Î£(1/RMSE_j). Example: If RMSE = [1000, 800, 900], weights â‰ˆ [0.28, 0.35, 0.31]. CONFIDENCE: Report prediction Â± (std across models). If std > 20% of prediction, flag for review. VALIDATION: Verify ensemble RMSE < best individual model RMSE.",
                "hint": "Choose models that make different types of errors and decide how to weight their predictions"
            },
            {
                "title": "Implement Consensus Outlier Detection",
                "type": "scenario",
                "question": "You have sales data and want to detect anomalies. Three methods flag the following records: Z-score flags: [5, 23, 47]. IQR flags: [5, 23, 89]. Isolation Forest flags: [5, 47, 102]. Which records should be investigated?",
                "answer": "Consensus Analysis: Record 5: Flagged by ALL 3 methods (3/3) â†’ HIGH PRIORITY. Definitely investigate - multiple independent methods agree. Record 23: Flagged by Z-score and IQR (2/3) â†’ MEDIUM PRIORITY. Likely anomaly, worth investigating. Record 47: Flagged by Z-score and Isolation Forest (2/3) â†’ MEDIUM PRIORITY. Worth investigating. Record 89: Flagged by IQR only (1/3) â†’ LOW PRIORITY. Might be edge case for IQR method, less likely true anomaly. Record 102: Flagged by Isolation Forest only (1/3) â†’ LOW PRIORITY. Single method disagreement. Investigation order: 5 â†’ 23 â†’ 47 â†’ 89 â†’ 102. Also consider: the methods that flagged it (Z-score sensitive to extremes, IQR to distribution, IF to isolation) give clues about the nature of the anomaly.",
                "hint": "Count how many methods flagged each record and prioritize by consensus"
            }
        ],
        "quiz": [
            {
                "question": "Bagging improves model performance primarily by:",
                "options": ["Increasing bias", "Reducing variance", "Adding more features", "Using more data"],
                "correct": 1,
                "explanation": "Bagging (Bootstrap Aggregating) reduces variance by training on random subsets and averaging predictions, which smooths out overfitting."
            },
            {
                "question": "In an ensemble, high prediction variance across models indicates:",
                "options": ["The ensemble is working well", "Low confidence in the prediction", "The models are identical", "Perfect accuracy"],
                "correct": 1,
                "explanation": "High variance means the models disagree, indicating uncertainty. This is actually useful information for flagging cases needing review."
            },
            {
                "question": "When building an ensemble, you should select models that are:",
                "options": ["All the same type", "Diverse with different error patterns", "Only the most accurate", "Random selections"],
                "correct": 1,
                "explanation": "Diverse models that make different types of errors complement each other, so their combination performs better than any individual."
            },
            {
                "question": "Boosting differs from bagging because it:",
                "options": ["Uses random subsets", "Trains models sequentially to fix errors", "Only uses one model", "Ignores training data"],
                "correct": 1,
                "explanation": "Boosting trains models sequentially, with each new model focusing on the errors of previous models. Bagging trains in parallel on random subsets."
            }
        ]
    },
    "ETL & Version Control": {
        "course": "Evaluation of Outcomes",
        "description": "Understand ETL systems and version control practices for collaborative data analysis.",
        "lessons": [
            {
                "title": "ETL in the Data Lifecycle",
                "content": """
**Understanding Extract, Transform, Load (ETL)**

ETL is the backbone of data analysis, moving data from sources to analysis-ready formats.

**ETL Components:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXTRACT  â”‚ â†’  â”‚  TRANSFORM   â”‚ â†’  â”‚   LOAD   â”‚
â”‚          â”‚    â”‚              â”‚    â”‚          â”‚
â”‚ Sources: â”‚    â”‚ Clean        â”‚    â”‚ Targets: â”‚
â”‚ -Databaseâ”‚    â”‚ Validate     â”‚    â”‚ -DW      â”‚
â”‚ -API     â”‚    â”‚ Aggregate    â”‚    â”‚ -Lake    â”‚
â”‚ -Files   â”‚    â”‚ Join         â”‚    â”‚ -Mart    â”‚
â”‚ -Streams â”‚    â”‚ Calculate    â”‚    â”‚ -Cache   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Extract Phase:**

| Source Type | Considerations |
|-------------|---------------|
| **Databases** | Query optimization, connection limits |
| **APIs** | Rate limits, authentication, pagination |
| **Files** | Format parsing, encoding issues |
| **Streams** | Real-time vs batch, ordering |

**Transform Phase:**

```
COMMON TRANSFORMATIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Cleaning
   â””â”€â”€ Handle nulls, fix formats, remove duplicates

2. Standardization
   â””â”€â”€ Consistent naming, units, date formats

3. Validation
   â””â”€â”€ Check ranges, business rules, integrity

4. Aggregation
   â””â”€â”€ Sum, count, average by dimensions

5. Enrichment
   â””â”€â”€ Add calculated fields, join reference data

6. Type Conversion
   â””â”€â”€ Cast strings to dates, numbers, etc.
```

**Load Phase:**

| Load Type | Use Case | Frequency |
|-----------|----------|-----------|
| **Full load** | Initial load, small datasets | Daily/weekly |
| **Incremental** | Large datasets, frequent updates | Hourly/daily |
| **Streaming** | Real-time requirements | Continuous |
| **Merge (upsert)** | Update existing + insert new | As needed |

**ETL vs ELT:**

```
ETL (Traditional):
Source â†’ Transform â†’ Load to Target
- Transform before loading
- Good for structured data
- Transform logic in ETL tool

ELT (Modern):
Source â†’ Load to Target â†’ Transform
- Load raw data first
- Transform in the data warehouse
- Uses DW computing power
- Good for big data
```
                """,
                "key_points": ["ETL: Extract, Transform, Load", "Transform includes cleaning, validation, aggregation", "Choose load strategy based on data size and frequency", "ELT loads first, transforms in the warehouse"]
            },
            {
                "title": "Data Pipeline Design",
                "content": """
**Building Reliable Data Pipelines**

A well-designed pipeline ensures consistent, reliable data for analysis.

**Pipeline Architecture:**

```
MODERN DATA PIPELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                 â”Œâ”€â”€â†’ Analytics
Sources                          â”‚
   â†“                             â”‚
[Ingestion] â†’ [Storage] â†’ [Processing] â†’ Dashboards
                                 â”‚
Raw Zone    Staging    Transform â””â”€â”€â†’ ML Models
   â†“           â†“          â†“
Landing    Cleaned    Aggregated
```

**Pipeline Components:**

| Component | Purpose | Tools |
|-----------|---------|-------|
| **Scheduler** | Trigger jobs at right time | Airflow, Cron |
| **Orchestrator** | Manage job dependencies | Airflow, Prefect |
| **Queue** | Handle async processing | Kafka, RabbitMQ |
| **Monitoring** | Track health and failures | Grafana, DataDog |
| **Alerting** | Notify on issues | PagerDuty, Slack |

**Pipeline Patterns:**

```
1. BATCH PROCESSING
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Run at scheduled intervals
   Process all data since last run
   Good for: Daily reports, aggregations
   
   [Schedule: Daily 2am] â†’ Process yesterday's data

2. MICRO-BATCH
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Small batches every few minutes
   Balance latency and efficiency
   Good for: Near-real-time dashboards
   
   [Schedule: Every 5 min] â†’ Process latest chunk

3. STREAMING
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Continuous processing
   Event-driven
   Good for: Real-time analytics, alerts
   
   [Event arrives] â†’ Process immediately
```

**Error Handling:**

```
PIPELINE ERROR STRATEGY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Retry with backoff
   Try again after: 1min, 5min, 30min

2. Dead letter queue
   Failed records go to separate queue for review

3. Partial success
   Continue with valid data, log failures

4. Circuit breaker
   Stop processing if failure rate too high

5. Alerting
   Notify team of critical failures
```

**Pipeline Best Practices:**
- Idempotent operations (re-running produces same result)
- Checkpointing (save progress, resume on failure)
- Data lineage tracking (know where data came from)
- Quality checks at each stage
                """,
                "key_points": ["Design for reliability with retry and error handling", "Choose batch vs streaming based on latency needs", "Make pipelines idempotent for safe reruns", "Track data lineage for debugging"]
            },
            {
                "title": "Version Control for Data Projects",
                "content": """
**Managing Changes in Collaborative Data Work**

Version control tracks changes, enables collaboration, and provides rollback capability.

**What to Version Control:**

| Asset | Version Control? | How |
|-------|-----------------|-----|
| **Code** | Yes | Git |
| **Model files** | Yes | Git LFS, DVC |
| **Configurations** | Yes | Git |
| **Small datasets** | Maybe | Git (if < 100MB) |
| **Large datasets** | Track separately | DVC, data catalogs |
| **Documentation** | Yes | Git |
| **Results/outputs** | Maybe | Depends on reproducibility |

**Git Basics for Data Projects:**

```
ESSENTIAL GIT WORKFLOW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Clone repository
   git clone <repo-url>

2. Create feature branch
   git checkout -b feature/new-model

3. Make changes and commit
   git add .
   git commit -m "Add churn prediction model v2"

4. Push to remote
   git push origin feature/new-model

5. Create pull request for review

6. Merge after approval
   git checkout main
   git merge feature/new-model
```

**Commit Message Best Practices:**

```
GOOD COMMIT MESSAGES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Format: <type>: <subject>

feat: Add customer segmentation model
fix: Correct date parsing in ETL pipeline
docs: Update model documentation
refactor: Simplify feature engineering code
test: Add unit tests for data validation
data: Update training dataset to Q4 2025

Include:
- What changed
- Why it changed
- Any breaking changes
```

**Branching Strategy:**

```
GITFLOW FOR DATA PROJECTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
       â†‘         â†‘
develop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
       â†‘    â†‘    â†‘
      feature  feature
      /model   /pipeline
       
- main: Production-ready code
- develop: Integration branch
- feature/*: Individual work items
```

**Handling Large Files:**

```
GIT LFS (Large File Storage)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Track model files:
git lfs track "*.pkl"
git lfs track "*.h5"

.gitattributes will include:
*.pkl filter=lfs diff=lfs merge=lfs -text
*.h5 filter=lfs diff=lfs merge=lfs -text

DVC (Data Version Control)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Track datasets:
dvc add data/training.csv
git add data/training.csv.dvc

Data stored externally (S3, GCS)
Git tracks metadata only
```
                """,
                "key_points": ["Version control code, configs, and documentation", "Use Git LFS or DVC for large files", "Follow branching strategy for collaboration", "Write clear commit messages"]
            },
            {
                "title": "Collaborative Data Workflows",
                "content": """
**Working Together on Data Projects**

Effective collaboration requires clear processes and communication.

**Collaboration Framework:**

```
COLLABORATIVE WORKFLOW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. PLAN
   â””â”€â”€ Define scope, assign roles, set timeline

2. DEVELOP
   â””â”€â”€ Work on branches, commit frequently

3. REVIEW
   â””â”€â”€ Code review, testing, documentation

4. INTEGRATE
   â””â”€â”€ Merge to main, deploy to staging

5. VALIDATE
   â””â”€â”€ Test in staging, verify results

6. DEPLOY
   â””â”€â”€ Release to production, monitor
```

**Code Review for Data Projects:**

```
DATA PROJECT CODE REVIEW CHECKLIST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â˜ Does the code run without errors?
â˜ Is the logic correct?
â˜ Are there appropriate tests?
â˜ Is the code documented?
â˜ Does it follow coding standards?
â˜ Are data validations in place?
â˜ Is performance acceptable?
â˜ Are secrets/credentials secure?
â˜ Is the commit message clear?
```

**Handling Merge Conflicts:**

| Conflict Type | Resolution Approach |
|--------------|---------------------|
| **Code conflicts** | Discuss with other author, merge carefully |
| **Configuration conflicts** | Document environment differences |
| **Data conflicts** | Typically use latest version or merge rules |
| **Model conflicts** | May need to retrain on combined changes |

**Communication Best Practices:**

```
ASYNC COMMUNICATION (preferred)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Pull request descriptions
- Code comments
- Documentation updates
- Slack/Teams for quick questions

SYNC COMMUNICATION (when needed)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Standup meetings (brief, focused)
- Design discussions
- Conflict resolution
- Pair programming on complex issues
```

**Change Documentation:**

```
CHANGELOG ENTRY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
## [1.2.0] - 2025-10-15

### Added
- New feature importance calculation
- Automated data quality checks

### Changed
- Updated training data to include Q3 2025
- Improved model accuracy from 85% to 88%

### Fixed
- Fixed timezone handling in date features
- Corrected null handling in customer age

### Deprecated
- Old scoring endpoint (use /v2/score)
```

**Documentation Locations:**

| Type | Where to Document |
|------|------------------|
| **Code logic** | Inline comments, docstrings |
| **API/interfaces** | README, API docs |
| **Decisions** | ADRs (Architecture Decision Records) |
| **Changes** | CHANGELOG |
| **Processes** | Wiki/Confluence |
                """,
                "key_points": ["Follow structured development workflow", "Conduct code reviews for quality", "Document changes in changelog", "Communicate async when possible"]
            }
        ],
        "exercises": [
            {
                "title": "Design an ETL Pipeline",
                "type": "practical",
                "question": "Design an ETL pipeline to load daily sales data from 3 sources (POS system, e-commerce API, returns database) into a data warehouse for analysis. Include error handling.",
                "answer": "ETL Pipeline Design: EXTRACT: 1) POS: Query database for yesterday's transactions at 2am (incremental by date). 2) E-commerce: API call with pagination, handle rate limits with backoff. 3) Returns: Query returns database, join with original transaction IDs. TRANSFORM: 1) Standardize date formats (all to ISO 8601). 2) Normalize product codes across systems. 3) Calculate net sales (gross - returns). 4) Validate: sales > 0, dates within range, required fields present. 5) Handle nulls: impute missing store_id from product mapping, flag records with critical nulls. LOAD: 1) Upsert to staging table (update if exists, insert if new). 2) Run quality checks on staging. 3) If checks pass, merge to production fact table. ERROR HANDLING: 1) Retry failed extracts 3 times with exponential backoff. 2) Dead letter queue for records failing validation. 3) Alert if >5% records fail. 4) Checkpoint after each source extraction. 5) Full pipeline idempotent (re-run safe).",
                "hint": "Consider each phase (Extract, Transform, Load) and what happens when something fails"
            },
            {
                "title": "Create Git Workflow",
                "type": "practical",
                "question": "Your team has 3 data scientists working on the same model. One is improving features, one is tuning hyperparameters, one is adding new data. Design a Git workflow to avoid conflicts.",
                "answer": "Git Workflow: BRANCH STRUCTURE: main (production) â†’ develop (integration) â†’ feature branches. FEATURE BRANCHES: 1) feature/improved-features (feature engineering changes). 2) feature/hyperparameter-tuning (model config changes). 3) feature/new-data-source (data pipeline changes). WORKFLOW: 1) Each person works on their branch, commits frequently. 2) Daily: pull latest develop, merge into feature branch, resolve conflicts early. 3) When ready: create PR to develop, request review from one other team member. 4) Code review checklist: tests pass, documentation updated, no hardcoded paths. 5) Weekly: merge all approved PRs to develop, run integration tests. 6) Monthly: merge develop to main, tag release version. CONFLICT PREVENTION: 1) Separate concerns: features.py (person 1), config.yaml (person 2), pipeline.py (person 3). 2) Communicate in daily standup about overlapping files. 3) Use feature flags for experimental code. 4) Model files tracked with DVC, not Git.",
                "hint": "Separate work by file/component and establish clear merge cadence"
            }
        ],
        "quiz": [
            {
                "question": "In ETL, the 'Transform' step includes:",
                "options": ["Only extracting data", "Cleaning, validating, and aggregating data", "Just loading to database", "Deleting old data"],
                "correct": 1,
                "explanation": "The Transform step processes raw data: cleaning, validating, standardizing, aggregating, and enriching before loading."
            },
            {
                "question": "Git LFS is used for:",
                "options": ["Small text files", "Large binary files like models", "Deleted files", "Branch management"],
                "correct": 1,
                "explanation": "Git LFS (Large File Storage) handles large binary files like trained models, storing them separately while Git tracks metadata."
            },
            {
                "question": "An idempotent pipeline means:",
                "options": ["It runs once and stops", "Re-running it produces the same result", "It cannot be modified", "It runs faster each time"],
                "correct": 1,
                "explanation": "Idempotent pipelines produce the same result whether run once or multiple times, which is important for safe retries and reruns."
            },
            {
                "question": "ELT differs from ETL because:",
                "options": ["It doesn't transform data", "It loads raw data first, then transforms in the warehouse", "It only works with streaming", "It requires no configuration"],
                "correct": 1,
                "explanation": "ELT loads raw data to the data warehouse first, then uses the warehouse's computing power to transform it. ETL transforms before loading."
            }
        ]
    },
    "Data Visualization Fundamentals": {
        "course": "Data Visualisation",
        "description": "Learn visualization techniques to present data findings effectively to both technical and non-technical audiences.",
        "lessons": [
            {
                "title": "Introduction to Data Visualization",
                "content": """
**Why Data Visualization Matters**

Data visualization transforms complex data into visual representations that are easy to understand at a glance.

**Key Benefits:**
- **Clarity**: Complex patterns become visible
- **Speed**: Faster comprehension than reading tables
- **Engagement**: Visual stories capture attention
- **Accessibility**: Non-technical audiences can understand

**The Visualization Process:**

```
1. UNDERSTAND the data and audience
2. SELECT the right chart type
3. DESIGN with clarity in mind
4. REFINE based on feedback
5. PRESENT with context
```

**Real-World Example:**
A hospital reduced patient wait times by 30% after visualizing patient flow patterns on a dashboard. Staff could instantly see bottlenecks that were invisible in spreadsheets.

**Types of Visualizations:**

| Type | Purpose | Example |
|------|---------|---------|
| **Bar Chart** | Compare categories | Sales by product |
| **Line Chart** | Show trends over time | Revenue growth |
| **Pie Chart** | Show proportions | Market share |
| **Scatter Plot** | Show relationships | Price vs. demand |
| **Heatmap** | Show patterns in matrices | Website clicks |
                """,
                "key_points": ["Visualization makes data accessible", "Choose charts based on purpose", "Design for your audience", "Visual stories are more engaging"]
            },
            {
                "title": "Choosing the Right Chart Type",
                "content": """
**Chart Selection Guide**

The most common mistake in visualization is choosing the wrong chart type for your data and message.

**Decision Framework:**

**What are you trying to show?**

| Goal | Best Charts |
|------|-------------|
| **Compare values** | Bar chart, Column chart |
| **Show change over time** | Line chart, Area chart |
| **Show parts of a whole** | Pie chart, Stacked bar |
| **Show relationships** | Scatter plot, Bubble chart |
| **Show distribution** | Histogram, Box plot |
| **Show geographic data** | Map, Choropleth |

**Common Mistakes:**

âŒ **Pie charts with too many slices** (max 5-7)
âŒ **3D charts** (distort perception)
âŒ **Dual y-axes** (confusing)
âŒ **Truncated axes** (misleading)
âŒ **Too many colors** (distracting)

**Chart Selection Examples:**

**Scenario 1:** "Compare Q1 revenue across 5 regions"
â†’ Use: Horizontal bar chart (easy to read labels)

**Scenario 2:** "Show website traffic trend over 12 months"
â†’ Use: Line chart (shows continuous change)

**Scenario 3:** "Show how budget is divided across departments"
â†’ Use: Pie chart (if â‰¤5 categories) or stacked bar

**Scenario 4:** "Explore relationship between ad spend and conversions"
â†’ Use: Scatter plot (shows correlation)
                """,
                "key_points": ["Match chart type to your goal", "Avoid 3D and excessive decoration", "Limit pie charts to 5-7 slices", "Consider your audience's familiarity"]
            },
            {
                "title": "Design Principles for Effective Visualizations",
                "content": """
**Universal Design Principles**

Good visualization design follows principles that make information clear and accessible.

**1. Simplicity (Data-Ink Ratio)**
Maximize the data shown, minimize unnecessary decoration.

| Good | Bad |
|------|-----|
| Clean gridlines | Heavy borders |
| Direct labels | Legends requiring lookup |
| White space | Crowded elements |
| Essential data only | Decorative graphics |

**2. Color Usage**

**Do:**
- Use color purposefully (highlight key data)
- Maintain contrast for readability
- Use colorblind-friendly palettes
- Keep consistent meaning (red = bad, green = good)

**Don't:**
- Use more than 5-7 colors
- Rely only on color to convey meaning
- Use red/green as only differentiator

**Colorblind-Safe Palettes:**
- Blue and Orange
- Blue and Yellow
- Purple and Green

**3. Typography**
- Use readable fonts (sans-serif for screens)
- Size hierarchy: Title > Subtitle > Labels > Annotations
- Left-align text for readability

**4. Layout and Hierarchy**
- Place the most important information prominently
- Use consistent alignment
- Group related elements
- Leave white space for breathing room

**5. Accessibility Checklist:**
- âœ“ Sufficient color contrast
- âœ“ Text alternatives for complex charts
- âœ“ Clear titles and labels
- âœ“ Logical reading order
                """,
                "key_points": ["Simplicity beats decoration", "Use color purposefully and accessibly", "Maintain consistent design language", "Design for accessibility from the start"]
            },
            {
                "title": "Telling Stories with Data",
                "content": """
**Data Storytelling**

The best visualizations tell a story that leads to action.

**Story Structure:**

```
SETUP      â†’  INSIGHT    â†’  ACTION
What is?      What matters?  What now?
```

**Components of a Data Story:**

1. **Context**: Why should the audience care?
2. **Conflict/Tension**: What's the problem or opportunity?
3. **Resolution**: What does the data reveal?
4. **Call to Action**: What should happen next?

**Example Data Story:**

**Context:** "Our customer satisfaction scores have been declining."

**Conflict:** "We analyzed 10,000 support tickets and found 60% mention 'wait time.'"

**Resolution:** [Show chart] "Wait times over 5 minutes correlate with 3x more negative reviews."

**Action:** "Investing in chat support could reduce wait times by 70% and recover our satisfaction scores."

**Presentation Tips:**

| Do | Don't |
|-----|--------|
| Start with the headline | Start with methodology |
| Build to the insight | Show all data at once |
| Use animation purposefully | Animate everything |
| Pause for questions | Rush through slides |
| Provide takeaway summary | End abruptly |

**Slideshow Best Practices:**
- One message per slide
- Maximum 6 data points per chart
- Include source citations
- Number slides for reference
- Provide handout with details
                """,
                "key_points": ["Lead with the insight, not the methodology", "Structure as setup â†’ insight â†’ action", "One message per visualization", "Always end with clear recommendations"]
            },
            {
                "title": "Visualization Tools and Software",
                "content": """
**Common Visualization Tools**

Different tools serve different purposes and skill levels.

**Tool Comparison:**

| Tool | Best For | Skill Level |
|------|----------|-------------|
| **Excel/Sheets** | Quick charts, basic dashboards | Beginner |
| **Tableau** | Interactive dashboards, exploration | Intermediate |
| **Power BI** | Microsoft integration, business dashboards | Intermediate |
| **Python (matplotlib, seaborn)** | Custom visualizations, automation | Advanced |
| **R (ggplot2)** | Statistical visualizations | Advanced |
| **D3.js** | Web-based custom visualizations | Expert |

**Excel/Google Sheets:**
- Quick and accessible
- Good for simple charts
- Limited interactivity
- Best for: Internal reports, quick analysis

**Tableau:**
- Drag-and-drop interface
- Excellent for exploration
- Interactive dashboards
- Best for: Business analytics, presentations

**Power BI:**
- Strong Microsoft 365 integration
- AI-powered insights
- Good for enterprise
- Best for: Corporate reporting, KPI dashboards

**Python Libraries:**
- matplotlib: Basic plotting
- seaborn: Statistical visualizations
- plotly: Interactive charts
- Best for: Reproducible analysis, automation

**Choosing Your Tool:**

Ask yourself:
1. What's my skill level?
2. What's my audience expecting?
3. Does it need to be interactive?
4. Will I need to update it regularly?
5. What tools does my organization use?
                """,
                "key_points": ["Match tool to skill level and need", "Excel works for quick, simple charts", "Tableau/Power BI for interactive dashboards", "Python for automation and reproducibility"]
            },
            {
                "title": "Ethics in Data Visualization",
                "content": """
**Ethical Visualization Practices**

Visualizations can mislead as easily as they inform. Ethical practice is essential.

**Common Manipulation Techniques (To Avoid):**

**1. Truncated Axes**
Starting a y-axis at a non-zero value exaggerates differences.

Example:
- Misleading: Bar chart showing 98% vs 100% with y-axis starting at 95%
- Honest: Same data with y-axis starting at 0%

**2. Cherry-Picking Time Frames**
Selecting dates that support your narrative.

Example:
- Misleading: "Sales doubled!" (comparing best month to worst)
- Honest: Show full year with context

**3. Missing Context**
Omitting comparison data or baselines.

Example:
- Misleading: "Crime up 50%!" (without population growth context)
- Honest: Show per-capita rates

**4. Aspect Ratio Manipulation**
Stretching charts to exaggerate or minimize trends.

**Ethical Guidelines:**

âœ“ Always start quantitative axes at zero (for bar charts)
âœ“ Include data sources
âœ“ Show uncertainty when relevant
âœ“ Provide appropriate time context
âœ“ Use consistent scales when comparing
âœ“ Acknowledge limitations
âœ“ Consider colorblind accessibility

**Questions to Ask Before Publishing:**

1. Does this accurately represent the data?
2. Could someone reasonably misinterpret this?
3. Am I being fair to all perspectives?
4. Would I be comfortable if this were fact-checked?
5. Does this serve the audience or my agenda?

**Your Responsibility:**
As a data analyst, you have power to influence decisions. Use visualization ethically to inform, not manipulate.
                """,
                "key_points": ["Avoid truncated axes and cherry-picked data", "Always provide context and sources", "Design for accurate interpretation", "Consider how visualizations might be misused"]
            }
        ],
        "exercises": [
            {
                "title": "Choose the Right Chart",
                "type": "scenario",
                "question": "You need to show how your company's market share has changed compared to 3 competitors over the past 5 years. What chart type would you use and why?",
                "answer": "Best choice: Stacked area chart or multiple line chart. The stacked area shows how the whole market (100%) is divided among competitors over time, making it easy to see both individual changes and relative positions. Alternatively, a multi-line chart works if you want to emphasize individual company trajectories. Avoid: Pie charts (only show one point in time), bar charts (harder to see trends), 3D charts (distort perception).",
                "hint": "Think about what you're showing: parts of a whole AND change over time"
            },
            {
                "title": "Identify Visualization Problems",
                "type": "practical",
                "question": "A chart shows monthly revenue with a y-axis starting at $950,000 instead of $0. The bars show $950K, $975K, and $1M for three months. What's wrong with this visualization?",
                "answer": "The truncated y-axis is misleading. By starting at $950K, the visual difference between bars is exaggerated. The $975K bar appears 50% taller than the $950K bar, when the actual difference is only 2.6% ($25K/$950K). The viewer's eye perceives much larger differences than exist in the data. Fix: Start the y-axis at $0, or use a different chart type (like a table with percentage change) if the small differences are truly significant.",
                "hint": "Calculate the actual percentage difference vs. how different the bars appear"
            },
            {
                "title": "Design for Accessibility",
                "type": "practical",
                "question": "Your dashboard uses red and green colors to show negative and positive values. A colleague mentions they're colorblind. How would you redesign this?",
                "answer": "Redesign solutions: 1) Use colorblind-friendly palette: Blue for negative, Orange for positive (distinguishable by most colorblind people). 2) Add icons/symbols: â–¼ for negative, â–² for positive (not relying on color alone). 3) Use patterns: Solid fill for positive, hatched/striped for negative. 4) Add text labels: Include actual values and +/- signs. 5) Use saturation: Keep same hue but vary brightness. Best practice: Combine color WITH another indicator (shape, pattern, or label) so information isn't conveyed by color alone.",
                "hint": "Don't rely solely on color to convey meaning - add redundant cues"
            }
        ],
        "quiz": [
            {
                "question": "Which chart is best for showing how a budget is divided among 5 departments?",
                "options": ["Line chart", "Scatter plot", "Pie chart", "Histogram"],
                "correct": 2,
                "explanation": "A pie chart shows parts of a whole, perfect for showing how a budget is divided. With only 5 categories, it remains readable. For more categories, consider a bar chart."
            },
            {
                "question": "What is wrong with starting a bar chart's y-axis at a non-zero value?",
                "options": ["It saves space", "It exaggerates visual differences", "It's standard practice", "It helps with small values"],
                "correct": 1,
                "explanation": "Starting at non-zero exaggerates differences, making small changes look dramatic. This can mislead viewers about the magnitude of differences."
            },
            {
                "question": "When presenting to executives, you should:",
                "options": ["Show all your data and methodology first", "Lead with the key insight and recommendation", "Use as many charts as possible", "Avoid simplifying complex data"],
                "correct": 1,
                "explanation": "Executives want the bottom line first. Lead with the insight and recommendation, then provide supporting data if asked. Don't make them wait for the conclusion."
            },
            {
                "question": "A colorblind-friendly visualization should:",
                "options": ["Only use shades of gray", "Rely solely on red and green", "Use color AND another indicator like shapes", "Avoid all color"],
                "correct": 2,
                "explanation": "Best practice is to use color AND another indicator (shapes, patterns, labels) so meaning isn't conveyed by color alone. This helps everyone, not just colorblind users."
            },
            {
                "question": "The 'data-ink ratio' principle suggests:",
                "options": ["Use more ink for important data", "Maximize decoration to engage viewers", "Remove unnecessary visual elements", "Add borders around all elements"],
                "correct": 2,
                "explanation": "Data-ink ratio (from Edward Tufte) means maximizing the data shown while minimizing non-essential 'ink' (decorations, heavy gridlines, unnecessary borders)."
            }
        ]
    },
    "Analysis Reporting Fundamentals": {
        "course": "Analysis Reporting",
        "description": "Learn report writing skills to present analytical work in professional documents for technical and non-technical audiences.",
        "lessons": [
            {
                "title": "Introduction to Analysis Reporting",
                "content": """
**Why Analysis Reports Matter**

A brilliant analysis is worthless if it can't be communicated effectively. Reports transform your findings into actionable documents.

**Key Purpose of Reports:**
- **Document findings**: Create permanent record of analysis
- **Enable decisions**: Provide information for stakeholders to act
- **Share knowledge**: Transfer insights across teams/organizations
- **Build credibility**: Establish you as a trusted analyst

**Report vs. Presentation:**

| Report | Presentation |
|--------|--------------|
| Comprehensive | Highlights only |
| Self-contained | Requires speaker |
| Reference document | One-time event |
| Detailed methodology | High-level overview |
| Read at reader's pace | Controlled timing |

**Types of Analysis Reports:**

| Type | Purpose | Audience | Length |
|------|---------|----------|--------|
| **Executive Summary** | Quick overview | Executives | 1-2 pages |
| **Technical Report** | Full methodology | Analysts | 10-30 pages |
| **Client Report** | Findings + recommendations | Clients | 5-15 pages |
| **Dashboard Report** | Ongoing metrics | Operations | 1-3 pages |
| **Ad-hoc Analysis** | Specific question | Requestor | 2-5 pages |

**The Report Writing Process:**

```
1. UNDERSTAND the audience and purpose
2. STRUCTURE the document logically
3. WRITE clearly and concisely
4. VISUALIZE data effectively
5. REVIEW and refine
6. DELIVER in appropriate format
```
                """,
                "key_points": ["Reports document and communicate findings", "Different report types serve different audiences", "Reports must be self-contained and actionable", "Writing process follows clear stages"]
            },
            {
                "title": "Report Structure and Organization",
                "content": """
**Standard Report Structure**

A well-organized report guides readers through your analysis logically.

**Essential Sections:**

**1. Executive Summary** (Write Last)
- Key findings in 1-2 paragraphs
- Main recommendations
- Critical numbers/metrics
- *Tip: Busy readers may only read this*

**2. Introduction/Background**
- Business context and problem statement
- Scope of analysis
- Data sources used
- Time period covered

**3. Methodology**
- How data was collected/cleaned
- Analysis techniques used
- Assumptions and limitations
- *Adjust detail level for audience*

**4. Findings**
- Present results clearly
- Use visualizations
- Organize by theme or priority
- Include supporting data

**5. Conclusions & Recommendations**
- What the findings mean
- Specific actions to take
- Expected impact of recommendations
- Next steps

**6. Appendix** (Optional)
- Detailed data tables
- Technical methodology
- Supplementary charts
- Glossary of terms

**Organization Tips:**

| Do | Don't |
|-----|--------|
| Use numbered sections | Use vague headings |
| Include table of contents (>5 pages) | Bury key findings |
| Put most important info first | Make readers hunt for answers |
| Use consistent formatting | Change styles mid-document |
                """,
                "key_points": ["Follow standard report structure", "Write executive summary last", "Adjust detail level for audience", "Put most important information first"]
            },
            {
                "title": "Academic Writing for Reports",
                "content": """
**Professional Writing Style**

Analysis reports require clear, professional language that builds credibility.

**Key Principles:**

**1. Clarity Over Complexity**
- Use simple words when possible
- Short sentences (15-20 words average)
- One idea per paragraph
- Active voice preferred

**Bad:** "The utilization of advanced analytical methodologies facilitated the identification of correlational patterns."
**Good:** "Our analysis found a strong link between marketing spend and sales."

**2. Be Precise and Specific**

| Vague | Precise |
|-------|---------|
| "Sales increased significantly" | "Sales increased 23% ($1.2M)" |
| "Most customers prefer..." | "67% of customers prefer..." |
| "Recently" | "In Q3 2025" |
| "Some improvement" | "15% improvement" |

**3. Objective Tone**
- State facts, not opinions
- Support claims with data
- Acknowledge limitations
- Avoid emotional language

**Bad:** "Clearly, the amazing results prove our strategy works perfectly."
**Good:** "Results show a 23% increase, suggesting the strategy is effective. Further testing is recommended."

**4. Consistent Terminology**
- Define terms on first use
- Use same word for same concept
- Create glossary if needed
- Avoid jargon with non-technical audiences

**5. Hedging Appropriately**
When certainty is limited, use appropriate hedging:
- "The data suggests..." (not "proves")
- "This may indicate..."
- "Based on available data..."
- "Further analysis would be needed to confirm..."

**Common Phrases for Reports:**

| Purpose | Phrases |
|---------|---------|
| Introducing findings | "The analysis reveals...", "Data indicates..." |
| Comparing | "In contrast...", "Similarly...", "However..." |
| Recommending | "We recommend...", "Consider...", "The next step is..." |
| Concluding | "In summary...", "The key takeaway is..." |
                """,
                "key_points": ["Clarity and precision over complexity", "Use specific numbers, not vague terms", "Maintain objective, professional tone", "Define terms and be consistent"]
            },
            {
                "title": "Integrating Visualizations in Reports",
                "content": """
**Using Graphics Effectively in Reports**

Static visualizations in reports differ from interactive dashboards - they must stand alone on paper.

**Principles for Report Graphics:**

**1. Every Chart Needs Context**
- Descriptive title (not just "Figure 1")
- Axis labels and units
- Source citation
- Brief caption explaining the insight

**Example:**
âœ… "Figure 3: Monthly Revenue Growth (2024-2025). Revenue increased 45% year-over-year, with Q4 showing the strongest growth."

âŒ "Figure 3: Revenue"

**2. Design for Print**
- High resolution (300 DPI minimum)
- Readable when printed in grayscale
- Sufficient font size (10pt minimum)
- No interactive elements (obviously!)

**3. Placement Guidelines**

| Rule | Reason |
|------|--------|
| Place near related text | Easy reference |
| Don't split across pages | Visual integrity |
| Consistent sizing | Professional appearance |
| White space around graphics | Readability |

**4. Reference Charts in Text**
Always mention figures in your narrative:
- "As shown in Figure 2, sales peaked in December..."
- "The correlation (see Figure 4) suggests..."

**5. Chart Selection for Static Reports**

| Works Well in Reports | Avoid in Reports |
|----------------------|------------------|
| Bar charts | Complex interactive charts |
| Line charts | Animations |
| Pie charts (limited) | Overly detailed maps |
| Tables | Charts requiring zoom |
| Simple scatter plots | 3D charts |

**Caption Writing:**

A good caption includes:
1. Figure number
2. Descriptive title
3. Key insight or takeaway
4. Data source (if not stated elsewhere)

**Example:**
"Figure 5: Customer Satisfaction Scores by Region (Q4 2025). The North region scored 15% higher than the company average, while the South region needs attention. Source: Customer Survey, n=2,500."
                """,
                "key_points": ["Every visualization needs context and caption", "Design for print (high resolution, grayscale-friendly)", "Reference all figures in the text", "Place visuals near related content"]
            },
            {
                "title": "Universal Design and Accessibility",
                "content": """
**Creating Accessible Reports**

Reports should be usable by everyone, including people with disabilities.

**Universal Design Principles:**

**1. Visual Accessibility**

**Color:**
- Don't rely solely on color to convey meaning
- Ensure sufficient contrast (4.5:1 ratio)
- Use colorblind-friendly palettes
- Provide text alternatives

**Fonts:**
- Sans-serif fonts for screens (Arial, Calibri)
- Minimum 11pt body text, 10pt for tables
- Avoid all-caps for long text
- Adequate line spacing (1.15-1.5)

**2. Document Structure**
- Use heading styles (not just bold text)
- Logical heading hierarchy (H1 â†’ H2 â†’ H3)
- Include table of contents for long documents
- Number sections consistently

**3. Table Accessibility**

| Accessible | Not Accessible |
|-----------|----------------|
| Header row identified | No headers |
| Simple structure | Merged cells |
| Reading order clear | Complex nesting |
| Alt text for table | No description |

**4. Alternative Text for Images**
- Describe what the image shows
- Include key data points
- Explain the insight
- Keep under 125 characters

**Good Alt Text:**
"Bar chart showing Q4 sales by region: North $2.3M, South $1.8M, East $2.1M, West $1.5M. North leads by 28%."

**5. Document Format**
- Use accessible PDF settings
- Include bookmarks for navigation
- Tag document structure
- Test with screen reader

**Accessibility Checklist:**
- âœ“ Color contrast meets standards
- âœ“ All images have alt text
- âœ“ Tables have headers identified
- âœ“ Document uses heading styles
- âœ“ Links have descriptive text
- âœ“ Font size is readable
- âœ“ Content order is logical
                """,
                "key_points": ["Design for all users, including those with disabilities", "Use proper heading structure and document tags", "Provide alt text for all visuals", "Test accessibility before distribution"]
            },
            {
                "title": "Report Tools and Distribution",
                "content": """
**Tools for Creating Reports**

Different tools suit different needs and organizational contexts.

**Common Report Tools:**

| Tool | Best For | Pros | Cons |
|------|----------|------|------|
| **Microsoft Word** | Traditional reports | Universal, familiar | Manual formatting |
| **Google Docs** | Collaborative work | Real-time collaboration | Limited offline |
| **LaTeX** | Technical/academic | Professional typography | Steep learning curve |
| **Jupyter/RMarkdown** | Reproducible reports | Code + narrative | Technical audience |
| **Power BI/Tableau** | Dashboard reports | Interactive | Export limitations |

**Word/Google Docs Tips:**
- Use styles for consistency
- Set up templates for repeated reports
- Use section breaks for different headers/footers
- Enable track changes for review

**Distribution Formats:**

| Format | When to Use |
|--------|-------------|
| **PDF** | Final distribution, archival |
| **Word/Doc** | Collaborative editing |
| **HTML** | Web publishing |
| **Print** | Formal presentations |

**Version Control:**
- Include version number in filename
- Date all versions: "Report_v2_2025-01-02.pdf"
- Keep track of what changed
- Archive final versions

**Review Process:**

Before distributing:
1. **Self-review**: Read aloud, check for errors
2. **Peer review**: Fresh eyes catch mistakes
3. **Stakeholder preview**: Ensure alignment
4. **Final proofread**: Spelling, formatting, links

**Quality Checklist:**
- âœ“ Spelling and grammar checked
- âœ“ All figures numbered and referenced
- âœ“ Page numbers included
- âœ“ Headers/footers consistent
- âœ“ Links working
- âœ“ Data is current
- âœ“ Recommendations are clear
- âœ“ Executive summary matches content
                """,
                "key_points": ["Choose tools based on audience and collaboration needs", "Use templates for consistency", "Follow clear version control practices", "Always review before distribution"]
            },
            {
                "title": "Ethical Report Writing",
                "content": """
**Ethics in Analysis Reporting**

As a report writer, you have responsibility for how information is presented and interpreted.

**Core Ethical Principles:**

**1. Accuracy**
- Report data truthfully
- Don't cherry-pick favorable results
- Acknowledge limitations and uncertainties
- Correct errors promptly

**2. Transparency**
- Disclose methodology
- Note data sources
- Explain assumptions
- Reveal conflicts of interest

**3. Fairness**
- Present balanced findings
- Include context that might change interpretation
- Consider multiple stakeholder perspectives
- Don't misrepresent others' views

**Common Ethical Pitfalls:**

| Pitfall | Example | How to Avoid |
|---------|---------|--------------|
| Cherry-picking | Only showing favorable quarters | Show full time period |
| Misleading visuals | Truncated axes | Follow visualization ethics |
| Omitting context | "Sales up 50%!" (from very low base) | Provide absolute numbers too |
| Overstating certainty | "This proves..." | Use appropriate hedging |
| Burying bad news | Hiding problems in appendix | Address issues directly |

**Client vs. Truth:**

What if findings are unfavorable to the client?

**Ethical approach:**
1. Present findings accurately
2. Provide context and explanation
3. Suggest constructive next steps
4. Never falsify or hide data

**Handling Pressure:**
- "Can you make these numbers look better?"
- "Can we leave out this part?"
- "The client won't like this finding"

**Response:**
- Explain why accuracy matters
- Offer alternative framing (not falsification)
- Escalate if pressured to be unethical
- Document requests in writing

**Attribution and Sources:**
- Cite data sources properly
- Credit others' work
- Respect data privacy
- Follow licensing requirements

**The Golden Rule:**
Would you be comfortable if your methodology and decisions were reviewed by a neutral third party? If not, reconsider your approach.
                """,
                "key_points": ["Report data accurately, even unfavorable findings", "Be transparent about methodology and limitations", "Don't mislead through omission or framing", "Maintain professional integrity under pressure"]
            }
        ],
        "exercises": [
            {
                "title": "Rewrite for Clarity",
                "type": "practical",
                "question": "Rewrite this sentence for a non-technical audience: 'The multivariate regression analysis yielded a statistically significant coefficient (Î²=0.43, p<0.01) for the marketing expenditure variable, indicating a positive relationship with sales outcomes.'",
                "answer": "Clear version: 'Our analysis found that marketing spending has a strong positive effect on sales. For every additional $1,000 spent on marketing, sales increased by approximately $430. This relationship is statistically reliable.' Key improvements: 1) Removed jargon (multivariate regression, coefficient, Î², p-value). 2) Explained what the numbers mean practically. 3) Used plain language. 4) Kept the essential insight while making it accessible.",
                "hint": "Replace technical terms with plain language, and explain what the finding means for the business"
            },
            {
                "title": "Structure an Executive Summary",
                "type": "scenario",
                "question": "You've completed an analysis showing that customer churn increased 25% last quarter, mainly due to slow support response times (average 4 hours vs. competitor's 30 minutes). You recommend investing $50K in a chat support system. Write bullet points for the executive summary.",
                "answer": "Executive Summary bullet points: KEY FINDING: Customer churn increased 25% last quarter, costing an estimated $200K in lost annual revenue. ROOT CAUSE: Support response time (4 hours) is 8x slower than industry average (30 minutes). Analysis of 500 exit surveys found 'slow support' cited as primary reason by 67% of churned customers. RECOMMENDATION: Implement chat support system ($50K investment). Expected outcomes: Reduce response time to under 30 minutes, reduce churn by 15-20%, ROI within 4 months based on reduced customer loss. NEXT STEPS: Vendor selection and pilot program in Q2. Note: This structure gives busy executives: the problem, the cause, the solution, and expected results in under 100 words.",
                "hint": "Include: the problem, the cause, the recommended solution, and expected outcomes"
            },
            {
                "title": "Write Alt Text for a Chart",
                "type": "practical",
                "question": "Write appropriate alt text for a bar chart showing quarterly revenue: Q1 $1.2M, Q2 $1.4M, Q3 $1.8M, Q4 $2.1M. The chart highlights that Q4 was the highest.",
                "answer": "Good alt text options: Option 1 (brief): 'Bar chart showing quarterly revenue growth: Q1 $1.2M, Q2 $1.4M, Q3 $1.8M, Q4 $2.1M. Q4 represents 75% growth from Q1.' Option 2 (detailed): 'Vertical bar chart displaying 2025 quarterly revenue. Revenue grew each quarter: Q1 at $1.2M, Q2 at $1.4M (17% increase), Q3 at $1.8M (29% increase), and Q4 reaching $2.1M (17% increase). Q4 is highlighted as the highest quarter, representing total annual growth of 75% from Q1 to Q4.' Key elements: Describes chart type, provides all data values, explains the key insight, quantifies the main finding.",
                "hint": "Include the chart type, the data values, and the key insight someone should take away"
            }
        ],
        "quiz": [
            {
                "question": "Which section should you write LAST in a report?",
                "options": ["Introduction", "Methodology", "Executive Summary", "Appendix"],
                "correct": 2,
                "explanation": "The Executive Summary should be written last because it summarizes the entire report. You can't summarize what you haven't written yet."
            },
            {
                "question": "When writing for non-technical audiences, you should:",
                "options": ["Use technical terms to show expertise", "Replace jargon with plain language", "Include all statistical details", "Avoid numbers entirely"],
                "correct": 1,
                "explanation": "Replace jargon with plain language. Technical terms create barriers. You can still include important numbers, but explain what they mean practically."
            },
            {
                "question": "Alt text for images in reports should:",
                "options": ["Just say 'Figure 1'", "Describe the visual and key insight", "Only be used for decorative images", "Repeat the caption exactly"],
                "correct": 1,
                "explanation": "Alt text should describe what the image shows and convey the key insight, making the content accessible to screen reader users and providing value if images don't load."
            },
            {
                "question": "If your analysis findings are unfavorable to the client, you should:",
                "options": ["Hide them in the appendix", "Present them accurately with context", "Change the numbers to be more positive", "Leave them out entirely"],
                "correct": 1,
                "explanation": "Present findings accurately with appropriate context. Hiding or falsifying results is unethical. You can frame findings constructively and suggest solutions."
            },
            {
                "question": "The minimum font size recommended for body text in reports is:",
                "options": ["8pt", "11pt", "14pt", "18pt"],
                "correct": 1,
                "explanation": "11pt is the recommended minimum for body text to ensure readability. Tables can go to 10pt, but anything smaller becomes difficult to read."
            }
        ]
    },
    "Exam Project 1 - Professional Data Analysis": {
        "course": "Exam Project 1",
        "description": "Develop skills to execute a comprehensive data analysis project from start to finish with professional standards, including project planning, execution, documentation, and presentation.",
        "lessons": [
            {
                "title": "Understanding the Exam Project",
                "content": """
**What is Exam Project 1?**

This is a major project that demonstrates all the competencies you've acquired during your first academic year. It reflects your ability to independently execute a complete data analysis project.

**Project Options:**
- **Individual Project**: Work independently on a self-chosen or assigned problem
- **Group Project**: Collaborate in a team (2-4 members)
- **Internship Project**: Work with a real company on an actual business problem (encouraged!)

**Project Scope:**
Your project must include:
1. **Problem Definition**: Clear statement of what you're solving
2. **Data Collection**: Ethically sourced, relevant data
3. **Analysis**: Using tools and techniques from Semester 1-2
4. **Visualization**: Clear, accessible charts and reports
5. **Recommendations**: Actionable insights for stakeholders

**Industry Standards:**
- Professional documentation
- Deadline compliance
- Efficient workflows
- Clear communication
- Quality deliverables

**Key Success Factors:**
- Plan thoroughly before starting
- Manage time effectively
- Document all decisions
- Communicate regularly with supervisors
- Focus on quality over quantity
                """,
                "key_points": ["Major project demonstrating all first-year skills", "Can be individual, group, or internship-based", "Must follow professional standards", "Documentation and communication are critical"]
            },
            {
                "title": "Project Planning & Scoping",
                "content": """
**Effective Project Planning**

Good planning is the foundation of successful project execution.

**1. Defining Your Problem**

Ask these questions:
- What business question am I answering?
- Who will use my findings?
- What decisions will this support?
- What does success look like?

**Problem Statement Template:**
"[Organization] needs to understand [topic] in order to [action]. This analysis will examine [data] to provide [deliverable] by [deadline]."

**Example:**
"TechCorp needs to understand customer churn patterns in order to improve retention strategies. This analysis will examine 2 years of customer data to provide a retention recommendation report by March 15."

**2. Scoping Your Project**

**In Scope vs. Out of Scope:**

| In Scope | Out of Scope |
|----------|--------------|
| Questions you WILL answer | Questions for future work |
| Data sources you WILL use | Data you won't access |
| Tools you WILL apply | Tools beyond your skills |
| Time period covered | Historical extensions |

**3. Timeline Planning**

Recommended time allocation:
- **Planning & Research**: 15-20%
- **Data Collection & Cleaning**: 20-25%
- **Analysis & Exploration**: 25-30%
- **Reporting & Visualization**: 20-25%
- **Review & Finalization**: 10-15%

**4. Milestone Setting**

Create checkpoints:
- Week 2: Problem defined, data sources identified
- Week 4: Data collected and cleaned
- Week 6: Initial analysis complete
- Week 8: Draft report ready
- Week 10: Final submission

**Risk Planning:**
- What could go wrong?
- What's your backup plan?
- Who can help if you're stuck?
                """,
                "key_points": ["Define clear problem statements", "Explicitly scope what's in and out", "Allocate time realistically", "Set milestones and prepare for risks"]
            },
            {
                "title": "Working with Real-World Data",
                "content": """
**Finding and Using Real Data**

For your exam project, you'll work with real-world dataâ€”either from an internship partner or publicly available sources.

**Internship Projects (Recommended)**

Benefits:
- Real business problems
- Mentorship from professionals
- Industry experience for your CV
- Networking opportunities

Finding internship projects:
- Approach local businesses
- Use school connections
- Check company internship programs
- Reach out to alumni

**Public Data Sources**

If not doing an internship:
- **Statistics Norway (SSB)**: Norwegian economic/demographic data
- **Kaggle**: Curated datasets for analysis
- **EU Open Data Portal**: European datasets
- **World Bank**: Global development data
- **Government Open Data**: Many countries publish data

**Ethical Data Sourcing**

Remember your GDPR and ethics training:
- âœ“ Ensure you have permission to use the data
- âœ“ Anonymize personal information
- âœ“ Document data sources properly
- âœ“ Check licensing requirements
- âœ— Don't scrape data without permission
- âœ— Don't use data for unintended purposes

**Data Quality Assessment**

Before starting analysis:
1. How complete is the data?
2. What's the time period covered?
3. Are there obvious errors or outliers?
4. Is the sample representative?
5. What transformations are needed?

**Documenting Your Data**

Create a data dictionary:
| Field | Type | Description | Source | Notes |
|-------|------|-------------|--------|-------|
| customer_id | Integer | Unique identifier | CRM | Primary key |
| revenue | Decimal | Monthly revenue | Finance DB | In NOK |
                """,
                "key_points": ["Internship projects provide valuable real-world experience", "Many public data sources are available", "Always source data ethically and legally", "Document data thoroughly"]
            },
            {
                "title": "Executing Your Analysis",
                "content": """
**Applying Your Skills**

Your exam project is where you demonstrate mastery of all first-year skills.

**Tools & Techniques to Apply:**

**From Spreadsheet Fundamentals:**
- Data cleaning in Excel/Sheets
- Pivot tables for summarization
- Formulas for calculations
- Charts for visualization

**From Statistical Tools:**
- Descriptive statistics
- Correlation analysis
- Z-scores for outlier detection
- Hypothesis testing (if applicable)

**From Programming Fundamentals:**
- Python for data manipulation
- pandas for DataFrames
- Automated data cleaning
- Reproducible analysis scripts

**From Databases:**
- SQL queries for data extraction
- Joining multiple data sources
- Aggregations and groupings

**From Data Visualization:**
- Choosing appropriate chart types
- Following design principles
- Creating accessible visualizations
- Building dashboards

**Analysis Workflow:**

1. **Explore**: Understand your data
   - Summary statistics
   - Initial visualizations
   - Pattern identification

2. **Clean**: Prepare for analysis
   - Handle missing values
   - Fix errors
   - Standardize formats

3. **Analyze**: Answer your questions
   - Apply appropriate methods
   - Document your approach
   - Validate findings

4. **Interpret**: Extract meaning
   - What do the results mean?
   - What's actionable?
   - What are the limitations?

**Common Pitfalls:**
- Starting analysis before understanding the data
- Skipping data cleaning
- Not documenting your process
- Overcomplicating the analysis
- Ignoring limitations
                """,
                "key_points": ["Apply all tools learned in Semester 1-2", "Follow a systematic analysis workflow", "Document your process thoroughly", "Avoid common analysis pitfalls"]
            },
            {
                "title": "Professional Documentation",
                "content": """
**Documentation Standards**

Professional documentation distinguishes student work from industry-ready deliverables.

**What to Document:**

**1. Project Documentation**
- Project plan and timeline
- Scope and objectives
- Stakeholder requirements
- Risk assessment

**2. Technical Documentation**
- Data dictionary
- Methodology description
- Code comments
- Version history

**3. Analysis Documentation**
- Decisions made and why
- Assumptions stated
- Limitations acknowledged
- Findings validated

**Report Structure (from Analysis Reporting course):**

1. **Executive Summary**: Key findings for decision-makers
2. **Introduction**: Problem context and objectives
3. **Methodology**: How you approached the analysis
4. **Data Description**: Sources, preparation, limitations
5. **Findings**: Results with visualizations
6. **Recommendations**: Actionable next steps
7. **Appendix**: Technical details, additional charts

**Professional Writing Tips:**

- Write for your audience (technical vs. non-technical)
- Use clear, concise language
- Support claims with evidence
- Include visualizations effectively
- Proofread carefully

**Version Control:**

Even for solo projects:
- Use meaningful file names
- Track major changes
- Keep backups
- Document what changed

**Example naming:**
`project_report_v1.0_2025-03-10.docx`
`analysis_script_v2_cleaned_data.py`
                """,
                "key_points": ["Document project, technical, and analysis aspects", "Follow professional report structure", "Write clearly for your audience", "Maintain version control"]
            },
            {
                "title": "Presentation & Defense",
                "content": """
**Presenting Your Project**

Your exam includes presenting your work to evaluators. This is your chance to demonstrate both technical competence and communication skills.

**Presentation Structure:**

**1. Opening (2 minutes)**
- Introduce yourself and project
- State the problem you solved
- Hook the audience with a key finding

**2. Context (3 minutes)**
- Business background
- Why this matters
- Your objectives

**3. Methodology (3 minutes)**
- Data sources used
- Tools and techniques applied
- Key decisions made

**4. Findings (5 minutes)**
- Main results with visuals
- Insights discovered
- Patterns identified

**5. Recommendations (3 minutes)**
- Actionable suggestions
- Expected impact
- Next steps

**6. Conclusion (2 minutes)**
- Summary of key points
- Lessons learned
- Questions invitation

**Presentation Tips:**

**Visual Design:**
- One key message per slide
- Minimal text, maximum visuals
- Consistent formatting
- Readable fonts (18pt minimum)

**Delivery:**
- Practice out loud multiple times
- Time yourself
- Prepare for questions
- Speak to the audience, not slides

**Handling Questions:**

Common question types:
- "Why did you choose this approach?"
- "What were the limitations?"
- "What would you do differently?"
- "How confident are you in these findings?"

Answering strategies:
- Pause before answering
- Be honest about limitations
- Refer to your documentation
- Say "I don't know" if needed, then explain how you'd find out

**Professional Terminology:**

Use appropriate industry language:
- "The analysis revealed..." (not "I found...")
- "The data suggests..." (not "It's obvious that...")
- "I recommend..." (not "You should...")
- "Within the scope of this project..." (acknowledging limits)
                """,
                "key_points": ["Structure presentation logically", "Practice delivery multiple times", "Prepare for tough questions", "Use professional terminology"]
            },
            {
                "title": "Quality & Self-Assessment",
                "content": """
**Ensuring Quality**

Before submission, critically assess your own work against professional standards.

**Quality Checklist:**

**Project Execution:**
- â˜ Problem clearly defined and scoped
- â˜ Data sourced ethically
- â˜ Methodology appropriate for the problem
- â˜ Analysis thorough and documented
- â˜ Findings validated

**Technical Quality:**
- â˜ Data cleaned properly
- â˜ Calculations verified
- â˜ Code/formulas documented
- â˜ No obvious errors
- â˜ Reproducible process

**Report Quality:**
- â˜ Clear executive summary
- â˜ Logical structure
- â˜ Visualizations accessible
- â˜ Recommendations actionable
- â˜ Proofread for errors

**Presentation Quality:**
- â˜ Within time limit
- â˜ Key points clear
- â˜ Visuals readable
- â˜ Prepared for questions

**Self-Reflection Questions:**

Ask yourself:
1. Did I answer the original question?
2. Would a stakeholder find this useful?
3. Are my conclusions supported by evidence?
4. Did I acknowledge limitations honestly?
5. What would I do differently next time?

**Common Issues to Check:**

| Issue | How to Check |
|-------|--------------|
| Missing data handling | Document all decisions |
| Overclaiming | Review language for hedging |
| Unclear visuals | Show to someone unfamiliar |
| Weak recommendations | Are they specific and actionable? |
| Poor time management | Compare to original plan |

**Getting Feedback:**

Before submission:
- Have a peer review your work
- Ask your supervisor for input
- Read your report from the audience's perspective
- Practice your presentation with feedback

**After Submission:**

Document lessons learned:
- What went well?
- What was challenging?
- What skills do you need to develop?
- How will you apply this experience?
                """,
                "key_points": ["Use quality checklists before submission", "Self-assess honestly against standards", "Get feedback from peers and supervisors", "Document lessons learned for future projects"]
            }
        ],
        "exercises": [
            {
                "title": "Write a Problem Statement",
                "type": "practical",
                "question": "A local restaurant chain (5 locations) has noticed declining customer visits over the past 6 months. They have data on daily sales, customer reviews, and competitor activity. Write a clear problem statement for a data analysis project.",
                "answer": "Strong problem statement: 'Restaurant Group Oslo needs to understand the drivers of declining customer visits over the past 6 months in order to develop strategies to reverse the trend. This analysis will examine daily sales data, customer review sentiment, and competitor activity across all 5 locations to identify patterns and provide actionable recommendations by [deadline]. The project will focus on identifying which locations are most affected, what time periods show the greatest decline, and whether customer feedback reveals specific issues.' This statement includes: who needs the analysis, what question needs answering, what data will be used, what the deliverable is, and the scope boundaries.",
                "hint": "Include: who, what problem, what data, what deliverable, and any scope boundaries"
            },
            {
                "title": "Create a Project Timeline",
                "type": "scenario",
                "question": "You have 8 weeks to complete your exam project analyzing e-commerce sales data. Create a timeline with phases and milestones, allocating appropriate percentages of time.",
                "answer": "8-Week Project Timeline: WEEK 1-2 (25%): Planning & Data Collection - Week 1: Define problem, identify stakeholders, scope project, create project plan. Week 2: Obtain data access, initial data exploration, create data dictionary. Milestone: Project plan approved, data ready. WEEK 3-4 (25%): Data Cleaning & Preparation - Week 3: Clean data, handle missing values, fix errors. Week 4: Transform data for analysis, validate quality. Milestone: Clean dataset documented. WEEK 5-6 (25%): Analysis & Visualization - Week 5: Exploratory analysis, identify patterns. Week 6: Deep analysis, create visualizations. Milestone: Key findings identified. WEEK 7 (12.5%): Reporting - Draft report, integrate visualizations, write recommendations. Milestone: Draft report for review. WEEK 8 (12.5%): Review & Finalize - Peer review, revisions, finalize presentation, submit. Milestone: Final submission. Buffer: Built into each phase for unexpected issues.",
                "hint": "Consider the recommended percentages: Planning 15-20%, Data prep 20-25%, Analysis 25-30%, Reporting 20-25%, Review 10-15%"
            },
            {
                "title": "Prepare for Tough Questions",
                "type": "scenario",
                "question": "During your project presentation, an evaluator asks: 'Your sample size is only 500 customers out of 10,000. How confident can we be in your findings?' How would you respond professionally?",
                "answer": "Professional response: 'That's an important consideration. Our sample of 500 customers represents 5% of the total customer base, which provides a statistically meaningful sample with approximately a 4% margin of error at 95% confidence. The sample was selected randomly to ensure representativeness. I verified the sample demographics matched the overall customer base in terms of age distribution, location, and purchase history. While a larger sample would increase precision, the patterns we identifiedâ€”particularly the 25% difference in satisfaction between segmentsâ€”are well above the margin of error, giving us confidence in the directional findings. In the recommendations section, I've noted that implementing changes with a pilot group would allow validation before full rollout.' Key elements: 1) Acknowledge the valid concern, 2) Explain your methodology, 3) Quantify the confidence level, 4) Note limitations, 5) Suggest mitigation.",
                "hint": "Acknowledge the concern, explain your reasoning, quantify confidence, note limitations, suggest mitigation"
            }
        ],
        "quiz": [
            {
                "question": "What percentage of project time should typically be allocated to planning and research?",
                "options": ["5-10%", "15-20%", "40-50%", "60-70%"],
                "correct": 1,
                "explanation": "15-20% is the recommended allocation for planning and research. Rushing into analysis without proper planning often leads to wasted effort later."
            },
            {
                "question": "What type of project is encouraged for Exam Project 1?",
                "options": ["Theoretical research", "Internship with a real company", "Simulation exercise", "Literature review"],
                "correct": 1,
                "explanation": "Internship projects are encouraged as they provide real-world experience, professional mentorship, and practical skills that enhance your employability."
            },
            {
                "question": "When presenting findings, you should:",
                "options": ["Use as much technical jargon as possible", "Lead with the most important finding", "Save conclusions for the very end", "Avoid mentioning limitations"],
                "correct": 1,
                "explanation": "Lead with the most important finding to immediately engage your audience. Technical jargon should be minimized, conclusions should be clear throughout, and limitations should be acknowledged honestly."
            },
            {
                "question": "A good problem statement should include:",
                "options": ["Only the data sources available", "The organization, question, and deliverable", "Just the deadline", "Technical methodology details"],
                "correct": 1,
                "explanation": "A good problem statement includes who needs the analysis (organization), what question needs answering, and what the deliverable will be. Methodology comes later in planning."
            },
            {
                "question": "Before submitting your project, you should:",
                "options": ["Skip peer review to save time", "Only check for spelling errors", "Use a quality checklist and get feedback", "Assume everything is correct"],
                "correct": 2,
                "explanation": "Using a quality checklist and getting feedback from peers helps catch errors and improve your work. Never skip the review phaseâ€”fresh eyes often spot issues you've missed."
            }
        ]
    }
}

# Course lessons for Learn & Practice section
course_lessons = {
    "FI1BBDF05": [
        {
            "lesson_number": "1.1",
            "title": "Introduction to Data Analysis",
            "content": """
### What is Data Analysis?

Understanding and using data is becoming increasingly important in the quickly changing digital environment. The discipline of **data analysis** is focused on drawing insightful conclusions from data. It involves methods and tools for gathering unprocessed data and then turning it into information that can help with decision-making. Typically, specialised hardware and software are used to accomplish this.

#### ðŸŽ¯ Core Concept

<div class="mermaid">
flowchart LR
    A[Raw Data<br/>ðŸ“Š<br/>Unprocessed Information] -->|Processing| B[Clean & Transform<br/>ðŸ”„]
    B -->|Analysis| C[Extract Patterns<br/>ðŸ”]
    C -->|Insights| D[Decision-Making<br/>ðŸ’¡]
    
    style A fill:#e3f2fd,stroke:#2196F3,stroke-width:2px
    style B fill:#fff3cd,stroke:#ffc107,stroke-width:2px
    style C fill:#c8e6c9,stroke:#4CAF50,stroke-width:2px
    style D fill:#f3e5f5,stroke:#9C27B0,stroke-width:2px
</div>

#### ðŸ“Š Types of Data Analysis

Based on the unique requirements of a task or project, different methods and techniques are used in the broad field of data analysis. Here are the most popular types:

| Type | Purpose | Example |
|------|---------|---------|
| **Text Analysis** | Extract insights from text data | Analyzing customer reviews |
| **Statistical Analysis** | Use statistical methods to understand data | Calculating averages, correlations |
| **Diagnostic Analysis** | Understand why something happened | Root cause analysis |
| **Predictive Analysis** | Forecast future trends | Sales forecasting |
| **Data Mining** | Discover patterns in large datasets | Market basket analysis |

#### ðŸ”— Related Fields

Data analysis is closely related to several other fields that enhance its capabilities:

**1. Programming** ðŸ’»
- Create scripts to pre-process, transform, and analyse data
- Languages: Python, R, SQL
- Enables: Large datasets, intricate calculations, task automation

**2. Machine Learning** ðŸ¤–
- Subset of artificial intelligence
- Systems learn from experience without explicit programming
- Build predictive models from historical data
- Helps with trend forecasting and data-driven decision-making

**3. Statistics** ðŸ“ˆ
- Foundation of data analysis
- Methods for understanding and interpreting data
- From basic (mean, median, mode) to advanced (hypothesis testing, regression)

**4. Data Visualisation** ðŸ“Š
- Communicate complex findings using graphs and charts
- Tools: Tableau, PowerBI, Matplotlib, Seaborn
- Makes data accessible to non-technical audiences

**5. Business Intelligence** ðŸ¢
- Technology-driven process for data analysis and information presentation
- Helps executives and managers make informed business decisions
- Data analysts work within BI boundaries to help organizations use data effectively

**6. Big Data** ðŸŒ
- Focuses on handling massive datasets that traditional software can't address
- Techniques: MapReduce, Hadoop frameworks
- Essential for data analysts working with extremely large datasets

#### ðŸŽ¨ Visual Overview: Data Analysis Ecosystem

<div class="mermaid">
graph TD
    A[Data Analysis Fundamentals] --> B[Statistics]
    A --> C[Machine Learning]
    A --> D[Business Intelligence]
    
    B --> E[Programming & Tools]
    C --> E
    D --> E
    
    E --> F[Python]
    E --> G[R]
    E --> H[SQL]
    E --> I[Tableau]
    
    E --> J[Big Data]
    J --> K[Hadoop]
    J --> L[Spark]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px
    style D fill:#4ECDC4,stroke:#2C7873,stroke-width:2px
    style E fill:#95E1D3,stroke:#4ECDC4,stroke-width:2px
    style J fill:#FFA07A,stroke:#FF6347,stroke-width:2px
</div>

#### ðŸ”„ Data Analysis Workflow

<div class="mermaid">
flowchart TD
    Start([Start Analysis]) --> Collect[Collect Data]
    Collect --> Clean[Clean Data]
    Clean --> Explore[Explore Data]
    Explore --> Analyze[Analyze Data]
    Analyze --> Visualize[Visualize Results]
    Visualize --> Interpret[Interpret Findings]
    Interpret --> Decision{Make Decision}
    Decision -->|Action Needed| Act[Take Action]
    Decision -->|More Analysis| Analyze
    Act --> End([End])
    
    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style Collect fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Clean fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px
    style Explore fill:#4ECDC4,stroke:#2C7873,stroke-width:2px
    style Analyze fill:#95E1D3,stroke:#4ECDC4,stroke-width:2px
    style Visualize fill:#FFA07A,stroke:#FF6347,stroke-width:2px
    style Interpret fill:#C7CEEA,stroke:#8B9DC3,stroke-width:2px
    style Decision fill:#F8B500,stroke:#E6A200,stroke-width:3px
    style Act fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style End fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
</div>

#### ðŸ‘¥ Roles and Responsibilities

The discipline of data analysis encompasses a variety of roles and responsibilities. In data-driven businesses, these various roles frequently overlap, but each has a specific function in handling, comprehending, and presenting data. The following are descriptions of the general roles and responsibilities within the field of data analysis:

**1. Data Analyst** ðŸ“Š
- Gathers, organises, and analyses data statistically
- Interprets data using statistical methods
- Delivers ongoing reports and identifies patterns and trends
- Maintains databases and data systems
- Designs and implements data collection methods
- Enhances statistical effectiveness and data quality

**2. Data Scientist** ðŸ”¬
- Uses algorithms and predictive models to create new data modelling processes
- Creates Artificial Intelligence (AI) systems using machine learning
- Applies ML algorithms to data, text, images, video, audio, and other data types
- Tasks include: enhancing marketing efficiency, forecasting election outcomes, diagnosing diseases

**3. Data Engineer** âš™ï¸
- Sets up the 'big data' infrastructure for data scientists
- Designs, builds, and integrates data from various sources
- Manages software systems for big data
- Ensures data is easily accessible and performance-optimised
- Writes complex queries

**4. Business Intelligence Analyst** ðŸ’¼
- Analyses data to create a clearer picture of the company's position
- Identifies business and market trends
- Gathers data from various sources
- Develops thorough reports and dashboards
- Presents findings to business decision-makers

**5. Data Architect** ðŸ—ï¸
- Focuses on design and management of high-level structures in data ecosystem
- Creates frameworks for data engineers to implement
- Big-picture aspects of data management
- Closely related to data engineer role (design vs implementation)

**6. Statistician** ðŸ“ˆ
- Uses statistical methods to gather and analyse data
- Assists in resolving practical issues in business, engineering, healthcare, and other fields
- Chooses what information is needed and how to gather it
- Plans surveys and experiments
- Examines data to identify patterns and make inferences
- Presents findings

**7. Machine Learning Engineer** ðŸ¤–
- Creates and maintains platforms for machine learning projects
- Expert programmers who develop machines and systems
- Systems that can learn and apply knowledge independently
- Develops algorithms and programs for autonomous device behavior

**8. Data Visualisation Specialist** ðŸŽ¨
- Turns complex findings into understandable visual presentations
- Produces dashboards, charts, and graphs
- Uses tools like Tableau, PowerBI, Python and R libraries
- Supports business decision-making through visual communication

#### ðŸŽ¯ Roles Relationship Diagram

<div class="mermaid">
graph TB
    subgraph "Data Collection & Infrastructure"
        DE[Data Engineer<br/>âš™ï¸<br/>Big Data Infrastructure]
        DA[Data Architect<br/>ðŸ—ï¸<br/>System Design]
    end
    
    subgraph "Data Processing & Analysis"
        DA2[Data Analyst<br/>ðŸ“Š<br/>Statistical Analysis]
        DS[Data Scientist<br/>ðŸ”¬<br/>ML & AI Models]
        ST[Statistician<br/>ðŸ“ˆ<br/>Statistical Methods]
    end
    
    subgraph "Visualization & Reporting"
        BI[BI Analyst<br/>ðŸ’¼<br/>Business Insights]
        DV[Data Visualization<br/>Specialist<br/>ðŸŽ¨<br/>Charts & Dashboards]
    end
    
    subgraph "Advanced Implementation"
        MLE[ML Engineer<br/>ðŸ¤–<br/>ML Platforms]
    end
    
    DA --> DE
    DE --> DA2
    DE --> DS
    DA2 --> ST
    DS --> MLE
    DA2 --> BI
    DS --> BI
    ST --> BI
    BI --> DV
    DS --> DV
    
    style DE fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style DA fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style DA2 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style DS fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style ST fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style BI fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style DV fill:#FFA07A,stroke:#FF6347,stroke-width:2px
    style MLE fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

#### ðŸ”„ Data Analysis Team Workflow

<div class="mermaid">
flowchart LR
    A[Data Architect<br/>Designs System] --> B[Data Engineer<br/>Builds Infrastructure]
    B --> C[Data Sources]
    C --> D[Data Analyst<br/>Statistical Analysis]
    C --> E[Data Scientist<br/>ML Models]
    C --> F[Statistician<br/>Experiments]
    D --> G[BI Analyst<br/>Business Insights]
    E --> H[ML Engineer<br/>Deploy Models]
    F --> G
    G --> I[Data Visualization<br/>Specialist]
    H --> I
    I --> J[Decision Makers<br/>ðŸ’¡]
    
    style A fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style B fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style C fill:#95E1D3,stroke:#4ECDC4,stroke-width:2px
    style D fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style G fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style I fill:#FFA07A,stroke:#FF6347,stroke-width:2px
    style J fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
</div>

#### ðŸ“‹ Roles Comparison Table

| Role | Primary Focus | Key Skills | Typical Output |
|------|---------------|------------|---------------|
| **Data Analyst** | Statistical analysis & reporting | Statistics, SQL, Excel | Reports, insights |
| **Data Scientist** | ML models & AI systems | Python, ML, algorithms | Predictive models, AI systems |
| **Data Engineer** | Infrastructure & pipelines | Big data tools, databases | Data pipelines, infrastructure |
| **BI Analyst** | Business insights | Business acumen, dashboards | Business reports, dashboards |
| **Data Architect** | System design | System design, architecture | Data architecture frameworks |
| **Statistician** | Statistical methods | Advanced statistics, R | Statistical analysis, experiments |
| **ML Engineer** | ML platforms | Programming, ML ops | ML platforms, deployed models |
| **Data Visualization Specialist** | Visual communication | Tableau, PowerBI, design | Charts, dashboards, visualizations |

#### ðŸ’¡ Important Notes

<div class="key-concept">
**Role Overlap**: In smaller organisations, one person might oversee several of these positions. Organisations may have their own internal definition of roles and may use role names specific to their organisation.
</div>

#### ðŸŽ¯ Skills Requirements

The two types of skills significant in the workplace are **hard skills** and **soft skills**, which frequently complement one another. Technical or specialised skills are considered hard skills because they are simple to define and quantify. Formal education, training courses, certification programmes, or on-the-job training are frequently used to acquire them. Hard skills are often unique to a given job or task.

##### ðŸ’» Hard Skills

**1. Mathematical and Statistical Skills** ðŸ“Š
- Basic understanding of statistics and maths when interpreting data
- Understanding the data, performing calculations, and reaching reliable conclusions
- Essential for producing accurate reports

**2. Programming Skills** ðŸ’»
- Languages: Python, R, SQL
- Not all data analyst roles require it, but it's a tremendous asset
- Enables automation of procedures
- Clean up and process massive amounts of data
- Carry out more complex analyses

**3. Database Management/SQL** ðŸ—„ï¸
- Fundamental skill for a data analyst
- Ability to write SQL queries and comprehend database systems
- Extract, modify, manage, and store data
- Comfortable working with database systems

**4. Data Visualisation Skills** ðŸ“ˆ
- One of the most important aspects of a data analyst's job
- Present data in an understandable visual format
- Tools: Tableau, PowerBI, Python libraries (Matplotlib, Seaborn)
- Create graphs, charts, and other visualisations

**5. Knowledge of Data Cleaning/Wrangling** ðŸ§¹
- Rarely is data pure and prepared for analysis
- Handle missing or inconsistent data
- Prepare data for analysis through cleaning and pre-processing
- Essential for data quality

##### ðŸ¤ Soft Skills

Soft skills often correlate with a person's personality traits, social graces, communication skills, and personal attributes. Soft skills are less concrete and more difficult to measure. They have to do with how you conduct yourself at work and with others.

**1. Analytical Skills** ðŸ”
- Examine statistics, patterns, and other information
- Draw conclusions from the results
- Pose the proper inquiries
- Interpret the findings
- Offer perceptions that can aid decision-making

**2. Problem-Solving Skills** ðŸ§©
- Troubleshoot and find solutions
- Handle problems with data or analysis
- Troubleshoot data collection procedures
- Determine how to deal with incomplete or inconsistent data

**3. Communication Skills** ðŸ’¬
- Strong written and verbal communication skills
- Convey findings to audiences that may not share technical expertise
- Communicate complex ideas in a way that others can comprehend
- Present findings clearly and effectively

**4. Understanding of the Business/Industry** ðŸ¢
- Business or industry knowledge is significantly beneficial
- Comprehend the context of the data being used
- Produce pertinent and beneficial analyses for the enterprise
- Connect data insights to business objectives

#### ðŸ“Š Skills Overview Diagram

<div class="mermaid">
graph TB
    subgraph "Hard Skills ðŸ’»"
        HS1[Mathematical &<br/>Statistical Skills<br/>ðŸ“Š]
        HS2[Programming Skills<br/>Python, R, SQL<br/>ðŸ’»]
        HS3[Database Management<br/>SQL Queries<br/>ðŸ—„ï¸]
        HS4[Data Visualization<br/>Tableau, PowerBI<br/>ðŸ“ˆ]
        HS5[Data Cleaning<br/>Wrangling<br/>ðŸ§¹]
    end
    
    subgraph "Soft Skills ðŸ¤"
        SS1[Analytical Skills<br/>Critical Thinking<br/>ðŸ”]
        SS2[Problem-Solving<br/>Troubleshooting<br/>ðŸ§©]
        SS3[Communication<br/>Written & Verbal<br/>ðŸ’¬]
        SS4[Business Understanding<br/>Industry Knowledge<br/>ðŸ¢]
    end
    
    HS1 --> Success[Successful<br/>Data Analyst<br/>âœ…]
    HS2 --> Success
    HS3 --> Success
    HS4 --> Success
    HS5 --> Success
    SS1 --> Success
    SS2 --> Success
    SS3 --> Success
    SS4 --> Success
    
    style HS1 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style HS2 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style HS3 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style HS4 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style HS5 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style SS1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style SS2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style SS3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style SS4 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Success fill:#FFD700,stroke:#B8860B,stroke-width:3px
</div>

#### ðŸ”„ Skills Development Path

<div class="mermaid">
flowchart TD
    Start([Begin Learning]) --> Foundation[Foundation Skills]
    
    Foundation --> Hard[Hard Skills Development]
    Foundation --> Soft[Soft Skills Development]
    
    Hard --> H1[Math & Statistics]
    Hard --> H2[Programming]
    Hard --> H3[Database/SQL]
    Hard --> H4[Visualization]
    Hard --> H5[Data Cleaning]
    
    Soft --> S1[Analytical Thinking]
    Soft --> S2[Problem Solving]
    Soft --> S3[Communication]
    Soft --> S4[Business Acumen]
    
    H1 --> Practice[Practice & Apply]
    H2 --> Practice
    H3 --> Practice
    H4 --> Practice
    H5 --> Practice
    S1 --> Practice
    S2 --> Practice
    S3 --> Practice
    S4 --> Practice
    
    Practice --> Experience[Gain Experience]
    Experience --> Expert[Expert Data Analyst]
    
    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style Foundation fill:#95E1D3,stroke:#4ECDC4,stroke-width:2px
    style Hard fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style Soft fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Practice fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Experience fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style Expert fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ“‹ Skills Comparison Table

| Skill Type | Characteristics | How Acquired | Examples |
|------------|----------------|--------------|----------|
| **Hard Skills** | Technical, quantifiable, job-specific | Formal education, training, certifications | SQL, Python, Statistics, Tableau |
| **Soft Skills** | Personality-based, interpersonal | Experience, practice, self-development | Communication, Problem-solving, Analytical thinking |
| **Complementary** | Both essential for success | Continuous learning and development | Technical expertise + Communication = Effective analyst |

#### ðŸ’¡ Key Insights

<div class="important-info">
**Why Both Skills Matter**: In the workplace, it's crucial to have both hard and soft skills. Technical expertise is frequently built on a foundation of hard skills. Still, soft skills are essential for effectively putting those technical skills to use, working with teammates, and fostering a positive work environment. Employers frequently place a high value on soft skills because they can be more challenging to teach than hard skills and because they are crucial to a person's capacity for collaborative work.
</div>

#### ðŸ“… A Day in the Life of a Data Analyst

There are never any two alike days at this job. There are always fresh data sets to examine, new issues to resolve, and fresh revelations to make. People who enjoy puzzles, have a keen eye for detail, and are curious to find the stories hidden in data are well suited for this role.

Let's discuss a typical day for a data analyst working in a mid-sized tech company. A data analyst's day can vary greatly depending on the industry, company size, and specific role.

#### â° Daily Timeline

<div class="mermaid">
gantt
    title A Day in the Life of a Data Analyst
    dateFormat HH:mm
    axisFormat %H:%M
    
    section Morning
    Coffee & Email Check           :08:00, 30m
    Daily Stand-up Meeting         :08:30, 30m
    Draft Sales Report             :09:00, 90m
    Data Validation & Cleaning     :09:00, 90m
    
    section Analysis
    Data Analysis (Python)         :10:30, 90m
    Lunch Break                    :12:00, 60m
    
    section Afternoon
    Create Visualizations          :13:00, 60m
    Complete & Send Report         :14:00, 30m
    Prepare New Dataset            :14:30, 60m
    ML Team Meeting                :15:30, 30m
    Wrap Up & Planning             :16:00, 30m
</div>

#### ðŸ“‹ Detailed Daily Activities

**08:00 - Morning Routine** â˜•
- Start the day with coffee
- Check email for updates from team and departments
- Respond to critical messages
- Scan for urgent data requests requiring immediate attention

**08:30 - Team Collaboration** ðŸ‘¥
- Attend daily stand-up meeting
- Discuss daily tasks and project updates
- Share problems and upcoming deadlines
- Communicate current work: cleaning new dataset for predictive model and quarterly sales report

**09:00 - Data Preparation** ðŸ“Š
- Begin drafting sales report for current quarter
- Use SQL queries to retrieve data from company database
- Verify data accuracy
- Look for outliers and inconsistencies
- Check for gaps in data

**10:30 - Data Analysis** ðŸ”
- Begin analysis after validating and organizing data
- Examine sales patterns
- Identify top-performing products
- Understand consumer behavior
- Compare current quarter with previous quarters
- Use Python with Pandas and NumPy for data manipulation and calculations

**12:00 - Break** ðŸ½ï¸
- Lunch break
- Catch up with co-workers

**13:00 - Data Visualization** ðŸ“ˆ
- Visualize interesting insights from sales data
- Use Tableau to create dynamic dashboards
- Display data on:
  - Customer behavior
  - Product performance
  - Sales trends

**14:00 - Report Delivery** ðŸ“¤
- Complete the sales report
- Send report with summary of findings to sales team
- Make yourself available for additional analysis or questions

**14:30 - New Project Preparation** ðŸ†•
- Prepare fresh dataset for predictive model
- Handle large dataset with many missing values
- Perform extensive pre-processing and cleaning

**15:30 - Cross-Team Collaboration** ðŸ¤
- Meeting with machine learning team
- Review cleaned data together
- Explain what each variable means
- Discuss problems encountered during cleaning
- Hand off data for model construction

**16:00 - End of Day** âœ…
- Finish up for the day
- Set to-do items for tomorrow:
  - Finish data cleaning
  - Begin exploratory data analysis on new dataset
  - Respond to sales team comments on quarterly report

#### ðŸ”„ Daily Workflow Diagram

<div class="mermaid">
flowchart TD
    Start([Start of Day<br/>08:00]) --> Email[Check Email<br/>& Updates<br/>â˜•]
    Email --> Standup[Daily Stand-up<br/>Team Meeting<br/>ðŸ‘¥]
    Standup --> SQL[SQL Queries<br/>Retrieve Data<br/>ðŸ—„ï¸]
    SQL --> Validate[Validate Data<br/>Check Quality<br/>âœ“]
    Validate --> Analyze[Analyze Data<br/>Python/Pandas<br/>ðŸ”]
    Analyze --> Lunch[Lunch Break<br/>ðŸ½ï¸]
    Lunch --> Visualize[Create Visualizations<br/>Tableau<br/>ðŸ“ˆ]
    Visualize --> Report[Complete Report<br/>Send to Team<br/>ðŸ“¤]
    Report --> Clean[Clean New Dataset<br/>Pre-processing<br/>ðŸ§¹]
    Clean --> Meeting[ML Team Meeting<br/>Review Data<br/>ðŸ¤]
    Meeting --> Plan[Plan Tomorrow<br/>Set To-Dos<br/>âœ…]
    Plan --> End([End of Day<br/>16:00])
    
    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style Email fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Standup fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style SQL fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style Validate fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style Analyze fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style Lunch fill:#FFA07A,stroke:#FF6347,stroke-width:2px
    style Visualize fill:#95E1D3,stroke:#4ECDC4,stroke-width:2px
    style Report fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Clean fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style Meeting fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style Plan fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style End fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
</div>

#### ðŸŽ¯ Key Activities Breakdown

<div class="mermaid">
pie title Daily Time Distribution
    "Data Analysis" : 90
    "Data Preparation" : 90
    "Visualization" : 60
    "Meetings" : 60
    "Report Writing" : 30
    "Email & Planning" : 60
    "Lunch" : 60
</div>

#### ðŸ’¡ Key Insights from a Typical Day

<div class="key-concept">
**Variety is Key**: No two days are the same. Data analysts work on multiple projects simultaneously, handle various data sources, and collaborate with different teams. This variety keeps the role interesting and challenging.

**Technical Skills in Action**: The day demonstrates the practical application of:
- SQL for data retrieval
- Python (Pandas, NumPy) for analysis
- Tableau for visualization
- Data cleaning and validation techniques

**Collaboration is Essential**: The role involves constant communication with:
- Team members (stand-ups)
- Other departments (sales team)
- Cross-functional teams (ML team)

**Problem-Solving Focus**: Much of the day involves identifying and solving data quality issues, finding patterns, and making data-driven recommendations.
</div>

#### ðŸ“Š Levels of Data Analysis

Four levels or types of data analysis can be distinguished, each offering more profound insights and increasing complexity. These are the four levels in this hierarchy: **descriptive**, **diagnostic**, **predictive**, and **prescriptive analytics**.

##### 1. Descriptive Analytics ðŸ“ˆ

The most fundamental type of data analytics is **descriptive analytics**. The raw data must be comprehended and summarised to find patterns and trends. The main goal of descriptive analytics is to summarise or describe data in a way humans can understand.

**Key Questions**: "What happened?"

**Examples**:
- Statistical data on website traffic
- Social media post metrics
- Monthly revenue reports
- Sales summaries
- Customer demographics

**Characteristics**:
- Most basic level
- Summarizes historical data
- Identifies patterns and trends
- Provides context for what occurred

##### 2. Diagnostic Analytics ðŸ”

Deeper data analysis is used in **diagnostic analytics** to identify the factors that led to a particular result. It looks at information or content to address concerns about the reasons behind events. More varied data inputs and statistical techniques are used to analyse the data and look for patterns or correlations.

**Key Questions**: "Why did it happen?"

**Examples**:
- Investigating decline in quarterly sales
- Root cause analysis
- Identifying factors affecting performance
- Understanding correlations between variables

**Characteristics**:
- Deeper than descriptive
- Investigates causes
- Uses statistical techniques
- Looks for patterns and correlations
- Explains past events

##### 3. Predictive Analytics ðŸ”®

Forecasting future outcomes based on historical data is the goal of **predictive analytics**. Regression analysis, time series analysis, and other sophisticated statistical techniques are utilised.

**Key Questions**: "What is likely to occur in the future?" and "What trends can we anticipate?"

**Examples**:
- Predicting sales for upcoming quarter
- Forecasting customer churn
- Weather predictions
- Stock market forecasts
- Demand forecasting

**Characteristics**:
- Looks to the future
- Uses historical data patterns
- Employs advanced statistical techniques
- Regression analysis, time series analysis
- Machine learning models

##### 4. Prescriptive Analytics ðŸ’¡

The highest level of data analytics is **prescriptive analytics**. In addition to making future predictions, it makes recommendations for different actions. It can give advice and provide solutions to problems.

**Key Questions**: "What should we do to achieve our goals?"

**Examples**:
- Optimizing delivery routes to save time and fuel
- Recommending marketing strategies
- Resource allocation optimization
- Risk management recommendations
- Pricing optimization

**Characteristics**:
- Highest complexity level
- Provides recommendations
- Uses complex event processing
- Decision trees, simulation, optimization methods
- Actionable insights

#### ðŸŽ¯ Analytics Hierarchy Diagram

<div class="mermaid">
graph TD
    A[Prescriptive Analytics<br/>ðŸ’¡<br/>What should we do?<br/>Highest Complexity] --> B[Predictive Analytics<br/>ðŸ”®<br/>What will happen?<br/>Future Forecasting]
    B --> C[Diagnostic Analytics<br/>ðŸ”<br/>Why did it happen?<br/>Root Cause Analysis]
    C --> D[Descriptive Analytics<br/>ðŸ“ˆ<br/>What happened?<br/>Most Basic]
    
    style A fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style B fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
</div>

#### ðŸ”„ Analytics Progression Flow

<div class="mermaid">
flowchart LR
    Past[Past Events<br/>Historical Data] --> Desc[Descriptive<br/>What Happened?<br/>ðŸ“ˆ]
    Desc --> Diag[Diagnostic<br/>Why Did It Happen?<br/>ðŸ”]
    Diag --> Pred[Predictive<br/>What Will Happen?<br/>ðŸ”®]
    Pred --> Presc[Prescriptive<br/>What Should We Do?<br/>ðŸ’¡]
    Presc --> Future[Future Actions<br/>Optimized Decisions]
    
    style Past fill:#E0E0E0,stroke:#757575,stroke-width:2px
    style Desc fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style Diag fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Pred fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style Presc fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Future fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ“‹ Analytics Levels Comparison

| Level | Question | Focus | Complexity | Techniques | Example |
|-------|----------|-------|------------|------------|---------|
| **Descriptive** | What happened? | Past events | Low | Aggregation, summarization | Monthly sales report |
| **Diagnostic** | Why did it happen? | Root causes | Medium | Statistical analysis, correlation | Investigating sales decline |
| **Predictive** | What will happen? | Future outcomes | High | Regression, time series, ML | Sales forecast for next quarter |
| **Prescriptive** | What should we do? | Recommendations | Very High | Optimization, simulation, decision trees | Optimize delivery routes |

#### ðŸŽ¨ Analytics Value Pyramid

<div class="mermaid">
graph TB
    subgraph "Value & Complexity"
        P1[Prescriptive Analytics<br/>Highest Value<br/>Most Complex<br/>ðŸ’¡]
        P2[Predictive Analytics<br/>High Value<br/>Complex<br/>ðŸ”®]
        P3[Diagnostic Analytics<br/>Medium Value<br/>Moderate Complexity<br/>ðŸ”]
        P4[Descriptive Analytics<br/>Foundation Value<br/>Basic Complexity<br/>ðŸ“ˆ]
    end
    
    P4 --> P3
    P3 --> P2
    P2 --> P1
    
    style P1 fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style P2 fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style P3 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style P4 fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
</div>

#### ðŸ’¡ Key Insights

<div class="important-info">
**Foundation to Advanced**: The four levels form a hierarchy where each level builds upon the previous one. Descriptive analytics provides the foundation, diagnostic explains the past, predictive looks to the future, and prescriptive provides actionable recommendations.

**Increasing Complexity**: As you move from descriptive to prescriptive analytics, both the complexity and the value of insights increase. However, each level serves an important purpose in data-driven decision-making.

**Complementary Use**: Organizations typically use multiple levels simultaneously. Descriptive analytics provides context, diagnostic explains issues, predictive forecasts trends, and prescriptive optimizes actions.
</div>

#### ðŸ” Exploratory Data Analysis (EDA)

EDA is a data analysis method that focuses on visual, intuitive techniques. This method frequently comes before formal statistical modelling and is used to look for patterns and anomalies, test theories, and validate presumptions about the statistical model of choice. The first two levels of data analytics, **descriptive** and **diagnostic analytics**, are where EDA primarily fits in.

##### ðŸ“Š EDA Overview

**Purpose**: To get a sense of what the data can tell us by exploring and understanding the data before formal modeling.

**Key Characteristics**:
- Visual and intuitive techniques
- Performed before formal statistical modeling
- Identifies patterns and anomalies
- Tests theories and validates assumptions
- Prepares data for advanced analytics

##### ðŸŽ¯ EDA Techniques

**1. Early Stage EDA - Descriptive Analytics** ðŸ“ˆ

Analysts summarise the key characteristics of the data during the early stages of EDA. This includes:

**Measures of Central Tendency**:
- **Mean**: Average value
- **Median**: Middle value
- **Mode**: Most frequent value

**Measures of Dispersion**:
- **Range**: Difference between max and min
- **Variance**: Measure of spread
- **Standard Deviation**: Square root of variance

**Data Distribution Visualisations**:
- **Histograms**: Show frequency distribution
- **Box Plots**: Display quartiles and outliers
- **Bar Charts**: Compare categories
- **Summary Statistics**: Numerical summaries

**2. Advanced EDA - Diagnostic Analytics** ðŸ”

As the EDA process progresses, analysts delve deeper into relationships and causes:

**Relationship Analysis**:
- **Scatter Plots**: Understand relationships between variables
- **Correlation Analysis**: Measure strength of relationships
- **Heatmaps**: Visualize correlation matrices

**Pattern Discovery**:
- **Grouping Data**: Discover patterns through segmentation
- **Anomaly Detection**: Identify outliers and unusual patterns
- **Trend Analysis**: Identify trends over time

**Root Cause Investigation**:
- **Comparative Analysis**: Compare different groups
- **Drill-down Analysis**: Investigate specific anomalies
- **Hypothesis Testing**: Test assumptions about data

##### ðŸ”„ EDA Workflow

<div class="mermaid">
flowchart TD
    Start([Start EDA]) --> Load[Load Data]
    Load --> Initial[Initial Exploration]
    
    Initial --> Desc[Descriptive Analytics<br/>ðŸ“ˆ<br/>Summarize Characteristics]
    Desc --> Central[Central Tendency<br/>Mean, Median, Mode]
    Desc --> Dispersion[Dispersion<br/>Range, Variance, Std Dev]
    Desc --> Visual[Distribution Visualizations<br/>Histograms, Box Plots]
    
    Visual --> Diag[Diagnostic Analytics<br/>ðŸ”<br/>Investigate Relationships]
    Diag --> Scatter[Scatter Plots<br/>Relationships]
    Diag --> Group[Group Data<br/>Pattern Discovery]
    Diag --> Anomaly[Anomaly Detection<br/>Outliers]
    
    Anomaly --> Validate[Validate Assumptions]
    Validate --> Prepare[Prepare for Advanced Analytics]
    
    Prepare --> Predictive[Predictive Analytics<br/>ðŸ”®<br/>Feature Engineering]
    Prepare --> Prescriptive[Prescriptive Analytics<br/>ðŸ’¡<br/>Decision Scenarios]
    
    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style Load fill:#95E1D3,stroke:#4ECDC4,stroke-width:2px
    style Initial fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Desc fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style Central fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style Dispersion fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style Visual fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style Diag fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Scatter fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Group fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Anomaly fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Validate fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style Prepare fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Predictive fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style Prescriptive fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

##### ðŸŽ¨ EDA in Analytics Hierarchy

<div class="mermaid">
graph TB
    subgraph "EDA Primary Focus"
        Desc[Descriptive Analytics<br/>ðŸ“ˆ<br/>Summarize Data]
        Diag[Diagnostic Analytics<br/>ðŸ”<br/>Investigate Causes]
    end
    
    subgraph "EDA Prepares For"
        Pred[Predictive Analytics<br/>ðŸ”®<br/>Feature Engineering]
        Presc[Prescriptive Analytics<br/>ðŸ’¡<br/>Decision Scenarios]
    end
    
    Desc --> Diag
    Desc --> Pred
    Diag --> Pred
    Desc --> Presc
    Diag --> Presc
    
    style Desc fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style Diag fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Pred fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style Presc fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

##### ðŸ“‹ EDA Techniques Summary

| Stage | Analytics Level | Techniques | Purpose |
|-------|----------------|------------|---------|
| **Early EDA** | Descriptive | Mean, Median, Mode, Range, Variance, Histograms, Box Plots | Summarize key characteristics |
| **Advanced EDA** | Diagnostic | Scatter Plots, Correlation, Grouping, Anomaly Detection | Investigate relationships and causes |
| **Preparation** | Predictive/Prescriptive | Feature Engineering, Pattern Recognition | Prepare for advanced analytics |

##### ðŸ”¬ EDA Process Steps

<div class="mermaid">
flowchart LR
    A[1. Load Data] --> B[2. Initial Exploration]
    B --> C[3. Descriptive Statistics]
    C --> D[4. Visualizations]
    D --> E[5. Identify Patterns]
    E --> F[6. Detect Anomalies]
    F --> G[7. Investigate Relationships]
    G --> H[8. Validate Assumptions]
    H --> I[9. Document Findings]
    I --> J[10. Prepare for Modeling]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#95E1D3,stroke:#4ECDC4,stroke-width:2px
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style G fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style H fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style I fill:#FFA07A,stroke:#FF6347,stroke-width:2px
    style J fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

##### ðŸ’¡ Key Insights

<div class="key-concept">
**Primary Focus**: EDA primarily fits into descriptive and diagnostic analytics levels. It helps summarize data characteristics and investigate relationships and causes.

**Foundation for Advanced Analytics**: While EDA's main objective aligns with descriptive and diagnostic analytics, it also prepares for predictive and prescriptive analytics. Understanding data patterns and anomalies through EDA greatly influences feature engineering for predictive analytics and decision-making scenarios for prescriptive analytics.

**Visual and Intuitive**: EDA emphasizes visual techniques that make data exploration intuitive and accessible, helping analysts understand data before formal modeling.

**Iterative Process**: EDA is an iterative process where initial exploration leads to deeper investigation, which then informs further exploration and analysis.
</div>

#### ðŸ“š What Did I Learn in This Lesson?

This lesson provided comprehensive insights into the field of data analysis. Here's a summary of the key learnings:

##### ðŸŽ¯ Key Learning Points

**1. Various Aspects of Data Analysis** ðŸŒ

Data analysis is a multifaceted discipline that involves:
- Transforming raw data into actionable insights
- Using specialized tools and software
- Connecting multiple fields: programming, ML, statistics, visualization, BI, and big data
- Applying different types of analysis: text, statistical, diagnostic, predictive, and data mining

**2. Roles and Responsibilities** ðŸ‘¥

- There are **eight key roles** within the field of data analysis:
  - Data Analyst, Data Scientist, Data Engineer
  - Business Intelligence Analyst, Data Architect
  - Statistician, Machine Learning Engineer
  - Data Visualization Specialist
- These roles might be named differently in different organisations
- A single individual may fulfil more than one of these roles, especially in smaller organizations
- Each role has specific responsibilities but collaboration is essential

**3. Hard and Soft Skills** ðŸŽ“

A data analyst requires various skills:

**Hard Skills** (Technical):
- Mathematical and statistical skills
- Programming skills (Python, R, SQL)
- Database management/SQL
- Data visualization skills
- Data cleaning/wrangling knowledge

**Soft Skills** (Interpersonal):
- Analytical skills
- Problem-solving skills
- Communication skills
- Business/industry understanding

Both skill types are essential and complement each other.

**4. Daily Tasks and Activities** ðŸ“…

A data analyst may be required to perform various tasks on any given day:
- Email management and planning
- Team meetings and collaboration
- Data retrieval using SQL
- Data validation and cleaning
- Data analysis using Python/Pandas
- Creating visualizations with Tableau
- Report writing and delivery
- Cross-team collaboration
- Project preparation and planning

**5. Four Primary Levels of Data Analysis** ðŸ“Š

The four primary levels form a hierarchy:
1. **Descriptive Analytics** - "What happened?" (Foundation)
2. **Diagnostic Analytics** - "Why did it happen?" (Root cause)
3. **Predictive Analytics** - "What will happen?" (Forecasting)
4. **Prescriptive Analytics** - "What should we do?" (Recommendations)

Complexity and value increase from descriptive to prescriptive analytics.

**6. Exploratory Data Analysis (EDA)** ðŸ”

Exploratory data analysis allows a data analyst to:
- Test hypotheses and check assumptions
- Discover patterns and anomalies
- Summarize data characteristics (descriptive)
- Investigate relationships and causes (diagnostic)
- Prepare data for advanced analytics (predictive/prescriptive)

EDA primarily fits into descriptive and diagnostic analytics but serves as foundation for higher levels.

#### ðŸŽ¨ Lesson Overview Diagram

<div class="mermaid">
mindmap
  root((Data Analysis<br/>Introduction))
    Aspects
      Multiple Fields
      Tools & Software
      Analysis Types
      Interdisciplinary
    Roles
      Data Analyst
      Data Scientist
      Data Engineer
      BI Analyst
      Data Architect
      Statistician
      ML Engineer
      Visualization Specialist
    Skills
      Hard Skills
        Math & Statistics
        Programming
        Database/SQL
        Visualization
        Data Cleaning
      Soft Skills
        Analytical
        Problem-Solving
        Communication
        Business Understanding
    Daily Tasks
      Email & Planning
      Meetings
      Data Retrieval
      Analysis
      Visualization
      Reporting
      Collaboration
    Analytics Levels
      Descriptive
      Diagnostic
      Predictive
      Prescriptive
    EDA
      Visual Techniques
      Pattern Discovery
      Hypothesis Testing
      Assumption Validation
</div>

#### ðŸ“‹ Lesson Summary Checklist

<div class="mermaid">
flowchart TD
    Start([Lesson Complete]) --> L1[âœ“ Understand Data Analysis Aspects]
    L1 --> L2[âœ“ Know Roles & Responsibilities]
    L2 --> L3[âœ“ Identify Required Skills]
    L3 --> L4[âœ“ Understand Daily Tasks]
    L4 --> L5[âœ“ Learn Analytics Levels]
    L5 --> L6[âœ“ Master EDA Techniques]
    L6 --> Ready([Ready for Next Lesson])
    
    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style L1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style L2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style L3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style L4 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style L5 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style L6 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Ready fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ’¡ Reflection Questions

<div class="important-info">
**Test Your Understanding**:
1. Can you explain the difference between descriptive and prescriptive analytics?
2. What are the key hard and soft skills needed for a data analyst?
3. How does EDA relate to the four levels of data analysis?
4. Why might roles overlap in smaller organizations?
5. How do the four analytics levels build upon each other?
</div>

<svg width="680" height="82" viewBox="0 0 680 82" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Lesson task banner">
    <rect x="1" y="1" width="678" height="80" rx="10" fill="#EAFBF1" stroke="#39A96B" stroke-width="2"/>
    <text x="20" y="34" fill="#1D6B44" font-size="18" font-weight="700">Lesson Task</text>
    <text x="20" y="56" fill="#1D6B44" font-size="13">Self-reflection activity: define goals, skills, and your data analysis path.</text>
</svg>

#### The Task

In this lesson, you learnt about the fundamental roles in the field of data analysis, the various hard and soft skills required by data analysts and the different types of analysis which may be performed. For today's lesson task, you need to consider your goals in data analysis.

##### Self-Reflection Exercise

This is a bit of self-reflection to help you with your first step on your journey to becoming a data analyst. Take some time to think about and answer the following questions:

**1. Your Motivation** ðŸ’­
- Why are you pursuing this type of career?
- What interests you about data analysis?
- What problems do you want to solve with data?

**2. Your Current Skill Set** ðŸ“Š
- **Hard Skills**: Where do you currently see your technical skills?
  - Mathematical and statistical skills
  - Programming skills (Python, R, SQL)
  - Database management
  - Data visualization
  - Data cleaning/wrangling
- **Soft Skills**: How would you rate your interpersonal skills?
  - Analytical thinking
  - Problem-solving
  - Communication
  - Business understanding

**3. Your Role Aspiration** ðŸ‘¥
- What role do you see yourself fulfilling in data analysis?
  - Data Analyst
  - Data Scientist
  - Data Engineer
  - Business Intelligence Analyst
  - Data Architect
  - Statistician
  - Machine Learning Engineer
  - Data Visualization Specialist
- Why does this role appeal to you?

**4. Your Analytics Interest** ðŸ“ˆ
- What level of analysis sounds the most interesting to you?
  - Descriptive Analytics (What happened?)
  - Diagnostic Analytics (Why did it happen?)
  - Predictive Analytics (What will happen?)
  - Prescriptive Analytics (What should we do?)
- Why are you drawn to this level?

##### Your Data Analysis Journey

<div class="mermaid">
flowchart TD
    Start([Start Your Journey]) --> Reflect[Self-Reflection<br/>Assess Skills & Goals]
    Reflect --> Identify[Identify Your Path<br/>Role & Analytics Level]
    Identify --> Learn[Learn & Practice<br/>Build Skills]
    Learn --> Apply[Apply Knowledge<br/>Real Projects]
    Apply --> Grow[Grow & Specialize<br/>Advanced Skills]
    Grow --> Achieve[Achieve Your Goals<br/>Career Success]
    
    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style Reflect fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Identify fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Learn fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style Apply fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Grow fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style Achieve fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

##### Skills Assessment Template

<div class="key-concept">
**Rate Your Current Skills (1-5 scale)**:

**Hard Skills:**
- Mathematical/Statistical Skills: â­â­â­â­â­
- Programming (Python/R/SQL): â­â­â­â­â­
- Database Management: â­â­â­â­â­
- Data Visualization: â­â­â­â­â­
- Data Cleaning/Wrangling: â­â­â­â­â­

**Soft Skills:**
- Analytical Thinking: â­â­â­â­â­
- Problem-Solving: â­â­â­â­â­
- Communication: â­â­â­â­â­
- Business Understanding: â­â­â­â­â­

**Areas for Growth:**
- [ ] Identify 2-3 skills you want to develop
- [ ] Set specific learning goals
- [ ] Plan your skill development path
</div>

##### ðŸŽ¨ Role Interest Assessment

<div class="mermaid">
pie title Which Role Interests You Most?
    "Data Analyst" : 25
    "Data Scientist" : 20
    "Data Engineer" : 15
    "BI Analyst" : 10
    "Data Architect" : 8
    "Statistician" : 7
    "ML Engineer" : 10
    "Visualization Specialist" : 5
</div>

##### ðŸ“Š Analytics Level Interest

<div class="mermaid">
graph LR
    A[Your Interest] --> B{Which Level?}
    B -->|Foundation| C[Descriptive<br/>ðŸ“ˆ<br/>Understanding Past]
    B -->|Investigation| D[Diagnostic<br/>ðŸ”<br/>Finding Causes]
    B -->|Forecasting| E[Predictive<br/>ðŸ”®<br/>Predicting Future]
    B -->|Optimization| F[Prescriptive<br/>ðŸ’¡<br/>Recommending Actions]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style C fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style F fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

##### ðŸ’¬ Share Your Reflection

<div class="important-info">
**Remember**: You aren't on this journey alone! 

Feel free to share your reflection in your class group's Teams channel to:
- Hear what your fellow students have to say
- Learn from others' perspectives
- Build connections with classmates
- Get support and encouragement
- Exchange ideas and insights

**What to Share:**
- Your motivation for pursuing data analysis
- Your current skill assessment
- Your role and analytics level interests
- Your learning goals
- Any questions or concerns you have
</div>

##### âœï¸ Reflection Template

Use this template to structure your reflection:

```
**My Data Analysis Journey Reflection**

**1. My Motivation:**
[Why are you pursuing this career?]

**2. My Current Skills:**
- Hard Skills: [Rate and describe]
- Soft Skills: [Rate and describe]

**3. My Role Interest:**
[Which role appeals to you and why?]

**4. My Analytics Interest:**
[Which level of analysis interests you most?]

**5. My Learning Goals:**
[What skills do you want to develop?]

**6. My Questions:**
[What would you like to learn more about?]
```

#### ðŸ“š Exam Preparation: What's Important for the Exam

Based on this lesson, here are the **critical concepts and information** you need to know for your exam:

##### ðŸŽ¯ Core Definitions (Must Know)

**Data Analysis**: The discipline focused on drawing insightful conclusions from data. It involves methods and tools for gathering unprocessed data and turning it into information that can help with decision-making.

**Exploratory Data Analysis (EDA)**: A data analysis method that focuses on visual, intuitive techniques used before formal statistical modeling to look for patterns and anomalies, test theories, and validate assumptions.

##### ðŸ“Š The Four Levels of Analytics (Critical Distinction)

You **MUST** be able to distinguish between these four levels:

| Level | Question | Focus | Complexity | Key Characteristics | Examples |
|-------|----------|-------|------------|-------------------|----------|
| **Descriptive** | "What happened?" | Past events | Low | Summarizes historical data, identifies patterns | Monthly sales report, website traffic statistics, social media metrics, revenue summaries |
| **Diagnostic** | "Why did it happen?" | Root causes | Medium | Investigates causes, uses statistical techniques | Investigating sales decline, root cause analysis, correlation analysis, identifying factors affecting performance |
| **Predictive** | "What will happen?" | Future outcomes | High | Forecasts based on historical data, uses ML/regression | Sales forecast for next quarter, customer churn prediction, weather forecasting, demand forecasting |
| **Prescriptive** | "What should we do?" | Recommendations | Very High | Provides actionable recommendations, uses optimization | Optimize delivery routes, recommend marketing strategies, resource allocation optimization, pricing optimization |

**Exam Tip**: Be able to identify which level is being described in a scenario or example.

##### ðŸŽ¨ Visual Examples of Each Analytics Level

**1. Descriptive Analytics - "What happened?"**

<div class="mermaid">
graph LR
    A[Historical Data<br/>Q1-Q4 Sales] --> B[Calculate Metrics]
    B --> C[Total Sales: $2.5M<br/>Avg Monthly: $625K<br/>Best Month: December]
    C --> D[Visualization<br/>Bar Chart]
    D --> E[Report: What Happened<br/>ðŸ“Š]
    
    style A fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style B fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style C fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style D fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style E fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

**Example**: A retail store analyzes last year's sales data and creates a report showing:
- Total annual revenue: $2.5 million
- Best performing month: December ($350K)
- Worst performing month: February ($180K)
- Average monthly sales: $208K

**2. Diagnostic Analytics - "Why did it happen?"**

<div class="mermaid">
flowchart TD
    A[Problem: Sales Dropped 30%<br/>in Q3] --> B[Collect Data]
    B --> C[Compare Variables]
    C --> D{Investigate Causes}
    D --> E[Marketing Budget<br/>Reduced 40%]
    D --> F[Competitor Launched<br/>New Product]
    D --> G[Seasonal Factors<br/>Summer Slump]
    E --> H[Root Cause Found:<br/>Marketing Reduction]
    F --> H
    G --> H
    H --> I[Report: Why It Happened<br/>ðŸ”]
    
    style A fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style G fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style I fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

**Example**: After noticing a 30% sales drop in Q3, the analyst investigates and finds:
- Marketing budget was reduced by 40% in the same period
- Strong correlation (r=0.85) between marketing spend and sales
- Competitor launched a similar product in July
- **Conclusion**: The sales drop was primarily caused by reduced marketing investment

**3. Predictive Analytics - "What will happen?"**

<div class="mermaid">
graph TD
    A[Historical Data<br/>2019-2024 Sales] --> B[Build ML Model]
    B --> C[Train on Past Patterns]
    C --> D[Time Series Analysis]
    D --> E[Regression Model]
    E --> F[Forecast Next Quarter]
    F --> G[Prediction: Q1 2025<br/>Sales: $650K Â± $50K<br/>Confidence: 85%]
    G --> H[Report: What Will Happen<br/>ðŸ”®]
    
    style A fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style B fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style C fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style D fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style E fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style F fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

**Example**: Using 5 years of sales data, the analyst builds a predictive model that forecasts:
- **Q1 2025 Sales**: $650,000 (with 85% confidence interval of $600K-$700K)
- **Trend**: Upward trajectory expected based on seasonal patterns
- **Risk Factors**: Economic indicators suggest potential 10% variance

**4. Prescriptive Analytics - "What should we do?"**

<div class="mermaid">
flowchart TD
    A[Goal: Maximize Profit<br/>Minimize Costs] --> B[Current Situation]
    B --> C[Multiple Scenarios]
    C --> D[Scenario 1:<br/>Increase Price 10%<br/>Profit: +$50K<br/>Risk: -15% Sales]
    C --> E[Scenario 2:<br/>Optimize Routes<br/>Cost: -$30K<br/>Time: -20%]
    C --> F[Scenario 3:<br/>Bundle Products<br/>Profit: +$40K<br/>Risk: Low]
    D --> G[Optimization Algorithm]
    E --> G
    F --> G
    G --> H[Recommendation:<br/>Implement Scenario 2 + 3<br/>Expected ROI: 25%]
    H --> I[Action Plan: What Should We Do<br/>ðŸ’¡]
    
    style A fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style D fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style F fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style G fill:#9B59B6,stroke:#6C3483,stroke-width:3px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style I fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

**Example**: To maximize profit, the system analyzes multiple options:
- **Option 1**: Increase prices by 10% â†’ +$50K profit but -15% sales risk
- **Option 2**: Optimize delivery routes â†’ -$30K costs, -20% delivery time
- **Option 3**: Bundle products â†’ +$40K profit, low risk
- **Recommendation**: Implement Option 2 + Option 3 together
- **Expected Result**: 25% ROI increase with manageable risk

##### ðŸ“ˆ Real-World Scenario Comparison

<div class="mermaid">
graph TB
    subgraph "Business Problem: Declining Customer Retention"
        Problem[Customer Churn Rate<br/>Increased to 25%]
    end
    
    subgraph "Descriptive Analytics ðŸ“ˆ"
        Desc[What Happened?<br/>Report shows:<br/>- Churn rate: 25%<br/>- Most churn in Q3<br/>- 60% are premium users]
    end
    
    subgraph "Diagnostic Analytics ðŸ”"
        Diag[Why Did It Happen?<br/>Analysis reveals:<br/>- Price increase caused churn<br/>- Support response time â†‘<br/>- Competitor offers better value]
    end
    
    subgraph "Predictive Analytics ðŸ”®"
        Pred[What Will Happen?<br/>Model predicts:<br/>- Churn will reach 30%<br/>- Revenue loss: $500K<br/>- If no action taken]
    end
    
    subgraph "Prescriptive Analytics ðŸ’¡"
        Presc[What Should We Do?<br/>Recommendations:<br/>1. Reduce price for loyal customers<br/>2. Improve support response time<br/>3. Add value-added features<br/>Expected: Churn â†“ to 15%]
    end
    
    Problem --> Desc
    Desc --> Diag
    Diag --> Pred
    Pred --> Presc
    
    style Problem fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
    style Desc fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style Diag fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Pred fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style Presc fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

##### ðŸ‘¥ Key Roles in Data Analysis (Know the Differences)

You should be able to **identify and distinguish** between these roles:

1. **Data Analyst**: Gathers, organizes, analyzes data statistically; interprets data and delivers reports
2. **Data Scientist**: Uses algorithms and predictive models; creates AI systems with machine learning
3. **Data Engineer**: Sets up big data infrastructure; designs, builds, integrates data from sources
4. **Business Intelligence Analyst**: Analyzes data to identify business/market trends; creates reports and dashboards
5. **Data Architect**: Designs and manages high-level structures in data ecosystem (design focus)
6. **Statistician**: Uses statistical methods to gather/analyze data; plans surveys and experiments
7. **Machine Learning Engineer**: Creates and maintains ML platforms; develops autonomous systems
8. **Data Visualization Specialist**: Turns complex findings into visual presentations; creates dashboards and charts

**Exam Tip**: Questions may ask you to match role descriptions to role names, or identify which role performs specific tasks.

##### ðŸ“‹ Role Examples and Scenarios

**1. Data Analyst** ðŸ“Š

**Example Scenario**: A retail company wants to understand their Q4 sales performance.

**Tasks**:
- Gathers sales data from the database using SQL queries
- Calculates total revenue: $2.5M (up 15% from Q3)
- Identifies top-selling products: Electronics (40%), Clothing (35%), Home (25%)
- Creates a statistical summary: average order value, customer demographics
- Delivers a report: "Q4 Sales Analysis Report" with findings and recommendations

**Key Output**: Statistical reports, data summaries, trend analysis

**2. Data Scientist** ðŸ”¬

**Example Scenario**: An e-commerce company wants to predict which customers are likely to churn.

**Tasks**:
- Builds a machine learning model using historical customer data
- Trains a predictive algorithm (Random Forest) on 100,000 customer records
- Creates an AI system that scores each customer's churn probability (0-100%)
- Identifies key factors: low purchase frequency, no engagement with emails, negative reviews
- Deploys model that predicts: "Customer #12345 has 85% churn risk"

**Key Output**: Predictive models, AI systems, ML algorithms

**3. Data Engineer** âš™ï¸

**Example Scenario**: A company needs to integrate data from 5 different sources into a data warehouse.

**Tasks**:
- Sets up Hadoop cluster for big data processing
- Designs ETL pipeline to extract data from: CRM, ERP, website, mobile app, social media
- Builds data integration system that processes 10TB of data daily
- Ensures data is accessible and performance-optimized
- Creates automated data pipeline that runs every hour

**Key Output**: Data pipelines, infrastructure, data warehouses, ETL systems

**4. Business Intelligence Analyst** ðŸ’¼

**Example Scenario**: A company's executives need to understand market trends and competitive position.

**Tasks**:
- Analyzes market data to identify trends: "E-commerce growing 25% YoY"
- Gathers data from industry reports, competitor analysis, internal metrics
- Creates executive dashboard showing: market share (15%), growth rate, competitor comparison
- Identifies opportunity: "Untapped market segment in 25-35 age group"
- Presents findings to C-suite: "Market Position Report Q1 2025"

**Key Output**: Business reports, executive dashboards, market analysis

**5. Data Architect** ðŸ—ï¸

**Example Scenario**: A company is planning a new data infrastructure to support analytics.

**Tasks**:
- Designs high-level data architecture: "3-tier architecture with data lake, warehouse, and marts"
- Creates framework for data governance and security
- Plans data flow: "Raw data â†’ Staging â†’ Cleansed â†’ Analytics â†’ Reporting"
- Designs schema for 50+ data sources
- Creates blueprint for data engineers to implement

**Key Output**: Architecture blueprints, system designs, data frameworks

**6. Statistician** ðŸ“ˆ

**Example Scenario**: A pharmaceutical company needs to test if a new drug is effective.

**Tasks**:
- Designs randomized controlled trial with 1,000 participants
- Plans survey methodology: control group vs treatment group
- Uses statistical methods: hypothesis testing, p-values, confidence intervals
- Analyzes results: "Drug shows 30% improvement (p < 0.01, 95% CI)"
- Makes statistical inference: "Drug is significantly more effective than placebo"

**Key Output**: Statistical analysis, experimental designs, survey results

**7. Machine Learning Engineer** ðŸ¤–

**Example Scenario**: A company wants to deploy a recommendation system for their website.

**Tasks**:
- Creates ML platform infrastructure using TensorFlow and Kubernetes
- Develops recommendation algorithm that learns from user behavior
- Builds system that processes 1M requests/day in real-time
- Maintains and monitors model performance (accuracy, latency)
- Implements A/B testing framework for model improvements

**Key Output**: ML platforms, deployed models, autonomous systems

**8. Data Visualization Specialist** ðŸŽ¨

**Example Scenario**: Complex data analysis results need to be presented to non-technical stakeholders.

**Tasks**:
- Takes statistical findings from data scientist
- Creates interactive Tableau dashboard with 10+ visualizations
- Designs charts: sales trends (line), product distribution (pie), regional performance (map)
- Makes complex data accessible: "Revenue increased 25% - shown as clear bar chart"
- Produces presentation-ready visualizations for board meeting

**Key Output**: Dashboards, charts, visual presentations, infographics

##### ðŸŽ¨ Role Comparison: Same Problem, Different Approaches

<div class="mermaid">
graph TB
    subgraph "Business Problem: Improve Customer Retention"
        Problem[Customer Churn Rate: 30%<br/>Need to Reduce to 15%]
    end
    
    subgraph "Data Analyst ðŸ“Š"
        DA[Analyzes historical churn data<br/>Creates report: Who churned?<br/>When? What products?]
    end
    
    subgraph "Data Scientist ðŸ”¬"
        DS[Builds ML model to predict churn<br/>Identifies at-risk customers<br/>Churn probability scores]
    end
    
    subgraph "Data Engineer âš™ï¸"
        DE[Sets up data pipeline<br/>Integrates customer data sources<br/>Ensures real-time data access]
    end
    
    subgraph "BI Analyst ðŸ’¼"
        BI[Analyzes market trends<br/>Competitor retention strategies<br/>Business recommendations]
    end
    
    subgraph "Data Architect ðŸ—ï¸"
        DArch[Designs customer data architecture<br/>Plans data warehouse structure<br/>Creates data framework]
    end
    
    subgraph "Statistician ðŸ“ˆ"
        Stat[Designs A/B test for retention<br/>Statistical significance testing<br/>Confidence intervals]
    end
    
    subgraph "ML Engineer ðŸ¤–"
        MLE[Deploys churn prediction model<br/>Creates ML platform<br/>Monitors model performance]
    end
    
    subgraph "Visualization Specialist ðŸŽ¨"
        DV[Creates retention dashboard<br/>Visualizes churn patterns<br/>Executive presentation]
    end
    
    Problem --> DA
    Problem --> DS
    Problem --> DE
    Problem --> BI
    Problem --> DArch
    Problem --> Stat
    Problem --> MLE
    Problem --> DV
    
    style Problem fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
    style DA fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style DS fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style DE fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style BI fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style DArch fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style Stat fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style MLE fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style DV fill:#FFA07A,stroke:#FF6347,stroke-width:2px
</div>

##### ðŸ”„ Role Workflow: How Roles Collaborate

<div class="mermaid">
flowchart LR
    A[Data Architect<br/>ðŸ—ï¸<br/>Designs System] --> B[Data Engineer<br/>âš™ï¸<br/>Builds Infrastructure]
    B --> C[Data Sources<br/>Ready]
    C --> D[Data Analyst<br/>ðŸ“Š<br/>Analyzes Data]
    C --> E[Data Scientist<br/>ðŸ”¬<br/>Builds Models]
    C --> F[Statistician<br/>ðŸ“ˆ<br/>Statistical Tests]
    D --> G[BI Analyst<br/>ðŸ’¼<br/>Business Insights]
    E --> H[ML Engineer<br/>ðŸ¤–<br/>Deploys Models]
    F --> G
    G --> I[Visualization Specialist<br/>ðŸŽ¨<br/>Creates Dashboards]
    H --> I
    I --> J[Decision Makers<br/>ðŸ’¡]
    
    style A fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style B fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style C fill:#95E1D3,stroke:#4ECDC4,stroke-width:2px
    style D fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style G fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style I fill:#FFA07A,stroke:#FF6347,stroke-width:2px
    style J fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
</div>

##### ðŸ“Š Role Responsibilities Matrix

<div class="mermaid">
graph TB
    subgraph "Data Collection & Infrastructure"
        DE[Data Engineer<br/>âš™ï¸<br/>Builds pipelines<br/>Sets up infrastructure]
        DArch[Data Architect<br/>ðŸ—ï¸<br/>Designs architecture<br/>Creates frameworks]
    end
    
    subgraph "Data Analysis & Modeling"
        DA[Data Analyst<br/>ðŸ“Š<br/>Statistical analysis<br/>Reports]
        DS[Data Scientist<br/>ðŸ”¬<br/>ML models<br/>AI systems]
        Stat[Statistician<br/>ðŸ“ˆ<br/>Experiments<br/>Hypothesis testing]
    end
    
    subgraph "Business & Visualization"
        BI[BI Analyst<br/>ðŸ’¼<br/>Business insights<br/>Market trends]
        DV[Visualization Specialist<br/>ðŸŽ¨<br/>Dashboards<br/>Charts]
    end
    
    subgraph "ML Operations"
        MLE[ML Engineer<br/>ðŸ¤–<br/>Deploy models<br/>ML platforms]
    end
    
    DArch --> DE
    DE --> DA
    DE --> DS
    DE --> Stat
    DA --> BI
    DS --> MLE
    Stat --> BI
    BI --> DV
    MLE --> DV
    
    style DE fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style DArch fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style DA fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style DS fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style Stat fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style BI fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style DV fill:#FFA07A,stroke:#FF6347,stroke-width:2px
    style MLE fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

##### ðŸŽ“ Hard Skills vs Soft Skills (Critical Distinction)

**Hard Skills** (Technical, quantifiable):
- Mathematical and statistical skills
- Programming skills (Python, R, SQL)
- Database management/SQL
- Data visualization skills
- Data cleaning/wrangling knowledge

**Soft Skills** (Interpersonal, personality-based):
- Analytical skills
- Problem-solving skills
- Communication skills
- Business/industry understanding

**Exam Tip**: Be able to categorize skills as hard or soft, and explain why both are important.

##### ðŸŽ¨ Visual Comparison: Hard vs Soft Skills

<div class="mermaid">
graph TB
    subgraph "Hard Skills ðŸ’»<br/>Technical & Quantifiable"
        HS1[Mathematical &<br/>Statistical Skills<br/>ðŸ“Š<br/>Calculate mean, median<br/>Perform regression analysis]
        HS2[Programming Skills<br/>Python, R, SQL<br/>ðŸ’»<br/>Write code, automate tasks<br/>Process large datasets]
        HS3[Database Management<br/>SQL Queries<br/>ðŸ—„ï¸<br/>Extract data, optimize queries<br/>Manage data systems]
        HS4[Data Visualization<br/>Tableau, PowerBI<br/>ðŸ“ˆ<br/>Create charts, dashboards<br/>Design visualizations]
        HS5[Data Cleaning<br/>Wrangling<br/>ðŸ§¹<br/>Handle missing data<br/>Transform datasets]
    end
    
    subgraph "Soft Skills ðŸ¤<br/>Interpersonal & Personality-based"
        SS1[Analytical Skills<br/>Critical Thinking<br/>ðŸ”<br/>Examine patterns<br/>Draw conclusions]
        SS2[Problem-Solving<br/>Troubleshooting<br/>ðŸ§©<br/>Identify issues<br/>Find solutions]
        SS3[Communication<br/>Written & Verbal<br/>ðŸ’¬<br/>Explain findings<br/>Present to stakeholders]
        SS4[Business Understanding<br/>Industry Knowledge<br/>ðŸ¢<br/>Context for data<br/>Business objectives]
    end
    
    HS1 --> Success[Successful<br/>Data Analyst<br/>âœ…]
    HS2 --> Success
    HS3 --> Success
    HS4 --> Success
    HS5 --> Success
    SS1 --> Success
    SS2 --> Success
    SS3 --> Success
    SS4 --> Success
    
    style HS1 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style HS2 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style HS3 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style HS4 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style HS5 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style SS1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style SS2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style SS3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style SS4 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Success fill:#FFD700,stroke:#B8860B,stroke-width:3px
</div>

##### ðŸ“‹ Real-World Examples

**Hard Skills Examples:**

**1. Mathematical and Statistical Skills** ðŸ“Š

**Example Scenario**: Analyzing customer purchase patterns

**Hard Skill Application**:
- Calculates mean order value: $125.50
- Performs correlation analysis: Purchase frequency vs Customer lifetime value (r = 0.78)
- Uses regression to predict: Revenue = 50 + (2.5 Ã— Marketing Spend)
- Applies hypothesis testing: "Is the new pricing strategy effective?" (p < 0.05)

**2. Programming Skills (Python, R, SQL)** ðŸ’»

**Example Scenario**: Processing 1 million customer records

**Hard Skill Application**:
```python
# Python example
import pandas as pd
df = pd.read_csv('customers.csv')
df.groupby('region')['sales'].sum()
```

```sql
-- SQL example
SELECT region, SUM(sales) as total_sales
FROM customers
GROUP BY region
ORDER BY total_sales DESC;
```

- Writes SQL query to extract data from database
- Uses Python to automate data processing
- Creates script that processes 1M records in 5 minutes

**3. Database Management/SQL** ðŸ—„ï¸

**Example Scenario**: Setting up data warehouse for analytics

**Hard Skill Application**:
- Designs database schema with 20+ tables
- Writes optimized SQL queries (execution time: 2 seconds vs 30 seconds)
- Creates indexes to improve query performance
- Manages data integrity and relationships

**4. Data Visualization Skills** ðŸ“ˆ

**Example Scenario**: Creating executive dashboard

**Hard Skill Application**:
- Uses Tableau to create interactive dashboard
- Designs 10+ charts: bar charts, line graphs, heatmaps
- Implements filters and drill-down capabilities
- Exports to PowerPoint for presentation

**5. Data Cleaning/Wrangling** ðŸ§¹

**Example Scenario**: Preparing messy dataset for analysis

**Hard Skill Application**:
- Handles 15,000 missing values (30% of dataset)
- Removes 500 duplicate records
- Standardizes date formats across 3 different formats
- Transforms data from wide to long format
- Validates data quality: checks for outliers, inconsistencies

**Soft Skills Examples:**

**1. Analytical Skills** ðŸ”

**Example Scenario**: Interpreting complex data patterns

**Soft Skill Application**:
- Examines sales data and notices: "Sales spike every Friday"
- Asks critical question: "Why do customers buy more on Fridays?"
- Identifies pattern: "Email campaigns sent on Thursdays correlate with Friday sales"
- Draws conclusion: "Email timing affects purchase behavior"

**2. Problem-Solving Skills** ðŸ§©

**Example Scenario**: Data analysis reveals unexpected results

**Soft Skill Application**:
- **Problem**: Analysis shows negative correlation when positive was expected
- **Investigation**: Checks data quality, verifies calculations, reviews methodology
- **Solution**: Discovers data entry error in source system
- **Resolution**: Corrects data and re-runs analysis with accurate results

**3. Communication Skills** ðŸ’¬

**Example Scenario**: Presenting technical findings to non-technical executives

**Soft Skill Application**:
- **Technical finding**: "The model shows a 0.85 correlation coefficient"
- **Communication**: "Our analysis shows a strong relationship (85%) between marketing spend and sales"
- **Presentation**: Uses simple language, visual aids, and real-world examples
- **Result**: Executives understand and can make decisions based on findings

**4. Business/Industry Understanding** ðŸ¢

**Example Scenario**: Analyzing retail sales data

**Soft Skill Application**:
- **Data shows**: Sales drop in January
- **Business context**: "January is post-holiday season, expected sales decline"
- **Industry knowledge**: "Retail industry typically sees 20-30% drop in January"
- **Insight**: "Our 25% drop is within normal range, not a concern"
- **Action**: Recommends focusing on February recovery strategies

##### ðŸ”„ How Hard and Soft Skills Work Together

<div class="mermaid">
flowchart TD
    A[Business Problem:<br/>Sales Declining] --> B[Hard Skill: SQL Query<br/>Extract Sales Data]
    B --> C[Hard Skill: Python Analysis<br/>Calculate Trends]
    C --> D[Soft Skill: Analytical Thinking<br/>Identify Pattern]
    D --> E[Soft Skill: Problem-Solving<br/>Investigate Cause]
    E --> F[Hard Skill: Statistical Analysis<br/>Test Hypothesis]
    F --> G[Soft Skill: Business Understanding<br/>Interpret Results]
    G --> H[Soft Skill: Communication<br/>Present Findings]
    H --> I[Hard Skill: Visualization<br/>Create Dashboard]
    I --> J[Solution Implemented<br/>âœ…]
    
    style A fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style C fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style D fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style F fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style G fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style I fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style J fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

##### ðŸ“Š Skills in Action: Complete Scenario

<div class="mermaid">
graph TB
    subgraph "Scenario: Customer Churn Analysis Project"
        Start[Project Goal:<br/>Reduce Churn by 20%]
    end
    
    subgraph "Hard Skills Applied ðŸ’»"
        H1[SQL: Extract 100K customer records<br/>from database]
        H2[Python: Clean data, handle<br/>15K missing values]
        H3[Statistics: Calculate churn rate<br/>25%, correlation analysis]
        H4[ML: Build predictive model<br/>85% accuracy]
        H5[Tableau: Create churn dashboard<br/>with 8 visualizations]
    end
    
    subgraph "Soft Skills Applied ðŸ¤"
        S1[Analytical: Identify that premium<br/>users churn more]
        S2[Problem-Solving: Debug model<br/>performance issues]
        S3[Communication: Present findings<br/>to C-suite executives]
        S4[Business Understanding: Recommend<br/>retention strategies]
    end
    
    Start --> H1
    H1 --> H2
    H2 --> H3
    H3 --> S1
    S1 --> H4
    H4 --> S2
    S2 --> H5
    H5 --> S3
    S3 --> S4
    S4 --> Success[Result: Churn Reduced<br/>to 18% âœ…]
    
    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style H1 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style H2 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style H3 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style H4 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style H5 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style S1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style S2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style S3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style S4 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Success fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

##### ðŸ’¡ Why Both Skills Are Important

<div class="mermaid">
mindmap
  root((Data Analyst Success))
    Hard Skills
      Technical Foundation
        Can perform analysis
        Can use tools
        Can process data
      Quantifiable
        Certifications
        Code quality
        Performance metrics
    Soft Skills
      Effective Application
        Can explain results
        Can solve problems
        Can work in teams
      Interpersonal
        Communication
        Collaboration
        Business acumen
    Combined Power
      Technical Expertise
        Hard skills enable
      Effective Delivery
        Soft skills enable
      Complete Analyst
        Both essential
</div>

##### ðŸŽ¯ Skills Comparison Table with Examples

| Skill Type | Characteristic | How Measured | Example | Why Important |
|------------|---------------|--------------|---------|---------------|
| **Hard Skills** | Technical, quantifiable | Certifications, tests, code reviews | Can write SQL query that extracts data in 2 seconds | Provides technical foundation to perform analysis |
| **Soft Skills** | Interpersonal, personality-based | Performance reviews, feedback, observations | Can explain complex findings to non-technical audience | Enables effective application and collaboration |
| **Combined** | Complementary | Project success, stakeholder satisfaction | Uses SQL (hard) to extract data, then communicates findings (soft) to executives | Both essential for successful data analyst career |

##### ðŸ” EDA Techniques (Know the Categories)

**Early EDA (Descriptive Analytics)**:
- Measures of central tendency: Mean, Median, Mode
- Measures of dispersion: Range, Variance, Standard Deviation
- Visualizations: Histograms, Box Plots, Bar Charts

**Advanced EDA (Diagnostic Analytics)**:
- Scatter Plots (relationships)
- Correlation Analysis
- Grouping Data (pattern discovery)
- Anomaly Detection

**Exam Tip**: Questions may ask which EDA technique is appropriate for a given scenario, or which analytics level a technique belongs to.

##### ðŸ“Š EDA Techniques Visual Overview

<div class="mermaid">
graph TB
    subgraph "Early EDA - Descriptive Analytics ðŸ“ˆ"
        E1[Measures of Central Tendency<br/>Mean, Median, Mode]
        E2[Measures of Dispersion<br/>Range, Variance, Std Dev]
        E3[Visualizations<br/>Histograms, Box Plots, Bar Charts]
    end
    
    subgraph "Advanced EDA - Diagnostic Analytics ðŸ”"
        A1[Scatter Plots<br/>Relationships]
        A2[Correlation Analysis<br/>Strength of Relationships]
        A3[Grouping Data<br/>Pattern Discovery]
        A4[Anomaly Detection<br/>Outliers]
    end
    
    E1 --> Summary[Summarize Data<br/>What Happened?]
    E2 --> Summary
    E3 --> Summary
    Summary --> A1
    Summary --> A2
    Summary --> A3
    Summary --> A4
    A1 --> Investigate[Investigate Causes<br/>Why Did It Happen?]
    A2 --> Investigate
    A3 --> Investigate
    A4 --> Investigate
    
    style E1 fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style E2 fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style E3 fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style A1 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style A2 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style A3 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style A4 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Summary fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style Investigate fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

##### ðŸ“ˆ Early EDA Techniques - Examples

**1. Measures of Central Tendency**

**Example Scenario**: Analyzing customer purchase amounts

**Data**: [25, 30, 30, 35, 40, 45, 50, 50, 50, 100]

**Calculations**:
- **Mean (Average)**: (25+30+30+35+40+45+50+50+50+100) Ã· 10 = **$43.50**
  - *Use when*: Data is normally distributed, no extreme outliers
  - *Reveals*: Typical purchase amount

- **Median (Middle value)**: [25, 30, 30, 35, **40**, 45, 50, 50, 50, 100] = **$42.50**
  - *Use when*: Data has outliers (like the $100 purchase)
  - *Reveals*: Middle value, less affected by outliers

- **Mode (Most frequent)**: **$50** (appears 3 times)
  - *Use when*: Finding most common value
  - *Reveals*: Most popular purchase amount

**Visual Example**:
```
Purchase Amounts: $25, $30, $30, $35, $40, $45, $50, $50, $50, $100
                    â†“
Mean: $43.50 (affected by $100 outlier)
Median: $42.50 (middle value, robust to outliers)
Mode: $50 (most frequent)
```

**2. Measures of Dispersion**

**Example Scenario**: Comparing sales consistency across two stores

**Store A Sales**: [100, 105, 110, 115, 120] (in thousands)
**Store B Sales**: [80, 100, 110, 120, 140] (in thousands)

**Calculations**:
- **Range**: 
  - Store A: 120 - 100 = **$20K** (consistent)
  - Store B: 140 - 80 = **$60K** (variable)
  
- **Variance**:
  - Store A: **62.5** (low variance = consistent)
  - Store B: **400** (high variance = inconsistent)
  
- **Standard Deviation**:
  - Store A: **$7.91K** (predictable sales)
  - Store B: **$20K** (unpredictable sales)

**Insight**: Store A has more consistent sales (lower dispersion), while Store B has higher variability.

**3. Visualizations - Histograms**

**Example Scenario**: Understanding customer age distribution

**Data**: Customer ages from 18 to 65

**Histogram reveals**:
- **Peak**: Most customers are 25-35 years old
- **Distribution**: Bell-shaped (normal distribution)
- **Outliers**: Few customers over 60

**Use case**: "What age group should we target for marketing?"

**4. Visualizations - Box Plots**

**Example Scenario**: Comparing product prices across categories

**Box Plot shows**:
- **Median price**: Electronics ($500), Clothing ($50), Home ($200)
- **Quartiles**: 25th, 50th (median), 75th percentiles
- **Outliers**: Electronics has high-value outliers ($2000+ items)

**Use case**: "Which product category has the most price variation?"

**5. Visualizations - Bar Charts**

**Example Scenario**: Monthly sales comparison

**Bar Chart shows**:
- January: $100K
- February: $120K
- March: $150K
- April: $140K

**Use case**: "Which month had the highest sales?"

##### ðŸ” Advanced EDA Techniques - Examples

**1. Scatter Plots (Relationships)**

**Example Scenario**: Understanding relationship between marketing spend and sales

**Data Points**:
- Marketing $10K â†’ Sales $50K
- Marketing $20K â†’ Sales $80K
- Marketing $30K â†’ Sales $120K
- Marketing $40K â†’ Sales $150K

**Scatter Plot reveals**:
- **Positive relationship**: As marketing increases, sales increase
- **Linear pattern**: Points form a straight line
- **Strength**: Strong relationship (points close to line)

**Use case**: "Does increasing marketing budget lead to higher sales?"

**2. Correlation Analysis**

**Example Scenario**: Finding factors that affect customer satisfaction

**Correlation Matrix**:
| Factor | Satisfaction Score |
|--------|-------------------|
| Response Time | **0.85** (strong positive) |
| Price | **-0.60** (moderate negative) |
| Product Quality | **0.75** (strong positive) |
| Website Speed | **0.45** (weak positive) |

**Interpretation**:
- **Response Time (0.85)**: Strong positive correlation - faster response = higher satisfaction
- **Price (-0.60)**: Moderate negative - higher price = lower satisfaction
- **Product Quality (0.75)**: Strong positive - better quality = higher satisfaction

**Use case**: "What factors most strongly influence customer satisfaction?"

**3. Grouping Data (Pattern Discovery)**

**Example Scenario**: Discovering customer segments

**Grouping by Purchase Behavior**:
- **Group 1 (High Value)**: 20% of customers, 60% of revenue
  - Characteristics: Frequent purchases, high average order value
- **Group 2 (Regular)**: 50% of customers, 35% of revenue
  - Characteristics: Monthly purchases, moderate spending
- **Group 3 (Occasional)**: 30% of customers, 5% of revenue
  - Characteristics: Rare purchases, low spending

**Pattern discovered**: Small group (20%) generates most revenue (60%)

**Use case**: "How should we segment customers for targeted marketing?"

**4. Anomaly Detection**

**Example Scenario**: Identifying fraudulent transactions

**Normal Transaction Pattern**:
- Average: $50-200
- Frequency: 2-5 per month
- Location: Customer's home city

**Anomalies Detected**:
- **Transaction 1**: $5,000 (25x average) â†’ Flagged
- **Transaction 2**: 50 transactions in 1 day â†’ Flagged
- **Transaction 3**: Location: Different country â†’ Flagged

**Use case**: "Which transactions are unusual and need investigation?"

##### ðŸŽ¯ EDA Techniques Workflow Example

<div class="mermaid">
flowchart TD
    A[Dataset: 10,000 Customer Records] --> B[Early EDA - Descriptive]
    
    B --> C[Calculate Central Tendency<br/>Mean Age: 35<br/>Median Income: $50K<br/>Mode: City A]
    B --> D[Calculate Dispersion<br/>Age Range: 18-65<br/>Income Std Dev: $15K]
    B --> E[Create Visualizations<br/>Histogram: Age distribution<br/>Box Plot: Income by city<br/>Bar Chart: City counts]
    
    C --> F[Summary: What Happened?<br/>Average customer: 35 years,<br/>$50K income, lives in City A]
    D --> F
    E --> F
    
    F --> G[Advanced EDA - Diagnostic]
    
    G --> H[Scatter Plot<br/>Age vs Income<br/>Shows positive relationship]
    G --> I[Correlation Analysis<br/>Age & Income: r=0.65<br/>Strong positive correlation]
    G --> J[Grouping Data<br/>3 customer segments<br/>High/Medium/Low value]
    G --> K[Anomaly Detection<br/>5 outliers identified<br/>Unusual patterns found]
    
    H --> L[Insight: Why Patterns Exist?<br/>Older customers earn more<br/>Segments have different behaviors<br/>Outliers need investigation]
    I --> L
    J --> L
    K --> L
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style C fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style D fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style E fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style H fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style I fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style J fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style K fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style L fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

##### ðŸ“‹ When to Use Each EDA Technique

<div class="mermaid">
graph LR
    A[EDA Goal] --> B{What Question?}
    
    B -->|Summarize Data| C[Early EDA<br/>Descriptive]
    B -->|Find Relationships| D[Advanced EDA<br/>Diagnostic]
    
    C --> E[Central Tendency<br/>Mean/Median/Mode]
    C --> F[Dispersion<br/>Range/Variance/Std Dev]
    C --> G[Visualizations<br/>Histogram/Box/Bar]
    
    D --> H[Scatter Plot<br/>See relationships]
    D --> I[Correlation<br/>Measure strength]
    D --> J[Grouping<br/>Find patterns]
    D --> K[Anomaly Detection<br/>Find outliers]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style C fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style F fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style G fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style H fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style I fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style J fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style K fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
</div>

##### ðŸ’¡ Real-World EDA Scenario

**Scenario**: E-commerce company wants to understand customer behavior

**Step 1: Early EDA (Descriptive)**
- **Mean purchase**: $75
- **Median purchase**: $60 (outliers affect mean)
- **Range**: $5 - $500
- **Histogram**: Shows most purchases are $30-$100
- **Bar Chart**: Electronics (40%), Clothing (35%), Home (25%)

**Step 2: Advanced EDA (Diagnostic)**
- **Scatter Plot**: Age vs Purchase Amount â†’ Positive relationship
- **Correlation**: Age & Purchase (r=0.65), Income & Purchase (r=0.80)
- **Grouping**: 3 segments identified (High/Medium/Low spenders)
- **Anomaly Detection**: 50 transactions flagged (unusual patterns)

**Result**: 
- **What happened?** (Descriptive): Average customer spends $75, mostly on electronics
- **Why?** (Diagnostic): Older, higher-income customers spend more; 3 distinct customer segments exist

##### ðŸŽ¨ EDA Techniques Summary Table

| Technique | Analytics Level | When to Use | Example Question | Reveals |
|-----------|----------------|-------------|------------------|---------|
| **Mean/Median/Mode** | Descriptive | Summarize data | "What's the typical value?" | Central value |
| **Range/Variance** | Descriptive | Measure spread | "How consistent is the data?" | Data variability |
| **Histogram** | Descriptive | Distribution shape | "How is data distributed?" | Frequency patterns |
| **Box Plot** | Descriptive | Compare groups | "Which group varies more?" | Quartiles, outliers |
| **Bar Chart** | Descriptive | Compare categories | "Which category is largest?" | Category comparison |
| **Scatter Plot** | Diagnostic | See relationships | "Do two variables relate?" | Relationship pattern |
| **Correlation** | Diagnostic | Measure relationship strength | "How strong is the relationship?" | Correlation coefficient |
| **Grouping** | Diagnostic | Find patterns | "What groups exist in data?" | Segments, clusters |
| **Anomaly Detection** | Diagnostic | Find unusual data | "What data points are unusual?" | Outliers, exceptions |

##### ðŸ”— Key Relationships (Important Connections)

**EDA and Analytics Levels**:
- EDA primarily fits into **descriptive** and **diagnostic** analytics
- EDA results serve as inputs to **predictive** (feature engineering) and **prescriptive** (decision scenarios) analytics

**Analytics Hierarchy**:
- Each level builds upon the previous one
- Complexity and value increase from descriptive â†’ prescriptive
- All levels are important for decision-making

**Roles and Skills**:
- Different roles require different skill combinations
- Roles often overlap in smaller organizations
- Collaboration across roles is essential

##### ðŸ“‹ Exam Question Types to Expect

<div class="important-info">
**1. Definition Questions**: "What is [concept]?"
- Be able to define key terms clearly and concisely

**2. Comparison Questions**: "What is the difference between X and Y?"
- Descriptive vs Prescriptive analytics
- Hard skills vs Soft skills
- Data Analyst vs Data Scientist

**3. Classification Questions**: "Which analytics level does this describe?"
- Given a scenario, identify the analytics level
- Given a technique, identify which analytics level it belongs to

**4. Role Identification**: "Which role is responsible for [task]?"
- Match tasks to roles
- Understand role responsibilities

**5. EDA Questions**: "Which EDA technique would you use to [purpose]?"
- Match techniques to purposes
- Identify which analytics level EDA techniques belong to

**6. Application Questions**: "In this scenario, what would a data analyst do?"
- Apply concepts to real-world situations
- Understand the data analysis workflow
</div>

##### ðŸŽ¯ Critical Exam Topics Summary

<div class="mermaid">
mindmap
  root((Exam Topics))
    Analytics Levels
      Descriptive
        What happened
        Past events
        Foundation level
      Diagnostic
        Why happened
        Root causes
        Statistical techniques
      Predictive
        What will happen
        Forecasting
        ML models
      Prescriptive
        What should do
        Recommendations
        Optimization
    Roles
      Data Analyst
      Data Scientist
      Data Engineer
      BI Analyst
      Others
    Skills
      Hard Skills
        Technical
        Quantifiable
      Soft Skills
        Interpersonal
        Personality-based
    EDA
      Descriptive Techniques
      Diagnostic Techniques
      Purpose
      Analytics Levels
</div>

##### ðŸ’¡ Study Tips

1. **Create Flashcards** for:
   - Four analytics levels (questions, focus, complexity)
   - Eight roles (responsibilities, key differences)
   - Hard vs Soft skills (examples of each)
   - EDA techniques (which level they belong to)

2. **Practice Scenarios**: 
   - Given a business problem, identify which analytics level would be used
   - Given a task, identify which role would perform it
   - Given a goal, identify which EDA technique is appropriate

3. **Understand Relationships**:
   - How EDA relates to analytics levels
   - How roles work together
   - How skills complement each other

4. **Key Distinctions**:
   - Descriptive vs Diagnostic vs Predictive vs Prescriptive
   - Data Analyst vs Data Scientist vs Data Engineer
   - Hard Skills vs Soft Skills
   - Early EDA vs Advanced EDA

##### âš ï¸ Common Exam Mistakes to Avoid

1. **Confusing Analytics Levels**: Don't mix up descriptive (what happened) with prescriptive (what should we do)
2. **Role Confusion**: Data Analyst focuses on statistical analysis; Data Scientist focuses on ML/AI
3. **Skill Categorization**: Programming is a hard skill; Communication is a soft skill
4. **EDA Placement**: Remember EDA primarily fits descriptive/diagnostic but prepares for predictive/prescriptive

#### â­ Key Takeaways

1. **Data analysis transforms raw data into actionable insights** for decision-making
2. **Multiple types of analysis** exist (text, statistical, diagnostic, predictive, mining)
3. **Interdisciplinary field** connecting programming, ML, statistics, visualization, BI, and big data
4. **Specialized tools and software** are essential for effective data analysis
5. **Real-world application** requires understanding both technical and business contexts
6. **Eight key roles** in data analysis, each with specific responsibilities
7. **Roles often overlap** in smaller organizations
8. **Team collaboration** is essential for successful data projects
            """,
            "key_points": [
                "Data analysis converts raw data into actionable insights for decision-making",
                "Multiple analysis types: text, statistical, diagnostic, predictive, and data mining",
                "Interdisciplinary field combining programming, ML, statistics, visualization, BI, and big data",
                "Programming (Python/R/SQL) enables automation and complex calculations",
                "Machine learning builds predictive models from historical data",
                "Statistics provides the foundation for understanding and interpreting data",
                "Data visualization makes complex findings accessible through graphs and charts",
                "Business Intelligence helps organizations make data-driven decisions",
                "Big Data techniques handle massive datasets beyond traditional software capabilities",
                "Eight key roles: Data Analyst, Data Scientist, Data Engineer, BI Analyst, Data Architect, Statistician, ML Engineer, and Data Visualization Specialist",
                "Each role has specific responsibilities but roles often overlap in smaller organizations",
                "Data Architect designs systems, Data Engineer builds infrastructure, Data Analyst/Scientist analyze data, BI Analyst provides business insights, and Visualization Specialist creates visual presentations",
                "Team collaboration across roles is essential for successful data projects",
                "Hard skills (technical, quantifiable) include: Mathematical/Statistical skills, Programming (Python/R/SQL), Database management, Data visualization, and Data cleaning/wrangling",
                "Soft skills (interpersonal, personality-based) include: Analytical skills, Problem-solving, Communication, and Business/industry understanding",
                "Both hard and soft skills are essential - hard skills provide technical foundation, soft skills enable effective application and collaboration",
                "Employers value soft skills highly as they're harder to teach and crucial for collaborative work",
                "A typical day includes: email/planning, team meetings, data retrieval (SQL), data validation, analysis (Python), visualization (Tableau), report writing, and cross-team collaboration",
                "No two days are the same - data analysts work on multiple projects, handle various data sources, and collaborate with different teams",
                "Daily activities demonstrate practical application of technical skills: SQL, Python (Pandas/NumPy), Tableau, and data cleaning techniques",
                "The role requires balancing technical work (analysis, coding) with communication (meetings, reports, presentations)",
                "Four levels of data analysis: Descriptive (What happened?), Diagnostic (Why did it happen?), Predictive (What will happen?), and Prescriptive (What should we do?)",
                "Descriptive analytics is the foundation - summarizes historical data and identifies patterns",
                "Diagnostic analytics investigates root causes using statistical techniques and correlation analysis",
                "Predictive analytics forecasts future outcomes using regression, time series, and machine learning",
                "Prescriptive analytics provides actionable recommendations using optimization, simulation, and decision trees",
                "Complexity and value increase from descriptive to prescriptive analytics, but all levels are important for decision-making",
                "Exploratory Data Analysis (EDA) focuses on visual, intuitive techniques performed before formal statistical modeling",
                "EDA primarily fits into descriptive and diagnostic analytics levels - summarizing data and investigating relationships",
                "Early EDA uses measures of central tendency (mean, median, mode) and dispersion (range, variance, standard deviation)",
                "EDA visualizations include histograms, box plots, scatter plots for understanding data distributions and relationships",
                "Advanced EDA investigates relationships between variables, detects anomalies, and discovers patterns through grouping",
                "EDA results serve as critical inputs to predictive analytics (feature engineering) and prescriptive analytics (decision scenarios)",
                "EDA is an iterative process: load data â†’ explore â†’ describe â†’ visualize â†’ investigate â†’ validate â†’ prepare for modeling"
            ],
            "visual_elements": {
                "diagrams": True,
                "tables": True,
                "highlighted_sections": True
            }
        },
        {
            "lesson_number": "1.2",
            "title": "It's All About Data",
            "content": """
### What is Data?

When discussing data analysis, **data** refers to unprocessed, raw facts or statistics gathered for analysis. Data can be collected from various sources for various tasks, such as making decisions, forecasting trends, figuring out how users behave, and more. Data can be broadly divided into **qualitative** and **quantitative** categories.

#### ðŸŽ¯ Data Overview

<div class="mermaid">
graph TB
    A[Data<br/>Raw Facts & Statistics] --> B[Qualitative Data<br/>Non-numerical<br/>Categorical Variables]
    A --> C[Quantitative Data<br/>Numerical<br/>Measurable]
    
    B --> B1[Nominal<br/>No order]
    B --> B2[Ordinal<br/>Has order]
    B --> B3[Dichotomous<br/>Only 2 categories]
    
    C --> D[Discrete Data<br/>Countable<br/>Whole numbers only]
    C --> E[Continuous Data<br/>Any value in range<br/>Decimals possible]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B1 fill:#8E44AD,stroke:#6C3483,stroke-width:2px,color:#fff
    style B2 fill:#7D3C98,stroke:#6C3483,stroke-width:2px,color:#fff
    style B3 fill:#6C3483,stroke:#4A235A,stroke-width:2px,color:#fff
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
</div>

#### ðŸ“Š Qualitative vs Quantitative Data

| Type | Description | Examples |
|------|-------------|----------|
| **Qualitative** | Non-numerical, descriptive information | Customer feedback, survey responses, interview notes, categories (e.g., "satisfied", "neutral") |
| **Quantitative** | Numerical, measurable information | Sales figures, temperature, weight, number of visitors, revenue |

#### ðŸ“ Qualitative Data: Categories

Non-numerical, text-based **qualitative data** typically describes qualities or characteristics. They stand for distinct classifications or groups to which an observation may belong. This type of data's individual data points can be considered **categorical variables**, and they are not intended for use in calculations. However, they are necessary when classifying or grouping data. Additional categories for qualitative data include:

##### 1. Nominal Data ðŸ·ï¸

**Definition**: Although this data can be divided into categories, it **lacks any sense of priority or order**. Categories are labels with no inherent ranking.

**Characteristics**:
- Categories with no order
- Cannot rank or compare (apple is not "better" than banana)
- Used for classification only
- Often represented as text or codes

**Examples**:
- Types of fruit (apple, banana, orange)
- Gender (male, female, other)
- Country of residence (Norway, Sweden, Denmark)
- Product category (Electronics, Clothing, Home)
- Blood type (A, B, AB, O)

<div class="mermaid">
flowchart LR
    A[Nominal Data] --> B[No Order]
    B --> C[Categories Only]
    C --> D[Examples:<br/>â€¢ Fruit types<br/>â€¢ Gender<br/>â€¢ Country]
    
    style A fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
</div>

##### 2. Ordinal Data ðŸ“Š

**Definition**: This data is divided into different categories and has a **distinct order**. Categories can be ranked, but the distance between categories may not be equal.

**Characteristics**:
- Categories with meaningful order
- Can rank (good > average > poor)
- Distance between levels may not be equal
- Used for ratings and rankings

**Examples**:
- Movie ratings (poor, average, good, excellent)
- Satisfaction level (very dissatisfied, dissatisfied, neutral, satisfied, very satisfied)
- Education level (high school, bachelor's, master's, PhD)
- Size (small, medium, large)
- Likert scale (strongly disagree, disagree, neutral, agree, strongly agree)

<div class="mermaid">
flowchart LR
    A[Ordinal Data] --> B[Has Order]
    B --> C[Rankable]
    C --> D[Examples:<br/>â€¢ Movie ratings<br/>â€¢ Satisfaction<br/>â€¢ Education level]
    
    style A fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
</div>

##### 3. Dichotomous (Binary/Binomial) Data âš–ï¸

**Definition**: There are **only two categories** for nominal variables. They serve as indicators of an attribute's **presence or absence**.

**Characteristics**:
- Exactly two categories
- Yes/No, True/False type
- Special case of nominal data
- Often used in logic and filtering

**Examples**:
- Employment status (employed / unemployed)
- Gender (male / female) â€” when only two options
- Purchase made (yes / no)
- Pass/Fail
- Active/Inactive
- True/False

<div class="mermaid">
flowchart LR
    A[Dichotomous Data] --> B[Only 2 Categories]
    B --> C[Presence/Absence]
    C --> D[Examples:<br/>â€¢ Employed/Unemployed<br/>â€¢ Yes/No<br/>â€¢ Pass/Fail]
    
    style A fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
</div>

#### ðŸŽ¨ Qualitative Data Types Overview

<div class="mermaid">
graph TB
    subgraph "Qualitative Data - Categorical Variables"
        Q[Qualitative Data<br/>Non-numerical<br/>Descriptive]
        Q --> N[Nominal<br/>No order<br/>e.g. Fruit: apple, banana, orange]
        Q --> O[Ordinal<br/>Has order<br/>e.g. Ratings: poor, average, good, excellent]
        Q --> D[Dichotomous<br/>Only 2 categories<br/>e.g. Employed / Unemployed]
    end
    
    style Q fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style N fill:#8E44AD,stroke:#6C3483,stroke-width:2px,color:#fff
    style O fill:#7D3C98,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#6C3483,stroke:#4A235A,stroke-width:2px,color:#fff
</div>

#### ðŸ“‹ Qualitative Data Types Comparison

| Type | Order? | Categories | Examples |
|------|--------|------------|----------|
| **Nominal** | No | Multiple, no ranking | Fruit types, country, blood type |
| **Ordinal** | Yes | Multiple, rankable | Movie ratings, satisfaction level, education |
| **Dichotomous** | No (or N/A) | Exactly 2 | Employed/Unemployed, Yes/No, Pass/Fail |

#### ðŸ’¡ Why This Matters: Data Types Affect Analysis

Comprehending these types of variables and data in data analysis is **essential** because:

1. **Types of analyses you can run**: Different statistical tests and methods apply to different data types (e.g., chi-square for categorical, t-test for continuous).
2. **Data pre-processing methods**: How you handle the data depends on its type:
   - **Missing data**: Different strategies for categorical vs numerical (e.g., mode for nominal, mean for continuous).
   - **Normalisation/Scaling**: Numerical data may need scaling; categorical data may need encoding (e.g., one-hot encoding for nominal).
3. **Visualisation choices**: Bar charts for categorical, histograms for continuous, etc.
4. **Model selection**: Machine learning models require appropriate input types (e.g., decision trees handle categories, regression needs numerical).

<div class="mermaid">
flowchart TD
    A[Data Type] --> B[Affects Analysis]
    A --> C[Affects Pre-processing]
    
    B --> B1[Statistical tests<br/>e.g. Chi-square vs t-test]
    B --> B2[Visualisation<br/>Bar chart vs Histogram]
    B --> B3[Model selection<br/>Encoding requirements]
    
    C --> C1[Missing data handling<br/>Mode vs Mean]
    C --> C2[Normalise/Scale<br/>When and how]
    C --> C3[Encoding<br/>One-hot for categorical]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

#### ðŸ“ˆ Quantitative Data: Two Main Categories

**Quantitative information** is measurable or numerical information that can be used in statistical or mathematical calculations. These kinds of discrete data points are sometimes referred to as **numerical variables**.

Additional categories for quantitative data include:

##### 1. Discrete Data ðŸ“Š

**Definition**: Only particular (countable) values are allowable for this data. Only whole numbers should be used.

**Characteristics**:
- Countable values
- Whole numbers only
- No values between counts
- Often represents counts or categories

**Examples**:
- Number of people who visit a website (e.g., 1, 2, 3, 150, 1000)
- Number of automobiles a person owns (0, 1, 2, 3...)
- Number of students in a class
- Number of products sold
- Number of customer complaints

<div class="mermaid">
flowchart LR
    A[Discrete Data] --> B[Whole Numbers Only]
    B --> C[Countable]
    C --> D[Examples:<br/>â€¢ Website visitors<br/>â€¢ Cars owned<br/>â€¢ Products sold]
    
    style A fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
</div>

##### 2. Continuous Data ðŸ“

**Definition**: This data is not limited to whole numbers and can take any value within a specified range. Can include decimals and fractions.

**Characteristics**:
- Any value within a range
- Decimals and fractions possible
- Measurable (not just countable)
- Often represents measurements

**Examples**:
- Temperature of a room (20.5Â°C, 21.3Â°C, 22.7Â°C)
- A person's weight (70.45 kg, 70.47 kg, 71.2 kg)
- Height (175.3 cm)
- Time (2.5 hours, 3.75 minutes)
- Revenue ($1,234.56)

<div class="mermaid">
flowchart LR
    A[Continuous Data] --> B[Any Value in Range]
    B --> C[Decimals Possible]
    C --> D[Examples:<br/>â€¢ Temperature<br/>â€¢ Weight<br/>â€¢ Height<br/>â€¢ Time]
    
    style A fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style B fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style C fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style D fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
</div>

#### ðŸ”„ Discrete vs Continuous Comparison

<div class="mermaid">
graph TB
    subgraph "Discrete Data"
        D1[Whole numbers only]
        D2[Countable: 1, 2, 3...]
        D3[No values between]
        D4[Website visitors: 150]
        D5[Cars owned: 2]
    end
    
    subgraph "Continuous Data"
        C1[Any value in range]
        C2[Decimals: 70.45, 70.47...]
        C3[Infinite possibilities]
        C4[Weight: 70.45 kg]
        C5[Temperature: 21.3Â°C]
    end
    
    style D1 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D2 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D3 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D4 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D5 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C1 fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style C2 fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style C3 fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style C4 fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style C5 fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
</div>

#### ðŸ“‹ Quick Reference Table

| Aspect | Discrete Data | Continuous Data |
|--------|---------------|-----------------|
| **Values** | Whole numbers only | Any value in range (including decimals) |
| **Countable?** | Yes | No (measurable) |
| **Examples** | Visitors, cars, products sold | Temperature, weight, height, time |
| **Between values?** | No (e.g., can't have 2.5 cars) | Yes (e.g., 70.45 kg) |
| **Use in** | Counting, categories | Measurement, precision |

#### ðŸ“‚ Sources of Data

Given the widespread digitisation of information and interactions, modern data analysts can access a vast array of data sources. These sources can generally be divided into **internal** (produced within the organisation) and **external** (data from outside the organisation) sources.

##### ðŸ¢ Internal Data Sources

Internal sources are those produced within an organisation and include the following:

**1. Transactional Data** ðŸ’³

This comprises information from sales, deliveries, invoices, and other commercial dealings.

**Examples**:
- **Financial institution**: Financial transactions, account activity, loan repayments
- **Retail business**: Purchase histories, order records, payment transactions
- **E-commerce**: Order IDs, product IDs, quantities, prices, timestamps

**2. Log Data** ðŸ“‹

When various events occur, systems and applications automatically generate this type of data.

**Examples**:
- **Web server logs**: Page views, click paths, session duration
- **Application logs**: User actions, feature usage, errors
- **Error logs**: System failures, debugging information
- **Reveals**: User behaviour, system performance, troubleshooting data

**3. Customer Relationship Management (CRM) Data** ðŸ‘¥

Customers' contact details, purchase histories, interactions with customer service representatives, and other data are all readily available from CRM systems.

**Examples**:
- Contact details (name, email, phone)
- Purchase histories and order frequency
- Customer service interactions (tickets, calls, chat logs)
- Lead status and sales pipeline

**4. Human Resources (HR) Data** ðŸ‘¤

This contains information on employees, such as their job descriptions, achievements, and length of service.

**Examples**:
- Job descriptions and roles
- Performance reviews and achievements
- Length of service (tenure)
- Salary bands, departments, locations

##### ðŸŒ External Data Sources

Data that may be gathered outside of an organisation is represented by external data sources. Examples include:

**1. Social Media Data** ðŸ“±

Social media platform data can reveal information about customer sentiment, new trends, and more.

**Examples**:
- Customer sentiment (positive/negative/neutral)
- Trending topics and hashtags
- Engagement metrics (likes, shares, comments)
- Brand mentions and reviews

**2. Public Data Sets** ðŸ“Š

Governments and other organisations frequently release data sets on various subjects, from health statistics to economic indicators.

**Examples**:
- Government: Census data, health statistics, economic indicators
- Open data: Weather data, traffic data, demographic data
- Research: Academic datasets, scientific publications

**3. Web Scraping** ðŸ•·ï¸

Web scraping tools can be used to collect data from websites. This may include data from news websites, forums, and other online resources.

**Examples**:
- News websites (articles, headlines)
- Forums and discussion boards
- Product listings and prices (competitor analysis)
- Job postings, real estate listings

**4. Third-Party Data Providers** ðŸª

Companies that gather and market data, including credit scores and market research data.

**Examples**:
- Credit scores and financial data
- Market research reports
- Demographic and lifestyle data
- Industry benchmarks

**5. Internet of Things (IoT) Devices** ðŸ“¡

IoT devices like connected cars, smart appliances, and wearable technology produce vast amounts of data.

**Examples**:
- Connected cars: Location, speed, fuel consumption
- Smart appliances: Usage patterns, energy consumption
- Wearable technology: Heart rate, steps, sleep patterns
- Smart home: Temperature, security, energy usage

**6. Satellite and Aerial Imagery** ðŸ›°ï¸

Geospatial and environmental data.

**Examples**:
- Satellite imagery (land use, agriculture, urban planning)
- Aerial photography (construction, surveying)
- Environmental monitoring (climate, deforestation)
- Mapping and navigation data

**7. Sensor Data** ðŸ“Ÿ

Sensors are widely used in business operations. Manufacturers may use sensors on assembly lines to collect data, utilities may use smart meters to collect usage information, and logistics firms may use GPS information from delivery vehicles.

**Examples**:
- **Manufacturing**: Assembly line sensors (temperature, pressure, quality checks)
- **Utilities**: Smart meters (energy usage, consumption patterns)
- **Logistics**: GPS from delivery vehicles (location, route, speed)
- **Retail**: Foot traffic sensors, inventory sensors

#### ðŸŽ¨ Data Sources Overview

<div class="mermaid">
graph TB
    A[Data Sources] --> B[Internal<br/>Within Organisation]
    A --> C[External<br/>Outside Organisation]
    
    B --> B1[Transactional Data<br/>Sales, Invoices, Purchases]
    B --> B2[Log Data<br/>Web, Application, Error logs]
    B --> B3[CRM Data<br/>Customers, Interactions]
    B --> B4[HR Data<br/>Employees, Performance]
    
    C --> C1[Social Media<br/>Sentiment, Trends]
    C --> C2[Public Data Sets<br/>Government, Open data]
    C --> C3[Web Scraping<br/>Websites, Forums]
    C --> C4[Third-Party Providers<br/>Credit, Market research]
    C --> C5[IoT Devices<br/>Connected devices]
    C --> C6[Satellite/Imagery<br/>Geospatial, Environmental]
    C --> C7[Sensor Data<br/>Manufacturing, GPS, Smart meters]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B4 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C1 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C2 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C3 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C4 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C5 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C6 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C7 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
</div>

#### ðŸ“‹ Internal vs External Sources Summary

| Source Type | Description | Examples |
|-------------|-------------|----------|
| **Transactional** | Sales, deliveries, invoices, commercial dealings | Purchase histories, financial transactions, order records |
| **Log Data** | Auto-generated by systems when events occur | Web server logs, application logs, error logs |
| **CRM Data** | Customer information from CRM systems | Contact details, purchase history, service interactions |
| **HR Data** | Employee information | Job descriptions, achievements, tenure |
| **Social Media** | Data from social platforms | Sentiment, trends, engagement |
| **Public Data** | Released by governments/organisations | Census, health stats, economic indicators |
| **Web Scraping** | Data collected from websites | News, forums, product listings |
| **Third-Party** | Data from data providers | Credit scores, market research |
| **IoT** | Data from connected devices | Smart appliances, wearables, connected cars |
| **Satellite/Imagery** | Geospatial and environmental | Satellite imagery, aerial photography |
| **Sensor Data** | Data from sensors in operations | Assembly line sensors, smart meters, GPS |

#### ðŸ—„ï¸ Data Storage

Since there are so many data sources, how the data is stored plays a crucial role in the data analysis process. The chosen storage technique can significantly impact the **speed of access**, the **effectiveness of analyses**, and the **types of analysis** that are practical.

##### ðŸ§­ Storage Options at a Glance

<div class="mermaid">
flowchart TB
    A[Data Storage Options] --> B[Files<br/>Spreadsheets & CSV]
    A --> C[Databases<br/>SQL & NoSQL]
    A --> D[Analytics Platforms<br/>Warehouse & Lake]
    A --> E[Cloud Object Storage<br/>S3 / GCS / Azure Blob]
    A --> F[In-memory Stores<br/>Redis / Ignite]

    C --> C1[SQL (Structured)<br/>MySQL / PostgreSQL / Oracle]
    C --> C2[NoSQL (Flexible)<br/>MongoDB / Cassandra / Firebase]

    D --> D1[Data Warehouse<br/>Snowflake / Redshift / BigQuery]
    D --> D2[Data Lake<br/>Raw data in original format]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff
    style F fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style C1 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C2 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

##### 1. Spreadsheets and CSV Files ðŸ“„

Spreadsheets (like Microsoft Excel) and CSV (Comma-Separated Values) files are popular and straightforward storage formats for **smaller datasets**.

- **Pros**: Easy to use, human-readable, great for quick exploration
- **Cons**: Limited performance and scalability for large datasets

**Example use**: A marketing team exports a CSV of 5,000 leads to analyse campaign performance.

##### 2. Databases ðŸ—ƒï¸ (SQL vs NoSQL)

Databases are usually used for **larger datasets**.

**Structured (SQL) databases** store data in tables with relationships (like interconnected spreadsheets):
- Examples: MySQL, Oracle, PostgreSQL
- Best for: structured data, consistent schema, strong querying (SQL)

**Unstructured / semi-structured (NoSQL) databases** are more flexible:
- Examples: MongoDB, Cassandra, Firebase
- Best for: variable schemas, documents/JSON, high scale, mixed data types

<div class="mermaid">
flowchart LR
    A[Databases] --> B[SQL<br/>Structured tables]
    A --> C[NoSQL<br/>Flexible schema]
    B --> B1[Example: Orders table<br/>order_id, customer_id, total]
    C --> C1[Example: JSON document<br/>{customer:{...}, events:[...] }]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#000
    style C1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#000
</div>

##### 3. Data Warehouses ðŸ¢

Data warehouses are frequently used for **enormous datasets** and **analytics/reporting**. They collect information from various organisational sources and are designed for fast querying over large amounts of historical data.

- Examples: Snowflake, Amazon Redshift, Google BigQuery
- Best for: BI dashboards, historical reporting, consistent metrics (KPIs)

**Example use**: Finance runs monthly profitability reports across 5 years of transactions.

##### 4. Data Lakes ðŸžï¸

A data lake stores a very large amount of **raw, unprocessed data** in its original format until it is needed. Data lakes support flexible analysis and can store **structured, semi-structured, and unstructured** data.

- Flat architecture (files/objects) rather than a strict warehouse schema
- Best for: storing raw data, experimentation, ML feature building

**Example use**: Keeping raw clickstream logs + images + CRM extracts for future modeling.

##### 5. Cloud Storage â˜ï¸

Large datasets are frequently stored on cloud storage platforms like AWS S3, Google Cloud Storage, or Azure Blob Storage.

- **Pros**: scalable, reliable, integrates well with cloud analytics services
- **Best for**: raw files, backups, data lake storage, sharing datasets across teams

##### 6. In-memory Storage âš¡

In-memory storage can be used for **real-time analytics**. Data is stored in RAM (e.g., Redis, Apache Ignite) enabling rapid access.

- Best for: caching, realtime dashboards, low-latency features
- Trade-off: more expensive per GB than disk storage

##### ðŸ“‹ Storage Comparison Table

| Storage Type | Best For | Data Types | Typical Scale | Examples |
|--------------|----------|-----------|--------------|----------|
| **Spreadsheets/CSV** | Quick analysis, small datasets | Structured | Small | Excel, CSV |
| **SQL DB** | Transactions + querying | Structured | Mediumâ€“Large | PostgreSQL, MySQL |
| **NoSQL DB** | Flexible schema, mixed data | Semi/unstructured | Large | MongoDB, Cassandra |
| **Data Warehouse** | BI & reporting, historical analytics | Structured (modeled) | Very Large | Snowflake, BigQuery |
| **Data Lake / Object Storage** | Raw data storage, ML | Any | Very Large | S3, GCS, Azure Blob |
| **In-memory** | Real-time analytics, caching | Key/value, fast access | Medium | Redis, Ignite |

##### ðŸ’¡ How to Choose (Exam-friendly rule of thumb)

- **Small + human-readable**: Spreadsheet/CSV  
- **Transactions + strong structure**: SQL database  
- **Flexible schema + high scale**: NoSQL database  
- **Fast analytics on historical data**: Data warehouse  
- **Store everything raw for later**: Data lake / cloud object storage  
- **Need speed (milliseconds)**: In-memory storage  

#### ðŸš§ Data Silos

Remember that data may be stored separately in different places or at a specific location in the organisation's overall structure, making it difficult for other employees to access. A situation where data is **compartmentalised or isolated** within a particular department, unit, system, or application and is **not easily accessible** to or shared with other parts of the organisation is called a **'data silo'**.

##### ðŸŽ¯ What is a Data Silo?

<div class="mermaid">
graph TB
    subgraph "Organisation"
        A[Sales Department<br/>CRM System]
        B[Marketing Department<br/>Email Platform]
        C[Finance Department<br/>ERP System]
        D[HR Department<br/>HRIS System]
    end
    
    A -.No Access.-> B
    B -.No Access.-> C
    C -.No Access.-> D
    D -.No Access.-> A
    
    style A fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style B fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style C fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style D fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
</div>

**Problem**: Each department has its own data system, but they cannot easily share or access each other's data.

##### ðŸ” Causes of Data Silos

There are several causes for this compartmentalisation:

**1. Technical Reasons** ðŸ’»

Different systems or applications within the organisation may store data in different formats or use incompatible technology.

**Examples**:
- Sales uses MySQL database, Marketing uses MongoDB
- Finance system uses XML format, HR uses JSON
- Legacy systems that don't integrate with modern platforms
- Different APIs or data protocols

**2. Organisational Reasons** ðŸ¢

Within the organisation (between departments), various systems or applications might store data in multiple formats or use unrelated technologies.

**Examples**:
- Each department purchased its own system independently
- No central IT strategy or data governance
- Mergers and acquisitions bringing in different systems
- Lack of standardisation across departments

**3. Cultural Reasons** ðŸ‘¥

Due to unit competition, worries about data privacy, or a lack of knowledge about the advantages of data sharing, there may be resistance to or a lack of incentive for data sharing across the organisation.

**Examples**:
- Department competition ("We don't want to share our data")
- Privacy concerns (even when data could be safely shared)
- Lack of understanding about benefits of data sharing
- No incentives or culture promoting collaboration

##### ðŸ”„ Data Silo Causes Diagram

<div class="mermaid">
flowchart TD
    A[Data Silo] --> B[Technical Reasons<br/>ðŸ’»]
    A --> C[Organisational Reasons<br/>ðŸ¢]
    A --> D[Cultural Reasons<br/>ðŸ‘¥]
    
    B --> B1[Different formats<br/>Incompatible tech]
    B --> B2[Legacy systems<br/>Different APIs]
    
    C --> C1[Independent purchases<br/>No central strategy]
    C --> C2[Mergers & acquisitions<br/>Lack of standardisation]
    
    D --> D1[Department competition<br/>Privacy concerns]
    D --> D2[No sharing culture<br/>Lack of incentives]
    
    style A fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#000
    style B2 fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#000
    style C1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#000
    style C2 fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#000
    style D1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#000
    style D2 fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#000
</div>

##### âœ… Solutions to Data Silos

Organisations frequently use data integration initiatives to consolidate their data into a centralised data warehouse or adopt a unified data architecture to eliminate data silos.

**Technical Solutions** ðŸ”§:
- **Middleware and integration tools**: Connect different systems
- **ETL (Extract, Transform, Load)**: Move data from silos to central warehouse
- **API integration**: Enable systems to communicate
- **Unified data architecture**: Standardise data formats and storage

**Organisational Solutions** ðŸ¢:
- **Promote data-sharing culture**: Encourage collaboration
- **Establish data governance procedures**: Set standards and policies
- **Central data strategy**: Coordinate IT purchases and systems
- **Cross-departmental teams**: Break down barriers

<div class="mermaid">
flowchart LR
    A[Data Silos<br/>Isolated Systems] --> B[Integration Solutions]
    B --> C[Centralised<br/>Data Warehouse]
    B --> D[Unified<br/>Data Architecture]
    
    C --> E[All Departments<br/>Can Access Data]
    D --> E
    
    style A fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ”“ Open vs Closed Data Platforms

Another factor to consider is the platform on which the data will be stored - **open** or **closed**. Open and closed data platforms refer to various methods for using, sharing, and being able to access data.

##### ðŸŒ Open Data Platforms

A platform that allows for the **open sharing, use, and reuse of data** is known as an **open data platform**. This openness is frequently subject to minimal conditions, usually in the form of a license, such as sharing any modifications or derivatives under the same open terms or attributing the source.

**Characteristics**:
- Open sharing, use, and reuse
- Minimal restrictions (usually attribution or share-alike license)
- Publicly accessible
- Promotes transparency and collaboration

**Used in**:
- Governmental settings (transparency, public participation)
- Academic settings (research collaboration)
- Public service settings (innovation, public benefit)

**Examples**:
- **data.gov** (United States) - Government datasets
- **data.gov.uk** (United Kingdom) - UK government data
- OpenStreetMap - Open mapping data
- Kaggle Datasets - Public datasets for research

##### ðŸ”’ Closed Data Platforms

A **closed data platform** places **limitations on who can access and use the data**. Access may be restricted to particular users or groups because the data may be proprietary, confidential, or sensitive in nature.

**Characteristics**:
- Restricted access (specific users/groups)
- Requires permissions, contracts, or agreements
- Data is proprietary, confidential, or sensitive
- Protects privacy, intellectual property, or compliance

**Used in**:
- Businesses (protect trade secrets, customer data)
- Healthcare (patient privacy, HIPAA compliance)
- Financial institutions (regulatory compliance)
- Organisations with sensitive data

**Examples**:
- Company internal databases (employee-only access)
- Customer data platforms (restricted to authorised staff)
- Financial systems (compliance requirements)
- Healthcare records (privacy regulations)

##### ðŸŽ¨ Open vs Closed Platform Comparison

<div class="mermaid">
graph TB
    subgraph "Open Data Platform ðŸŒ"
        O1[Public Access]
        O2[Minimal Restrictions]
        O3[Promotes Innovation]
        O4[Examples:<br/>data.gov<br/>data.gov.uk]
    end
    
    subgraph "Closed Data Platform ðŸ”’"
        C1[Restricted Access]
        C2[Requires Permissions]
        C3[Protects Privacy/IP]
        C4[Examples:<br/>Company DB<br/>Healthcare Records]
    end
    
    style O1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style O2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style O3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style O4 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C1 fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style C2 fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style C3 fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style C4 fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

##### ðŸ“‹ Open vs Closed Comparison Table

| Aspect | Open Data Platform | Closed Data Platform |
|--------|-------------------|---------------------|
| **Access** | Public, open to all | Restricted to authorised users |
| **Restrictions** | Minimal (attribution, share-alike) | Requires permissions/contracts |
| **Purpose** | Transparency, innovation, collaboration | Privacy, IP protection, compliance |
| **Use Cases** | Government, academic, public service | Business, healthcare, finance |
| **Examples** | data.gov, data.gov.uk, OpenStreetMap | Company databases, patient records |
| **Data Type** | Public interest data | Proprietary, confidential, sensitive |

##### ðŸ’¡ Hybrid Approach

In reality, depending on the type of data and their requirements, many organisations use a **combination of open and closed data**. For instance, a business might make some of its data publicly accessible to foster innovation and collaboration while keeping other data private to safeguard client privacy or trade secrets.

**Example**: A retail company might:
- **Open**: Publish anonymised sales trends and market research (promote innovation)
- **Closed**: Keep customer personal information and pricing strategies private (protect privacy and trade secrets)

<div class="mermaid">
flowchart TD
    A[Organisation Data] --> B{Data Type?}
    B -->|Public Interest| C[Open Platform<br/>ðŸŒ<br/>Public Access]
    B -->|Sensitive/Private| D[Closed Platform<br/>ðŸ”’<br/>Restricted Access]
    
    C --> E[Examples:<br/>Market trends<br/>Research data<br/>Public statistics]
    D --> F[Examples:<br/>Customer PII<br/>Trade secrets<br/>Financial records]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style E fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style F fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

#### ðŸ’¡ Key Takeaways

1. **Data** = unprocessed, raw facts or statistics gathered for analysis
2. **Qualitative data** = non-numerical, categorical variables (not for calculations, but for classifying/grouping)
3. **Nominal data** = categories with no order (e.g., fruit types, country)
4. **Ordinal data** = categories with distinct order (e.g., movie ratings: poor, average, good, excellent)
5. **Dichotomous data** = only two categories, presence/absence (e.g., employed/unemployed)
6. **Quantitative data** = numerical, measurable (used in statistical/mathematical calculations)
7. **Discrete data** = countable, whole numbers only (e.g., website visitors, cars owned)
8. **Continuous data** = any value in range, decimals possible (e.g., temperature, weight)
9. **Data types affect** the analyses you can run, pre-processing (missing data, normalisation), and model selection
10. **Internal data sources**: Transactional data, log data, CRM data, HR data (produced within organisation)
11. **External data sources**: Social media, public data sets, web scraping, third-party providers, IoT, satellite/imagery, sensor data (from outside organisation)
12. **Data storage choices** affect access speed, scalability, and what analysis is practical
13. Common storage methods: **Spreadsheets/CSV**, **Databases (SQL/NoSQL)**, **Data Warehouses**, **Data Lakes**, **Cloud Object Storage**, **In-memory stores**
14. **Data silos** = data compartmentalised/isolated within departments, not easily accessible to other parts of organisation
15. **Data silo causes**: Technical (incompatible formats/tech), Organisational (independent systems), Cultural (competition, privacy concerns)
16. **Solutions to silos**: Data integration, centralised warehouse, unified architecture, data-sharing culture, data governance
17. **Open data platforms** = public access, minimal restrictions (e.g., data.gov, data.gov.uk) - used for transparency and innovation
18. **Closed data platforms** = restricted access, requires permissions (e.g., company DBs, healthcare records) - protects privacy/IP/compliance
19. Many organisations use **hybrid approach**: open data for public interest, closed data for sensitive/private information
            """,
            "key_points": [
                "Data refers to unprocessed, raw facts or statistics gathered for analysis",
                "Data is broadly divided into qualitative (non-numerical, categorical) and quantitative (numerical) categories",
                "Qualitative data points are categorical variables; not for calculations but necessary for classifying/grouping",
                "Nominal data: categories with no order (e.g., fruit types: apple, banana, orange)",
                "Ordinal data: categories with distinct order (e.g., movie ratings: poor, average, good, excellent)",
                "Dichotomous (binary): only two categories, indicates presence/absence (e.g., employed/unemployed)",
                "Quantitative data is measurable and used in statistical or mathematical calculations (numerical variables)",
                "Discrete data: countable, whole numbers only (e.g., website visitors, cars owned)",
                "Continuous data: any value in range, decimals possible (e.g., temperature, weight)",
                "Data types affect: types of analyses, pre-processing (missing data, normalisation/scale), and model selection",
                "Internal data sources: Transactional (sales, invoices), Log (web/application/error logs), CRM (customers), HR (employees)",
                "External data sources: Social media, public data sets, web scraping, third-party providers, IoT devices, satellite/imagery, sensor data",
                "Data storage affects speed of access, scalability, and what analyses are practical",
                "Storage options include: spreadsheets/CSV, SQL/NoSQL databases, data warehouses, data lakes, cloud object storage (e.g., S3), and in-memory stores (e.g., Redis)",
                "Data silos: data compartmentalised/isolated within departments, making it difficult for other employees to access",
                "Data silo causes: Technical (different formats/incompatible tech), Organisational (independent systems, no central strategy), Cultural (competition, privacy concerns, lack of sharing culture)",
                "Solutions to data silos: Data integration initiatives, centralised data warehouse, unified data architecture, middleware/integration tools, ETL, promoting data-sharing culture, data governance",
                "Open data platforms: allow open sharing/use/reuse with minimal restrictions (e.g., data.gov, data.gov.uk) - used in government, academic, public service settings",
                "Closed data platforms: restrict access to authorised users, require permissions/contracts - used for proprietary, confidential, or sensitive data (business, healthcare, finance)",
                "Hybrid approach: organisations often use both open (public interest data) and closed (sensitive/private data) platforms depending on data type and requirements"
            ],
            "visual_elements": {
                "diagrams": True,
                "tables": True,
                "highlighted_sections": True
            }
        },
        {
            "lesson_number": "1.2",
            "title": "It's All About Data",
            "content": """
### Data Representation

Data can be stored in various ways, some offering multiple ways of representing the data. In the broadest sense, data may be stored in an **unstructured manner** or represented in a **structured, tabular format**.

#### ðŸŽ¯ Core Concept: Structured vs Unstructured Data

<div class="mermaid">
flowchart LR
    A[Data Representation] --> B[Structured Data<br/>ðŸ“Š<br/>Organized & Tabular]
    A --> C[Unstructured Data<br/>ðŸ“<br/>Free-form & Flexible]

    B --> B1[Relational Databases]
    B --> B2[CSV/Excel Files]
    B --> B3[Data Frames]

    C --> C1[Text Documents]
    C --> C2[Images/Videos]
    C --> C3[Audio Files]
    C --> C4[NoSQL Databases]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B2 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B3 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style C2 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style C3 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style C4 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
</div>

---

### ðŸ“Š Structured Data (Tabular Format)

Data is typically kept in a **tabular format**. In this format, data is displayed as tables, similar to those found in spreadsheets, where each **row** corresponds to a **record or entity** and each **column** to one of its **attributes**.

#### ðŸŽ¨ Tabular Data Structure

<div class="mermaid">
graph TB
    subgraph "Tabular Data Structure"
        A[Table] --> B[Rows = Records/Entities]
        A --> C[Columns = Attributes]
        B --> D[Example: Customer 1<br/>Customer 2<br/>Customer 3]
        C --> E[Example: Name<br/>Age<br/>Email<br/>Purchase Amount]
    end

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style E fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
</div>

Several instances of structured data representation include the following:

#### 1. Relational Databases ðŸ—„ï¸

Relational databases frequently employ this type of representation (like **MySQL**, **PostgreSQL**, etc.). Data is stored in various tables here, and the **relationships between them are established using keys** (primary and foreign keys). This enables strong **SQL querying capabilities** (Structured Query Language).

**Characteristics**:
- Data stored in multiple related tables
- Primary keys uniquely identify records
- Foreign keys establish relationships between tables
- Powerful SQL querying capabilities
- ACID compliance (Atomicity, Consistency, Isolation, Durability)

**Examples**: MySQL, PostgreSQL, Oracle, SQL Server, SQLite

<div class="mermaid">
graph LR
    subgraph "Relational Database Structure"
        A[Customers Table] -->|Foreign Key| B[Orders Table]
        B -->|Foreign Key| C[Order Items Table]
        C -->|Foreign Key| D[Products Table]
    end

    A --> A1[customer_id PK<br/>name<br/>email<br/>phone]
    B --> B1[order_id PK<br/>customer_id FK<br/>order_date<br/>total]
    C --> C1[item_id PK<br/>order_id FK<br/>product_id FK<br/>quantity]
    D --> D1[product_id PK<br/>name<br/>price<br/>category]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style A1 fill:#E3F2FD,stroke:#2196F3,stroke-width:2px
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style D1 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
</div>

#### 2. CSV/Excel Files ðŸ“‘

Another popular method is using **CSV** or **Excel** files to represent tabular data. These are **readable by humans** and **simple to interpret**, but they lack the speed and scalability of databases.

**Characteristics**:
- Human-readable format
- Easy to create and edit
- Portable across different systems
- Limited scalability for large datasets
- No built-in data validation or relationships
- Simple to share and collaborate

**Use Cases**:
- Small to medium datasets
- Data exchange between systems
- Quick data exploration
- Reporting and exports

<div class="mermaid">
flowchart TD
    A[CSV/Excel Files] --> B[Advantages]
    A --> C[Disadvantages]

    B --> B1[âœ“ Human-readable]
    B --> B2[âœ“ Easy to use]
    B --> B3[âœ“ Portable]
    B --> B4[âœ“ Universal support]

    C --> C1[âœ— Limited scalability]
    C --> C2[âœ— No data validation]
    C --> C3[âœ— No relationships]
    C --> C4[âœ— Performance issues<br/>with large data]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B2 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B3 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B4 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style C2 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style C3 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style C4 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
</div>

#### 3. Data Frames ðŸ¼

Similar tabular structures are used in data analysis programming languages like **Python** (pandas library) and **R** under the name **Data Frames**. These are **mutable two-dimensional structures** that can hold various data types, including Python objects, strings, integers, and floating-point numbers (columns can be inserted and deleted).

**Characteristics**:
- Two-dimensional labeled data structure
- Columns can contain different data types
- Mutable (can be modified)
- Powerful indexing and selection capabilities
- Built-in data manipulation methods
- Integration with visualization libraries

**Examples**:
- **Python**: pandas DataFrame
- **R**: data.frame, tibble
- **Julia**: DataFrame

<div class="mermaid">
graph TB
    subgraph "Data Frame Structure"
        A[DataFrame] --> B[Index<br/>Row Labels]
        A --> C[Columns<br/>Column Names]
        A --> D[Data<br/>Values]

        B --> B1[0, 1, 2, 3...]
        C --> C1[Name, Age, City...]
        D --> D1[Mixed Types:<br/>int, float, str, object]
    end

    E[Operations] --> F[Selection]
    E --> G[Filtering]
    E --> H[Grouping]
    E --> I[Aggregation]
    E --> J[Merging]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
</div>

#### ðŸ“‹ Structured Data Comparison Table

| Type | Best For | Scalability | Query Capability | Ease of Use | Examples |
|------|----------|-------------|------------------|-------------|----------|
| **Relational DB** | Large datasets, complex relationships | High | Very High (SQL) | Medium | MySQL, PostgreSQL |
| **CSV/Excel** | Small datasets, data exchange | Low | Low | Very High | Excel, CSV files |
| **Data Frames** | Data analysis, manipulation | Medium | High (programmatic) | High | pandas, R data.frame |

---

### ðŸ“ Unstructured Data

Not all data in today's constantly connected, modern world still follows a tabular structure. Think about all the **text**, **audio**, **video**, and **sensor data** produced by social media platforms and the Internet of Things. Although more challenging to analyse, this kind of unstructured data is **rich in information**.

<div class="mermaid">
graph TB
    A[Unstructured Data Sources] --> B[Social Media<br/>ðŸ“±]
    A --> C[IoT Devices<br/>ðŸŒ]
    A --> D[Multimedia<br/>ðŸŽ¬]
    A --> E[Documents<br/>ðŸ“„]

    B --> B1[Posts, Comments<br/>Tweets, Reviews]
    C --> C1[Sensor Data<br/>Telemetry, Logs]
    D --> D1[Images, Videos<br/>Audio Files]
    E --> E1[Emails, PDFs<br/>Reports, Articles]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
</div>

Here are some illustrations of unstructured data representation:

#### 1. Text ðŸ“

Text information may come from various places, including **emails**, **social media**, **news articles**, etc. Methods like **Natural Language Processing (NLP)** are employed to organise and analyse text data.

**Sources**:
- Social media posts (Twitter, Facebook, LinkedIn)
- Customer reviews and feedback
- News articles and blogs
- Emails and chat messages
- Documents and reports

**Analysis Techniques**:
- Sentiment analysis
- Topic modeling
- Named entity recognition
- Text classification
- Language translation

<div class="mermaid">
flowchart LR
    A[Text Data] --> B[NLP Processing]
    B --> C[Tokenization]
    B --> D[Cleaning]
    B --> E[Feature Extraction]

    C --> F[Analysis Tasks]
    D --> F
    E --> F

    F --> G[Sentiment Analysis<br/>ðŸ˜Š ðŸ˜ ðŸ˜ž]
    F --> H[Topic Modeling<br/>ðŸ“š]
    F --> I[Entity Recognition<br/>ðŸ‘¤ ðŸ“ ðŸ¢]
    F --> J[Classification<br/>ðŸ·ï¸]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style F fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style G fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style H fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style I fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style J fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
</div>

#### 2. Images/Videos ðŸŽ¬

Multi-dimensional arrays are typically used to represent image and video data:
- **Images**: height Ã— width Ã— colour channels (RGB: 3 channels)
- **Videos**: frames Ã— height Ã— width Ã— colour channels

**Representation**:
- Images stored as pixel matrices
- Each pixel contains color information (RGB values)
- Videos are sequences of image frames
- Metadata includes resolution, format, compression

**Analysis Techniques**:
- Computer vision
- Object detection and recognition
- Image classification
- Facial recognition
- Video analytics

<div class="mermaid">
graph TB
    subgraph "Image Representation"
        A[Image] --> B[Height: 1920 pixels]
        A --> C[Width: 1080 pixels]
        A --> D[Channels: 3 RGB]

        D --> D1[Red Channel]
        D --> D2[Green Channel]
        D --> D3[Blue Channel]
    end

    subgraph "Video Representation"
        E[Video] --> F[Frames: 30 fps]
        E --> G[Duration: 60 seconds]
        E --> H[Total: 1800 frames]

        H --> I[Each frame is<br/>an image]
    end

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style E fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style D fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style H fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

#### 3. Audio ðŸŽµ

Using methods like the **Fourier transformation**, audio data is frequently represented as a **time series** or in the **frequency domain**.

**Representation**:
- Time series: amplitude over time
- Frequency domain: spectral analysis
- Waveform data
- Sampling rate (e.g., 44.1 kHz)

**Analysis Techniques**:
- Speech recognition
- Music analysis
- Sound classification
- Audio fingerprinting
- Noise reduction

<div class="mermaid">
flowchart TD
    A[Audio Data] --> B[Time Domain<br/>Waveform]
    A --> C[Frequency Domain<br/>Spectrum]

    B --> D[Fourier Transform]
    D --> C

    B --> E[Applications]
    C --> E

    E --> F[Speech Recognition<br/>ðŸŽ¤]
    E --> G[Music Analysis<br/>ðŸŽµ]
    E --> H[Sound Classification<br/>ðŸ”Š]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
</div>

#### 4. NoSQL Databases ðŸ—ƒï¸

Unstructured data is frequently stored in **NoSQL databases** (such as **MongoDB**, **Cassandra**, etc.). Because they are **flexible and scalable**, they can store and retrieve unstructured data such as **documents** (JSON, XML), **key-value pairs**, **wide-column stores**, or **graph formats**.

**Types of NoSQL Databases**:

**Document Stores** ðŸ“„
- Store data as documents (JSON, BSON, XML)
- Flexible schema
- Examples: MongoDB, CouchDB

**Key-Value Stores** ðŸ”‘
- Simple key-value pairs
- Fast retrieval
- Examples: Redis, DynamoDB

**Wide-Column Stores** ðŸ“Š
- Column-family storage
- Scalable for big data
- Examples: Cassandra, HBase

**Graph Databases** ðŸ•¸ï¸
- Store relationships between entities
- Network analysis
- Examples: Neo4j, ArangoDB

<div class="mermaid">
graph TB
    A[NoSQL Databases] --> B[Document Stores<br/>ðŸ“„]
    A --> C[Key-Value Stores<br/>ðŸ”‘]
    A --> D[Wide-Column Stores<br/>ðŸ“Š]
    A --> E[Graph Databases<br/>ðŸ•¸ï¸]

    B --> B1[MongoDB<br/>CouchDB<br/>JSON/BSON]
    C --> C1[Redis<br/>DynamoDB<br/>Fast retrieval]
    D --> D1[Cassandra<br/>HBase<br/>Big data]
    E --> E1[Neo4j<br/>ArangoDB<br/>Relationships]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style D1 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style E1 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
</div>

#### 5. Big Data Technologies ðŸŒ

At scale, unstructured data is processed and analysed using tools like **Hadoop** or **Spark**.

**Characteristics**:
- Distributed processing
- Handle massive datasets (petabytes)
- Parallel computation
- Fault tolerance
- Scalability

**Key Technologies**:
- **Hadoop**: Distributed storage and processing (HDFS, MapReduce)
- **Spark**: Fast in-memory processing
- **Kafka**: Real-time data streaming
- **Flink**: Stream processing

<div class="mermaid">
flowchart LR
    A[Big Data Sources<br/>Terabytes/Petabytes] --> B[Hadoop/Spark<br/>Distributed Processing]

    B --> C[Storage<br/>HDFS]
    B --> D[Processing<br/>MapReduce/Spark]
    B --> E[Streaming<br/>Kafka/Flink]

    C --> F[Analysis Results]
    D --> F
    E --> F

    F --> G[Insights &<br/>Decision Making<br/>ðŸ’¡]

    style A fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style E fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style F fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ“‹ Unstructured Data Comparison Table

| Type | Representation | Analysis Tools | Use Cases | Examples |
|------|----------------|----------------|-----------|----------|
| **Text** | Documents, strings | NLP, text mining | Sentiment analysis, topic modeling | Emails, social media, articles |
| **Images/Videos** | Multi-dimensional arrays | Computer vision, deep learning | Object detection, facial recognition | Photos, surveillance, media |
| **Audio** | Time series, frequency domain | Signal processing, ML | Speech recognition, music analysis | Voice recordings, podcasts |
| **NoSQL** | Documents, key-value, graphs | Database queries, aggregations | Flexible data storage | MongoDB, Redis, Neo4j |
| **Big Data** | Distributed files | Hadoop, Spark | Large-scale analytics | Web logs, sensor data, clickstreams |

---

### ðŸ”„ Combining Structured and Unstructured Data

To gain more thorough insights into user behaviour, you might, for instance, **combine structured data** about user demographics from a relational database with **unstructured text data** from user reviews.

<div class="mermaid">
flowchart TD
    A[Data Integration] --> B[Structured Data<br/>ðŸ“Š]
    A --> C[Unstructured Data<br/>ðŸ“]

    B --> B1[Customer Demographics<br/>Age, Location, Income]
    B --> B2[Purchase History<br/>Transactions, Amounts]

    C --> C1[Customer Reviews<br/>Text, Ratings]
    C --> C2[Social Media<br/>Posts, Comments]

    B1 --> D[Combined Analysis]
    B2 --> D
    C1 --> D
    C2 --> D

    D --> E[Comprehensive Insights<br/>ðŸ’¡]

    E --> F[Better Understanding<br/>of Customer Behavior]
    E --> G[Improved Decision<br/>Making]
    E --> H[Personalized<br/>Recommendations]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style F fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style G fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style H fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
</div>

#### ðŸ’¡ Important Insights

<div class="important-info">
**The complexity of handling unstructured data** has led to an explosion of new strategies and tools. For instance, **Natural Language Processing (NLP)** has emerged as a crucial tool for analysing textual data, assisting computers in subtly comprehending human language.

Furthermore, **machine learning** is becoming increasingly crucial in handling unstructured data, whether text, image, audio, or video. In addition to many other tasks, algorithms can be trained to:
- Classify images
- Convert speech to text
- Determine the tone of customer reviews
- And much more

This demonstrates the **enormous potential** that unstructured data possesses and the growing significance of being able to manage and analyse it well.
</div>

#### ðŸŽ¯ The Ultimate Goal

<div class="key-concept">
**Remember**: Data is only as valuable as the conclusions drawn from it. Whether the data is **structured** or **unstructured**, the ultimate objective is to derive **valuable and practical insights** to inform **decision-making**. And that's where **data analysis** really adds value.
</div>

<div class="mermaid">
flowchart TD
    A[Raw Data<br/>Structured + Unstructured] --> B[Data Processing<br/>& Analysis]

    B --> C[Extract Patterns]
    B --> D[Identify Trends]
    B --> E[Generate Insights]

    C --> F[Actionable Intelligence<br/>ðŸ’¡]
    D --> F
    E --> F

    F --> G[Informed Decision<br/>Making]

    G --> H[Business Value<br/>âœ…]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style F fill:#9B59B6,stroke:#6C3483,stroke-width:3px,color:#fff
    style G fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

### ðŸ“Š Data Analysis Workflow: From Data to Value

<div class="mermaid">
flowchart LR
    subgraph "Data Collection"
        A1[Structured Sources<br/>Databases, CSV]
        A2[Unstructured Sources<br/>Text, Images, Audio]
    end

    subgraph "Data Storage"
        B1[Relational DB]
        B2[NoSQL DB]
        B3[Data Lake]
    end

    subgraph "Data Processing"
        C1[SQL Queries]
        C2[NLP/ML]
        C3[Big Data Tools]
    end

    subgraph "Analysis & Insights"
        D1[Statistical Analysis]
        D2[Pattern Recognition]
        D3[Predictive Models]
    end

    subgraph "Decision Making"
        E1[Business Decisions<br/>ðŸ’¼]
        E2[Strategic Planning<br/>ðŸ“ˆ]
        E3[Operational Improvements<br/>âš™ï¸]
    end

    A1 --> B1
    A1 --> B3
    A2 --> B2
    A2 --> B3

    B1 --> C1
    B2 --> C2
    B3 --> C3

    C1 --> D1
    C2 --> D2
    C3 --> D3

    D1 --> E1
    D2 --> E2
    D3 --> E3

    style A1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style A2 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B2 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B3 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style C1 fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C2 fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C3 fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D1 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D2 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D3 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E1 fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E2 fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E3 fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

---

### ðŸ”‘ Key Technologies Summary

<div class="mermaid">
graph TB
    subgraph "Structured Data Technologies"
        S1[SQL Databases<br/>MySQL, PostgreSQL]
        S2[Spreadsheets<br/>Excel, CSV]
        S3[Data Frames<br/>pandas, R]
    end

    subgraph "Unstructured Data Technologies"
        U1[NLP Tools<br/>NLTK, spaCy]
        U2[Computer Vision<br/>OpenCV, TensorFlow]
        U3[NoSQL Databases<br/>MongoDB, Redis]
        U4[Big Data<br/>Hadoop, Spark]
    end

    subgraph "Analysis & ML"
        M1[Machine Learning<br/>scikit-learn, PyTorch]
        M2[Deep Learning<br/>Neural Networks]
        M3[Statistical Analysis<br/>R, SciPy]
    end

    S1 --> M1
    S2 --> M1
    S3 --> M1
    U1 --> M2
    U2 --> M2
    U3 --> M1
    U4 --> M1

    M1 --> Result[Insights &<br/>Predictions<br/>ðŸ’¡]
    M2 --> Result
    M3 --> Result

    style S1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style S2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style S3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style U1 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style U2 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style U3 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style U4 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style M1 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style M2 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style M3 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style Result fill:#FFD700,stroke:#B8860B,stroke-width:3px
</div>

---

### ðŸ”¢ Data Types

Tabular data structures can store a variety of **basic data types**. Although their naming differs across platforms, most storage systems share the same fundamental data types, such as **numeric**, **string-based**, **Boolean**, and **date/time**.

<div class="mermaid">
graph TB
    A[Fundamental Data Types] --> B[Numeric<br/>ðŸ”¢<br/>Numbers & Calculations]
    A --> C[String/Character<br/>ðŸ“<br/>Text Data]
    A --> D[Boolean<br/>âœ“/âœ—<br/>True/False]
    A --> E[Date/Time<br/>ðŸ“…<br/>Temporal Data]

    B --> B1[Integer]
    B --> B2[Float]
    B --> B3[Complex]

    C --> C1[Character]
    C --> C2[String]

    D --> D1[True 1]
    D --> D2[False 0]

    E --> E1[Date]
    E --> E2[Time]
    E --> E3[DateTime]
    E --> E4[TimeSpan]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B2 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B3 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style C2 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style D1 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style D2 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style E1 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style E2 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style E3 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style E4 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
</div>

---

#### 1. Numeric Data Types ðŸ”¢

**Numeric data** is used to store values on which **mathematical operations are to be carried out** and is the first type of data.

##### Integer (int) ðŸ”¢

This represents **positive and negative whole numbers**, such as -1, 0, 1, 2, etc.

**Characteristics**:
- Whole numbers only (no decimals)
- Can be positive, negative, or zero
- Used for counting, indexing, IDs
- Fixed size in memory (e.g., 32-bit, 64-bit)

**Examples**:
- Age: 25, 30, 45
- Count: 0, 1, 100, 1000
- Temperature: -10, 0, 25 (in Celsius)
- ID numbers: 12345, 67890

**Use Cases**:
- Counting items (inventory, users, transactions)
- Indexing arrays and databases
- Representing discrete quantities
- Loop counters and iterations

##### Float (Floating-Point) ðŸŽ¯

This represents **real numbers** (i.e., decimal numbers), such as -1.23, 0.0, 3.14, etc.

**Characteristics**:
- Can represent decimal values
- Approximate representation (not always exact)
- Used for measurements and calculations
- Variable precision (float32, float64/double)

**Examples**:
- Price: 19.99, 99.95, 1234.56
- Weight: 65.5 kg, 150.2 lbs
- Temperature: 36.6Â°C, 98.6Â°F
- Pi: 3.14159...

**Use Cases**:
- Financial calculations (prices, amounts)
- Scientific measurements
- Statistical analysis
- Coordinates (latitude, longitude)

##### Complex Numbers ðŸŒ€

This represents **complex numbers**, also called composite numbers, which include both **real and imaginary components**, such as 3 + 2j. Complex numbers are frequently represented by two float values in computing systems.

**Characteristics**:
- Combination of real and imaginary parts
- Format: a + bj (where j = âˆš-1)
- Used in advanced mathematics and engineering
- Stored as two float values

**Examples**:
- 3 + 2j
- -1 + 4j
- 5.5 + 0j (real number as complex)
- 0 + 3j (purely imaginary)

**Use Cases**:
- Signal processing
- Electrical engineering
- Quantum mechanics
- Advanced mathematical computations

<div class="mermaid">
graph TB
    subgraph "Numeric Data Types"
        A[Numeric Types] --> B[Integer<br/>Whole Numbers]
        A --> C[Float<br/>Decimal Numbers]
        A --> D[Complex<br/>Real + Imaginary]

        B --> B1[Examples:<br/>-1, 0, 1, 100]
        B --> B2[Use: Counting,<br/>IDs, Indexing]

        C --> C1[Examples:<br/>3.14, -1.23, 99.99]
        C --> C2[Use: Prices,<br/>Measurements]

        D --> D1[Examples:<br/>3+2j, -1+4j]
        D --> D2[Use: Engineering,<br/>Signal Processing]
    end

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B2 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style C2 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style D1 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style D2 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
</div>

---

#### 2. String/Character Data Types ðŸ“

Data can also be saved as **strings or characters**. This is commonly referred to as **text storage** in many systems. A **single entity**, like the letter 'a', is referred to as a **character**, while a **group of characters**, like 'abc', is referred to as a **string**.

<div class="key-concept">
**Important**: This type of data includes **any character** included in the encoding/representation format used on the local device in addition to alphabetic characters. This includes:
- Alphabetic characters (A-Z, a-z)
- Punctuation (., !, ?, etc.)
- Special characters (@, #, $, %, etc.)
- Numbers that aren't used in calculations (like a phone number: "555-1234")
- Whitespace (spaces, tabs, newlines)
- Unicode characters (emojis, international characters)
</div>

##### Character (char) ðŸ”¤

A **single character** or symbol.

**Characteristics**:
- Single letter, digit, or symbol
- Enclosed in single quotes: 'a', '1', '@'
- Fixed size (typically 1 byte for ASCII, more for Unicode)

**Examples**:
- 'a', 'Z', '5', '@', '!', ' '

##### String (str) ðŸ“„

A **sequence of characters** forming text.

**Characteristics**:
- Multiple characters grouped together
- Enclosed in quotes: "hello", 'world'
- Variable length
- Immutable in some languages, mutable in others

**Examples**:
- "Hello, World!"
- "John Doe"
- "555-1234" (phone number as text)
- "user@example.com"
- "2023-06-20" (date as text)

**Use Cases**:
- Names and addresses
- Email addresses and URLs
- Phone numbers (not for calculations)
- Descriptions and comments
- Identifiers (product codes, SKUs)

<div class="mermaid">
flowchart LR
    A[Text Data] --> B[Character<br/>Single Entity]
    A --> C[String<br/>Multiple Characters]

    B --> B1['a', 'Z', '5', '@']

    C --> C1["Names:<br/>'John Doe'"]
    C --> C2["Emails:<br/>'user@example.com'"]
    C --> C3["Phone:<br/>'555-1234'"]
    C --> C4["Text:<br/>'Hello, World!'"]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B1 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style C1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C2 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C3 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C4 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
</div>

---

#### 3. Boolean Data Type âœ“/âœ—

**Boolean formats** can also be used to represent data. This data type is used when only **True or False** values need to be stored. When storing Boolean (or Bool) data, only **one binary bit** is typically used, with **0 denoting false** and **1 denoting true**.

**Characteristics**:
- Only two possible values: True or False
- Minimal storage (1 bit)
- Used for logical operations and conditions
- Named after mathematician George Boole

**Representations**:
- True: 1, true, True, yes, on
- False: 0, false, False, no, off

**Examples**:
- is_active: True
- has_discount: False
- is_verified: True
- email_confirmed: False

**Use Cases**:
- Flags and switches (is_active, is_deleted)
- Conditional logic (if statements)
- User preferences (notifications_enabled)
- Status indicators (is_paid, is_shipped)
- Validation results (is_valid, has_error)

<div class="mermaid">
graph TB
    A[Boolean Data Type] --> B[True<br/>âœ“<br/>1]
    A --> C[False<br/>âœ—<br/>0]

    B --> B1[Examples:<br/>is_active = True<br/>has_discount = True<br/>is_verified = True]

    C --> C1[Examples:<br/>is_deleted = False<br/>has_error = False<br/>is_paid = False]

    B --> B2[Use Cases:<br/>Flags, Switches<br/>Conditions, Status]
    C --> B2

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style B2 fill:#E3F2FD,stroke:#2196F3,stroke-width:2px
</div>

---

#### 4. Date/Time Data Types ðŸ“…

The final data type that might be frequently used is **date/time**. They can take a variety of shapes, including **dates** (like 2023-06-20), **times** (like 13:45:30), **datetimes** (a mashup of dates and times), and **time spans**.

##### Date ðŸ“…

Represents a **calendar date** without time information.

**Characteristics**:
- Year, month, and day
- No time component
- Various formats (ISO, US, European)

**Common Formats**:
- ISO 8601: 2023-06-20 (YYYY-MM-DD)
- US Format: 06/20/2023 (MM/DD/YYYY)
- European: 20/06/2023 (DD/MM/YYYY)
- Long Format: June 20, 2023

**Examples**:
- Birth date: 1990-05-15
- Order date: 2023-12-25
- Expiry date: 2024-12-31

**Use Cases**:
- Birth dates and anniversaries
- Order and delivery dates
- Expiration dates
- Event scheduling

##### Time â°

Represents a **time of day** without date information.

**Characteristics**:
- Hours, minutes, seconds (and sometimes milliseconds)
- No date component
- 24-hour or 12-hour format

**Common Formats**:
- 24-hour: 13:45:30 (HH:MM:SS)
- 12-hour: 01:45:30 PM
- With milliseconds: 13:45:30.123

**Examples**:
- Meeting time: 14:30:00
- Store opening: 09:00:00
- Alarm time: 07:30:00

**Use Cases**:
- Appointment times
- Business hours
- Alarm settings
- Time tracking

##### DateTime ðŸ“…â°

Represents a **combination of date and time**.

**Characteristics**:
- Complete timestamp
- Includes both date and time
- Often includes timezone information

**Common Formats**:
- ISO 8601: 2023-06-20T13:45:30
- With timezone: 2023-06-20T13:45:30+00:00
- Unix timestamp: 1687267530 (seconds since 1970-01-01)

**Examples**:
- Order timestamp: 2023-12-25T10:30:00
- Login time: 2023-06-20T13:45:30Z
- Event start: 2024-01-01T00:00:00

**Use Cases**:
- Transaction timestamps
- Log entries
- Event scheduling with specific times
- Audit trails

##### TimeSpan/Duration â±ï¸

Represents a **duration or interval of time**.

**Characteristics**:
- Difference between two times
- Can be positive or negative
- Measured in days, hours, minutes, seconds

**Common Formats**:
- Days.Hours:Minutes:Seconds
- Total seconds or milliseconds
- Human-readable: "2 days, 3 hours, 15 minutes"

**Examples**:
- Duration: 2 days, 5 hours
- Elapsed time: 00:45:30 (45 minutes, 30 seconds)
- Time difference: -01:30:00 (1.5 hours ago)

**Use Cases**:
- Calculating age or duration
- Time differences between events
- Session duration
- Project timelines

<div class="mermaid">
graph TB
    subgraph "Date/Time Data Types"
        A[Temporal Data] --> B[Date<br/>ðŸ“…<br/>Calendar Day]
        A --> C[Time<br/>â°<br/>Time of Day]
        A --> D[DateTime<br/>ðŸ“…â°<br/>Date + Time]
        A --> E[TimeSpan<br/>â±ï¸<br/>Duration]

        B --> B1[2023-06-20<br/>06/20/2023<br/>June 20, 2023]
        B --> B2[Use: Birth dates,<br/>Order dates,<br/>Expiry dates]

        C --> C1[13:45:30<br/>01:45:30 PM<br/>14:30:00]
        C --> C2[Use: Appointments,<br/>Business hours,<br/>Alarms]

        D --> D1[2023-06-20T13:45:30<br/>Unix: 1687267530]
        D --> D2[Use: Timestamps,<br/>Logs, Events,<br/>Audit trails]

        E --> E1[2 days, 5 hours<br/>00:45:30<br/>-01:30:00]
        E --> E2[Use: Duration,<br/>Time differences,<br/>Session length]
    end

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style E fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style B1 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style B2 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style C1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C2 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style D1 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style D2 fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style E1 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style E2 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
</div>

#### ðŸ“‹ Data Types Summary Table

| Category | Type | Description | Examples | Common Uses |
|----------|------|-------------|----------|-------------|
| **Numeric** | Integer | Whole numbers (positive/negative) | -1, 0, 1, 100 | Counting, IDs, indexing |
| **Numeric** | Float | Decimal numbers | 3.14, -1.23, 99.99 | Prices, measurements, calculations |
| **Numeric** | Complex | Real + imaginary numbers | 3+2j, -1+4j | Engineering, signal processing |
| **Text** | Character | Single character | 'a', 'Z', '@' | Individual symbols |
| **Text** | String | Sequence of characters | "Hello", "user@email.com" | Names, emails, descriptions |
| **Logical** | Boolean | True or False | True, False, 1, 0 | Flags, conditions, status |
| **Temporal** | Date | Calendar date | 2023-06-20 | Birth dates, order dates |
| **Temporal** | Time | Time of day | 13:45:30 | Appointments, schedules |
| **Temporal** | DateTime | Date + Time | 2023-06-20T13:45:30 | Timestamps, logs, events |
| **Temporal** | TimeSpan | Duration/interval | 2 days, 5 hours | Duration, time differences |

#### ðŸŽ¯ Choosing the Right Data Type

<div class="mermaid">
flowchart TD
    Start{What kind of<br/>data?} --> Numbers{Numbers?}
    Start --> Text{Text?}
    Start --> Logic{True/False?}
    Start --> Temporal{Date/Time?}

    Numbers -->|Whole numbers| Integer[Integer<br/>-1, 0, 1, 100]
    Numbers -->|Decimals| Float[Float<br/>3.14, 99.99]
    Numbers -->|Complex math| Complex[Complex<br/>3+2j]

    Text -->|Single char| Char[Character<br/>'a', '@']
    Text -->|Multiple chars| String[String<br/>"Hello World"]

    Logic --> Boolean[Boolean<br/>True/False]

    Temporal -->|Just date| Date[Date<br/>2023-06-20]
    Temporal -->|Just time| Time[Time<br/>13:45:30]
    Temporal -->|Both| DateTime[DateTime<br/>2023-06-20T13:45:30]
    Temporal -->|Duration| TimeSpan[TimeSpan<br/>2 days, 5 hours]

    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style Integer fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Float fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Complex fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Char fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style String fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Boolean fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    style Date fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style Time fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style DateTime fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style TimeSpan fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
</div>

#### ðŸ’¡ Important Considerations

<div class="important-info">
**Data Type Selection Matters**:

1. **Storage Efficiency**: Choosing the right data type affects storage space and memory usage
2. **Performance**: Operations on appropriate data types are faster and more efficient
3. **Accuracy**: Using float for currency can cause rounding errors; use decimal types instead
4. **Validation**: Data types enforce constraints (e.g., Boolean can only be True/False)
5. **Compatibility**: Different systems may have different data type names but similar concepts
6. **Conversion**: Be aware of type conversion (casting) and potential data loss

**Common Pitfalls**:
- Storing phone numbers as integers (loses leading zeros and formatting)
- Using float for financial calculations (precision issues)
- Storing dates as strings (difficult to sort and calculate)
- Not considering timezone for DateTime values
</div>

---

### ðŸ”„ The Data Life Cycle

Every piece of data goes through a series of stages called the **data lifecycle**, starting with its initial generation or capture and ending with its eventual archival and retirement. Understanding this lifecycle can help organisations manage their data effectively and get the most value out of it.

<div class="mermaid">
graph LR
    A[1. Creation/<br/>Collection<br/>ðŸ“¥] --> B[2. Pre-processing<br/>ðŸ§¹]
    B --> C[3. Storage<br/>ðŸ’¾]
    C --> D[4. Processing<br/>âš™ï¸]
    D --> E[5. Analysis<br/>ðŸ“Š]
    E --> F[6. Visualisation<br/>& Reporting<br/>ðŸ“ˆ]
    F --> G[7. Action<br/>ðŸ’¡]
    G --> H[8. Archiving<br/>& Destruction<br/>ðŸ—„ï¸]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

#### The 8 Stages of the Data Life Cycle

---

##### 1. Creation or Collection ðŸ“¥

**Data generation or gathering** occurs during the lifecycle's initial phase. This might entail collecting raw data from various sources.

**Common Data Sources**:
- IoT devices and sensors
- User-generated content (social media, reviews)
- Transactional systems (sales, purchases)
- Data feeds and APIs
- Web scraping and crawling
- Manual data entry
- Third-party data providers

**Examples**:
- E-commerce: Customer clicks, purchases, cart additions
- Healthcare: Patient records, vital signs, lab results
- Manufacturing: Sensor readings, production metrics
- Social Media: Posts, likes, comments, shares

<div class="mermaid">
graph TB
    A[Data Collection] --> B[IoT Devices<br/>ðŸ“±<br/>Sensors, Wearables]
    A --> C[User Content<br/>ðŸ‘¥<br/>Social Media, Reviews]
    A --> D[Transactions<br/>ðŸ’³<br/>Sales, Purchases]
    A --> E[Data Feeds<br/>ðŸŒ<br/>APIs, Web Scraping]

    B --> F[Raw Data<br/>Storage]
    C --> F
    D --> F
    E --> F

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

---

##### 2. Pre-processing ðŸ§¹

To prepare the data for analysis, this stage frequently involves **cleaning and transforming** the data.

**Common Pre-processing Tasks**:
- **Data Cleaning**: Removing errors, inconsistencies, and noise
- **Handling Missing Data**: Imputation or removal of missing values
- **Removing Duplicates**: Identifying and eliminating duplicate records
- **Data Type Conversion**: Converting data to appropriate types
- **Normalization**: Scaling values to a standard range
- **Standardization**: Ensuring consistent formats (dates, addresses)
- **Outlier Detection**: Identifying and handling anomalous values

**Examples**:
- Standardizing date formats: "06/20/2023" â†’ "2023-06-20"
- Removing duplicate customer records
- Filling missing email addresses
- Converting text to lowercase for consistency
- Removing special characters from phone numbers

<div class="mermaid">
flowchart LR
    A[Raw Data<br/>âŒ Messy] --> B[Data Cleaning<br/>ðŸ§¹]
    B --> C[Handle Missing<br/>Values]
    C --> D[Remove<br/>Duplicates]
    D --> E[Convert<br/>Data Types]
    E --> F[Normalize/<br/>Standardize]
    F --> G[Clean Data<br/>âœ… Ready]

    style A fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style E fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style F fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style G fill:#50C878,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

##### 3. Storage ðŸ’¾

Once the data is prepared, it is **stored in a way that enables easy access and retrieval**. Depending on the size and structure of the data, different storage solutions may be used.

**Storage Options**:
- **Relational Databases**: MySQL, PostgreSQL (structured data)
- **NoSQL Databases**: MongoDB, Cassandra (unstructured/semi-structured)
- **Data Lakes**: Store raw data in native format (all types)
- **Data Warehouses**: Optimized for analytics and reporting
- **Cloud Storage**: AWS S3, Azure Blob, Google Cloud Storage
- **File Systems**: CSV, JSON, Parquet files

**Considerations**:
- Data volume and growth rate
- Access patterns and query requirements
- Cost and scalability
- Security and compliance
- Backup and disaster recovery

<div class="mermaid">
graph TB
    A[Clean Data] --> B{Data Type &<br/>Volume?}

    B -->|Structured,<br/>Relational| C[Relational DB<br/>ðŸ—„ï¸<br/>MySQL, PostgreSQL]
    B -->|Unstructured,<br/>Large Scale| D[NoSQL DB<br/>ðŸŒ<br/>MongoDB, Cassandra]
    B -->|All Types,<br/>Raw Format| E[Data Lake<br/>ðŸžï¸<br/>Store Everything]
    B -->|Analytics,<br/>Reporting| F[Data Warehouse<br/>ðŸ“Š<br/>Optimized Queries]

    C --> G[Ready for<br/>Processing]
    D --> G
    E --> G
    F --> G

    style A fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style F fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

---

##### 4. Processing âš™ï¸

The stored data is **prepared for analysis** at this stage. This could entail aggregating data, creating derived variables, or combining data from various sources.

**Common Processing Tasks**:
- **Data Aggregation**: Summarizing data (totals, averages, counts)
- **Feature Engineering**: Creating new variables from existing ones
- **Data Integration**: Combining data from multiple sources
- **Data Transformation**: Reshaping data for analysis
- **Filtering**: Selecting relevant subsets of data
- **Joining**: Merging related datasets

**Examples**:
- Calculating total sales per customer
- Creating age groups from birth dates
- Combining customer data with transaction data
- Calculating moving averages
- Creating customer segments based on behavior

<div class="mermaid">
flowchart TB
    A[Stored Data] --> B[Aggregation<br/>ðŸ“Š<br/>Sum, Avg, Count]
    A --> C[Feature Engineering<br/>ðŸ”§<br/>New Variables]
    A --> D[Data Integration<br/>ðŸ”—<br/>Combine Sources]

    B --> E[Processed Data<br/>Ready for Analysis]
    C --> E
    D --> E

    E --> F[Example:<br/>Customer 360Â° View]

    style A fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#50C878,stroke:#2E7D32,stroke-width:3px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
</div>

---

##### 5. Analysis ðŸ“Š

**Insights are gleaned** from the processed data through analysis. This is where the real value of data emerges.

**Analysis Techniques**:
- **Exploratory Data Analysis (EDA)**: Understanding data patterns and distributions
- **Statistical Analysis**: Hypothesis testing, correlation, regression
- **Machine Learning**: Predictive modeling, classification, clustering
- **Time Series Analysis**: Trend analysis, forecasting
- **A/B Testing**: Comparing different approaches
- **Cohort Analysis**: Analyzing groups over time

**Examples**:
- Identifying customer churn patterns
- Predicting future sales
- Segmenting customers by behavior
- Detecting fraud or anomalies
- Recommending products

<div class="mermaid">
graph TB
    A[Processed Data] --> B{Analysis Type?}

    B --> C[Exploratory<br/>ðŸ“ˆ<br/>Understand Patterns]
    B --> D[Statistical<br/>ðŸ“Š<br/>Test Hypotheses]
    B --> E[Machine Learning<br/>ðŸ¤–<br/>Predict & Classify]
    B --> F[Time Series<br/>â±ï¸<br/>Trends & Forecasts]

    C --> G[Insights &<br/>Discoveries<br/>ðŸ’¡]
    D --> G
    E --> G
    F --> G

    style A fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

##### 6. Visualisation and Reporting ðŸ“ˆ

The results of analyses are frequently **visualised and presented in reports** to aid comprehension and decision-making.

**Visualisation Types**:
- **Charts**: Bar charts, line charts, pie charts, scatter plots
- **Dashboards**: Interactive displays of key metrics
- **Reports**: Detailed documents with findings and recommendations
- **Infographics**: Visual representations of complex information
- **Maps**: Geographic data visualization
- **Heatmaps**: Showing patterns and correlations

**Tools**:
- Tableau, Power BI, Looker (BI tools)
- Matplotlib, Seaborn, Plotly (Python libraries)
- D3.js (JavaScript visualization)
- Excel, Google Sheets (spreadsheets)

**Examples**:
- Sales dashboard showing KPIs
- Customer segmentation visualization
- Trend analysis charts
- Geographic sales maps
- Executive summary reports

<div class="mermaid">
flowchart LR
    A[Insights] --> B[Visualisation<br/>Tools]

    B --> C[Charts<br/>ðŸ“Š<br/>Bar, Line, Pie]
    B --> D[Dashboards<br/>ðŸ“ˆ<br/>Interactive KPIs]
    B --> E[Reports<br/>ðŸ“„<br/>Detailed Findings]

    C --> F[Decision Makers<br/>ðŸ‘¥]
    D --> F
    E --> F

    F --> G[Understanding<br/>& Clarity<br/>ðŸ’¡]

    style A fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style G fill:#50C878,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

##### 7. Action ðŸ’¡

**Business decisions and actions** are guided by the revelations gained from data analysis.

**Types of Actions**:
- **Strategic Decisions**: Long-term planning and direction
- **Operational Improvements**: Process optimization and efficiency
- **Product Development**: New features or products
- **Marketing Campaigns**: Targeted promotions and messaging
- **Customer Experience**: Personalization and service improvements
- **Risk Management**: Identifying and mitigating risks
- **Resource Allocation**: Optimizing budgets and staffing

**Examples**:
- Launching a targeted marketing campaign
- Redesigning a website based on user behavior
- Adjusting pricing strategy
- Improving supply chain efficiency
- Personalizing product recommendations
- Implementing fraud prevention measures

<div class="mermaid">
graph TB
    A[Data-Driven<br/>Insights] --> B{Action Type?}

    B --> C[Strategic<br/>ðŸŽ¯<br/>Long-term Planning]
    B --> D[Operational<br/>âš™ï¸<br/>Process Optimization]
    B --> E[Marketing<br/>ðŸ“¢<br/>Campaigns & Targeting]
    B --> F[Product<br/>ðŸ›ï¸<br/>Development & Features]

    C --> G[Business<br/>Impact<br/>ðŸ“ˆ]
    D --> G
    E --> G
    F --> G

    G --> H[Competitive<br/>Advantage<br/>ðŸ†]

    style A fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
</div>

---

##### 8. Archiving and Destruction ðŸ—„ï¸

Data is frequently **archived** after it has served its purpose in case it is needed in the future or for legal reasons. When a certain amount of time has passed, and the data is no longer required, it may be **deleted** in accordance with data retention policies and laws.

**Archiving**:
- Long-term storage of inactive data
- Compliance with legal requirements
- Historical reference and auditing
- Reduced storage costs (cold storage)
- Disaster recovery and backup

**Destruction**:
- Secure deletion of obsolete data
- Compliance with data retention policies
- Privacy regulations (GDPR, CCPA)
- Reducing security risks
- Freeing up storage space

**Considerations**:
- Legal and regulatory requirements
- Data retention policies
- Privacy and security
- Cost optimization
- Audit trails

<div class="mermaid">
flowchart TD
    A[Data Lifecycle<br/>Complete] --> B{Still Needed?}

    B -->|Yes, for<br/>compliance| C[Archive<br/>ðŸ—„ï¸<br/>Long-term Storage]
    B -->|No, obsolete| D[Destroy<br/>ðŸ—‘ï¸<br/>Secure Deletion]

    C --> E[Cold Storage<br/>Low Cost]
    C --> F[Compliance<br/>Legal Requirements]

    D --> G[Privacy<br/>GDPR, CCPA]
    D --> H[Security<br/>Reduce Risk]

    E --> I[Future Reference<br/>if Needed]
    F --> I

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style E fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style F fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style G fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style H fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style I fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
</div>

---

#### ðŸ”‘ Key Considerations at Each Stage

<div class="important-info">
Each stage in this lifecycle has its own **data quality, governance, privacy, and security considerations**:

**Data Quality**:
- Accuracy, completeness, consistency, timeliness
- Validation and verification at each stage
- Error detection and correction

**Data Governance**:
- Policies and procedures for data management
- Roles and responsibilities
- Standards and best practices
- Compliance and auditing

**Privacy**:
- Personal data protection (GDPR, CCPA)
- Consent and transparency
- Data minimization
- Right to be forgotten

**Security**:
- Access controls and authentication
- Encryption (at rest and in transit)
- Backup and disaster recovery
- Threat detection and prevention
</div>

---

### ðŸ“š Real-World Example: RetailCo's Data Life Cycle

Let's consider the example of a retail organisation that relies heavily on customer data to drive its strategic decisions. For simplicity's sake, we'll call this organisation **'RetailCo'**. Here's how data moves through the data lifecycle at RetailCo:

<div class="mermaid">
graph TB
    subgraph "RetailCo Data Life Cycle"
        A[1. Collection<br/>ðŸ“¥<br/>E-commerce, POS,<br/>Loyalty Cards] --> B[2. Pre-processing<br/>ðŸ§¹<br/>Clean, Standardize,<br/>Remove Duplicates]
        B --> C[3. Storage<br/>ðŸ’¾<br/>Relational DB + NoSQL<br/>+ Data Lake]
        C --> D[4. Processing<br/>âš™ï¸<br/>Combine Transaction<br/>+ Feedback Data]
        D --> E[5. Analysis<br/>ðŸ“Š<br/>ML Forecasting,<br/>Behavior Trends]
        E --> F[6. Visualisation<br/>ðŸ“ˆ<br/>KPI Dashboards,<br/>Reports]
        F --> G[7. Action<br/>ðŸ’¡<br/>Marketing Campaigns,<br/>Website Redesign]
        G --> H[8. Archive/Destroy<br/>ðŸ—„ï¸<br/>Compliance,<br/>Data Retention]
    end

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

#### Detailed RetailCo Example by Stage

##### 1. Creation or Collection ðŸ“¥

RetailCo gathers information from various sources. For instance, **transactional data** may come from their e-commerce platform or point-of-sale systems in their physical stores. They might also gather information from **customer surveys** or **loyalty cards**. All of this unprocessed data is initially collected and stored for later processing.

**RetailCo's Data Sources**:
- E-commerce platform (online purchases, browsing behavior)
- Point-of-Sale (POS) systems (in-store transactions)
- Loyalty card programs (customer IDs, purchase history)
- Customer surveys (satisfaction ratings, feedback)
- Website analytics (clicks, page views, time on site)
- Mobile app usage data

##### 2. Pre-processing ðŸ§¹

The next step is to clean and transform this raw data. For RetailCo, this might entail **removing duplicate customer records**, **standardising data formats**, checking for **missing or incorrect data** (such as a customer's address or email), and ensuring that data is **consistent across all sources**.

**RetailCo's Pre-processing Tasks**:
- Remove duplicate customer records (same person, multiple entries)
- Standardize date formats across all systems
- Validate email addresses and phone numbers
- Fill missing customer addresses using postal code lookup
- Standardize product names and categories
- Convert currency values to consistent format
- Remove test transactions and invalid data

##### 3. Storage ðŸ’¾

After cleaning, the data is easily accessible and stored for later analysis. For more unstructured data, such as transactional data, RetailCo may use a combination of **relational databases** and **NoSQL databases** or **data lakes** (like customer feedback).

**RetailCo's Storage Strategy**:
- **Relational Database (PostgreSQL)**: Customer profiles, product catalog, transactions
- **NoSQL Database (MongoDB)**: Customer reviews, feedback, social media mentions
- **Data Lake (AWS S3)**: Raw logs, clickstream data, images
- **Data Warehouse (Snowflake)**: Aggregated data for analytics and reporting

##### 4. Processing âš™ï¸

The data is now prepared for processing and analysis. To obtain a comprehensive understanding of the customer, RetailCo may **combine data from various sources**. For instance, they might combine **transaction data with customer feedback** to better understand the relationship between customer satisfaction and spending.

**RetailCo's Processing Activities**:
- Join customer profiles with transaction history
- Combine online and offline purchase data
- Merge customer feedback with product purchases
- Calculate customer lifetime value (CLV)
- Create customer segments based on behavior
- Aggregate sales by product, category, region, time period
- Calculate key metrics: average order value, purchase frequency

##### 5. Analysis ðŸ“Š

After processing the data, insights are gleaned through analysis. RetailCo may employ **machine learning algorithms** to forecast future spending patterns or **statistical analysis** to comprehend trends in consumer behaviour.

**RetailCo's Analysis Techniques**:
- **Customer Segmentation**: Clustering customers by behavior (RFM analysis)
- **Churn Prediction**: ML models to identify at-risk customers
- **Sales Forecasting**: Time series analysis for demand prediction
- **Market Basket Analysis**: Identifying product associations
- **Sentiment Analysis**: NLP on customer reviews and feedback
- **A/B Testing**: Comparing different marketing approaches
- **Cohort Analysis**: Tracking customer groups over time

##### 6. Visualisation and Reporting ðŸ“ˆ

The analysis's findings are presented in a straightforward manner for decision-makers to comprehend. RetailCo may design **dashboards** that show key performance indicators (KPIs) or in-depth **reports** that analyse consumer behaviour in great detail.

**RetailCo's Visualizations**:
- **Executive Dashboard**: Sales, revenue, customer acquisition, churn rate
- **Product Performance**: Best sellers, slow movers, inventory levels
- **Customer Analytics**: Segmentation, lifetime value, satisfaction scores
- **Geographic Analysis**: Sales by region, store performance maps
- **Trend Reports**: Year-over-year comparisons, seasonal patterns
- **Campaign Performance**: Marketing ROI, conversion rates

##### 7. Action ðŸ’¡

RetailCo can then make strategic decisions based on these revelations. For instance, they might **redesign a portion of their e-commerce website** to enhance the user experience or **target a specific customer segment** with a marketing campaign.

**RetailCo's Data-Driven Actions**:
- **Targeted Marketing**: Email campaign to high-value customers at risk of churning
- **Website Redesign**: Improve checkout process based on abandonment analysis
- **Personalization**: Product recommendations based on purchase history
- **Inventory Optimization**: Stock levels adjusted based on demand forecasts
- **Pricing Strategy**: Dynamic pricing for slow-moving products
- **Customer Service**: Proactive outreach to dissatisfied customers
- **Store Layout**: Optimize product placement based on market basket analysis

##### 8. Archiving and Destruction ðŸ—„ï¸

Some of the gathered information will lose significance over time. This information will be **archived** by RetailCo, keeping it secure but out of the way. Old data that is no longer useful or that they are no longer required to keep for compliance reasons may also be **destroyed**.

**RetailCo's Data Retention**:
- **Active Data** (0-2 years): Readily accessible in production databases
- **Archived Data** (2-7 years): Moved to cold storage for compliance
- **Destroyed Data** (7+ years): Securely deleted after retention period
- **Compliance**: GDPR right to be forgotten, PCI DSS requirements
- **Audit Trails**: Maintain logs of data access and deletion

<div class="important-info">
**RetailCo's Compliance and Ethics**:

Throughout this lifecycle, RetailCo must comply with **data protection laws** and **ethical standards**, especially when handling sensitive customer data. To guarantee that data is dealt with properly at every stage of its lifecycle, they must also have strong **data governance structures** in place.

**Key Requirements**:
- **GDPR Compliance**: Customer consent, data minimization, right to access/deletion
- **PCI DSS**: Secure handling of payment card information
- **Data Governance**: Clear policies, roles, and responsibilities
- **Security**: Encryption, access controls, regular audits
- **Ethics**: Transparency, fairness, customer trust
</div>

---

### ðŸŒŸ Benefits of Employing a Data Life Cycle

There are several advantages to using a **data lifecycle approach** within an organisation. A systematic approach to managing data throughout its lifecycle can transform how organisations operate and compete.

<div class="mermaid">
graph TB
    A[Data Life Cycle<br/>Approach] --> B[Enhanced<br/>Decision-Making<br/>ðŸŽ¯]
    A --> C[Improved<br/>Data Quality<br/>âœ“]
    A --> D[Efficient<br/>Management<br/>âš™ï¸]
    A --> E[Regulatory<br/>Compliance<br/>ðŸ“‹]
    A --> F[Cost<br/>Savings<br/>ðŸ’°]
    A --> G[Security &<br/>Privacy<br/>ðŸ”’]
    A --> H[Maximise<br/>Value<br/>ðŸ“ˆ]

    B --> I[Better Business<br/>Outcomes]
    C --> I
    D --> I
    E --> I
    F --> I
    G --> I
    H --> I

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style I fill:#FFD700,stroke:#B8860B,stroke-width:3px
</div>

---

#### 1. Enhanced Decision-Making ðŸŽ¯

A structured data lifecycle makes sure that data is **gathered, saved, and analysed in an organised and systematic way**. This increases the data's dependability and, as a result, the insights that can be drawn from it, improving decision-making.

**How It Helps**:
- **Systematic Collection**: Ensures all relevant data is captured consistently
- **Reliable Storage**: Data is accessible when needed for analysis
- **Quality Analysis**: Clean, well-prepared data leads to accurate insights
- **Timely Insights**: Efficient processes enable faster decision-making
- **Confidence**: Decision-makers trust data-driven recommendations

**Examples**:
- Retail: Accurate sales forecasts based on clean historical data
- Healthcare: Reliable patient outcomes analysis for treatment decisions
- Finance: Risk assessment based on comprehensive, validated data
- Manufacturing: Production optimization using real-time sensor data

<div class="mermaid">
flowchart LR
    A[Structured<br/>Data Lifecycle] --> B[Organized<br/>Collection]
    B --> C[Systematic<br/>Storage]
    C --> D[Quality<br/>Analysis]
    D --> E[Reliable<br/>Insights]
    E --> F[Better<br/>Decisions<br/>ðŸŽ¯]

    F --> G[Competitive<br/>Advantage]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style G fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

---

#### 2. Improved Data Quality âœ“

Organisations can **maintain high data quality** throughout the process using a lifecycle approach. Systematic data cleaning, validation, and transformation steps ensure the **accuracy and dependability** of the data used for decision-making.

**Quality Dimensions**:
- **Accuracy**: Data correctly represents reality
- **Completeness**: All required data is present
- **Consistency**: Data is uniform across systems
- **Timeliness**: Data is up-to-date and available when needed
- **Validity**: Data conforms to defined formats and rules
- **Uniqueness**: No duplicate records

**Quality Assurance Activities**:
- Data validation at collection point
- Automated data cleaning processes
- Regular data quality audits
- Standardization and normalization
- Error detection and correction
- Data profiling and monitoring

**Impact**:
- Reduced errors in analysis and reporting
- Increased trust in data-driven insights
- Better customer experiences (accurate information)
- Fewer costly mistakes from bad data

<div class="mermaid">
graph TB
    A[Data Lifecycle<br/>Approach] --> B[Collection<br/>Validation]
    A --> C[Pre-processing<br/>Cleaning]
    A --> D[Storage<br/>Integrity]
    A --> E[Processing<br/>Verification]

    B --> F[High Quality<br/>Data<br/>âœ“]
    C --> F
    D --> F
    E --> F

    F --> G[Accurate<br/>Analysis]
    F --> H[Reliable<br/>Insights]
    F --> I[Trusted<br/>Decisions]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style G fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style H fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style I fill:#FFCDD2,stroke:#E57373,stroke-width:2px
</div>

---

#### 3. Efficient Data Management âš™ï¸

Data resources can be **managed effectively** by comprehending and adhering to a data lifecycle. It guarantees that data is appropriately stored, readily available, and archived, or properly disposed of when it is no longer required.

**Efficiency Gains**:
- **Optimized Storage**: Right data in the right place at the right time
- **Easy Access**: Quick retrieval of needed data
- **Reduced Redundancy**: Eliminate duplicate data storage
- **Automated Processes**: Streamlined workflows reduce manual effort
- **Clear Ownership**: Defined roles and responsibilities
- **Better Organization**: Structured data catalogs and metadata

**Resource Optimization**:
- Active data in fast, accessible storage
- Archived data in cost-effective cold storage
- Obsolete data properly deleted
- Storage costs aligned with data value
- Efficient backup and recovery processes

**Productivity Benefits**:
- Less time searching for data
- Faster data preparation for analysis
- Reduced IT overhead
- More time for value-added activities

<div class="mermaid">
flowchart TD
    A[Data Lifecycle<br/>Management] --> B{Data Age &<br/>Usage?}

    B -->|Active,<br/>Frequent| C[Hot Storage<br/>ðŸ’¾<br/>Fast Access]
    B -->|Occasional<br/>Access| D[Warm Storage<br/>ðŸ“¦<br/>Balanced]
    B -->|Rare,<br/>Compliance| E[Cold Storage<br/>ðŸ—„ï¸<br/>Low Cost]
    B -->|Obsolete| F[Delete<br/>ðŸ—‘ï¸<br/>Free Space]

    C --> G[Efficient<br/>Management<br/>âš™ï¸]
    D --> G
    E --> G
    F --> G

    G --> H[Cost Savings<br/>+ Performance]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#757575,stroke:#424242,stroke-width:2px,color:#fff
    style G fill:#50C878,stroke:#2E7D32,stroke-width:3px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

---

#### 4. Regulatory Compliance ðŸ“‹

Organisations can better comply with laws like the **GDPR**, **HIPAA**, or the **POPI Act** by incorporating data security and privacy considerations into the data lifecycle. This can lessen the possibility of legal problems, penalties, and reputational harm.

**Key Regulations**:
- **GDPR** (General Data Protection Regulation): EU data protection
- **CCPA** (California Consumer Privacy Act): California privacy rights
- **HIPAA** (Health Insurance Portability and Accountability Act): Healthcare data
- **POPI Act** (Protection of Personal Information Act): South African data protection
- **PCI DSS** (Payment Card Industry Data Security Standard): Payment data
- **SOX** (Sarbanes-Oxley Act): Financial reporting

**Compliance Benefits**:
- **Audit Trails**: Complete record of data handling
- **Data Lineage**: Track data from source to destination
- **Access Controls**: Who can access what data and when
- **Retention Policies**: Automated compliance with retention rules
- **Right to Erasure**: Ability to delete personal data on request
- **Consent Management**: Track and honor user consent

**Risk Mitigation**:
- Avoid hefty fines (GDPR: up to â‚¬20M or 4% of revenue)
- Prevent legal issues and lawsuits
- Protect brand reputation
- Maintain customer trust
- Ensure business continuity

<div class="mermaid">
graph TB
    A[Data Lifecycle<br/>with Compliance] --> B[GDPR<br/>ðŸ‡ªðŸ‡º<br/>Privacy Rights]
    A --> C[HIPAA<br/>ðŸ¥<br/>Healthcare Data]
    A --> D[PCI DSS<br/>ðŸ’³<br/>Payment Security]
    A --> E[POPI Act<br/>ðŸ‡¿ðŸ‡¦<br/>Data Protection]

    B --> F[Compliance<br/>Benefits]
    C --> F
    D --> F
    E --> F

    F --> G[Avoid Fines<br/>ðŸ’°]
    F --> H[Legal Protection<br/>âš–ï¸]
    F --> I[Trust & Reputation<br/>ðŸŒŸ]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style F fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style I fill:#FFC107,stroke:#F57C00,stroke-width:2px
</div>

---

#### 5. Cost Savings ðŸ’°

Cost savings can result from **effective data management** throughout its lifecycle. One way to reduce storage costs is to archive or delete old data no longer needed. Similarly, having high-quality, well-managed data can result in efficiency gains by cutting down on the time required to fix errors or handle data-related issues.

**Cost Reduction Areas**:

**Storage Costs**:
- Archive inactive data to cheaper cold storage
- Delete obsolete data to free up space
- Optimize storage tiers based on access patterns
- Reduce redundant data copies
- Compress and deduplicate data

**Operational Costs**:
- Less time fixing data quality issues
- Reduced manual data preparation effort
- Fewer errors requiring correction
- Automated processes reduce labor costs
- Faster time-to-insight

**Infrastructure Costs**:
- Right-sized storage and compute resources
- Cloud cost optimization (pay for what you use)
- Reduced backup and disaster recovery costs
- Lower network bandwidth usage

**Opportunity Costs**:
- Data scientists spend more time on analysis, less on data wrangling
- Faster decision-making leads to competitive advantages
- Better resource allocation based on insights
- Reduced risk of costly mistakes

**ROI Examples**:
- Retail: 30% reduction in storage costs through archiving
- Healthcare: 40% less time on data preparation
- Finance: Avoid $1M+ fines through compliance
- Manufacturing: 20% efficiency gains from quality data

<div class="mermaid">
graph TB
    A[Data Lifecycle<br/>Cost Savings] --> B[Storage<br/>Optimization<br/>ðŸ’¾]
    A --> C[Operational<br/>Efficiency<br/>âš™ï¸]
    A --> D[Quality<br/>Improvements<br/>âœ“]
    A --> E[Compliance<br/>Avoidance<br/>ðŸ“‹]

    B --> B1[Archive old data<br/>Delete obsolete<br/>Optimize tiers]
    C --> C1[Automation<br/>Less manual work<br/>Faster processes]
    D --> D1[Fewer errors<br/>Less rework<br/>Better decisions]
    E --> E1[Avoid fines<br/>Prevent breaches<br/>Reduce risk]

    B1 --> F[Total Cost<br/>Savings<br/>ðŸ’°]
    C1 --> F
    D1 --> F
    E1 --> F

    F --> G[Improved ROI<br/>ðŸ“ˆ]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B1 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style C1 fill:#FFE0B2,stroke:#FB8C00,stroke-width:2px
    style D1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style E1 fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style F fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style G fill:#FFD700,stroke:#B8860B,stroke-width:2px
</div>

---

#### 6. Security and Privacy ðŸ”’

Organisations can better **protect sensitive data** and **lower the risk of data breaches** by implementing data security measures at every stage of the data lifecycle. Similarly, maintaining customer trust can be facilitated by adhering to privacy principles throughout the data lifecycle.

**Security Measures by Stage**:

**Collection**:
- Secure data transmission (HTTPS, TLS)
- Authentication and authorization
- Input validation to prevent injection attacks
- Encrypted data capture

**Storage**:
- Encryption at rest (AES-256)
- Access controls and role-based permissions
- Database security hardening
- Regular security audits

**Processing & Analysis**:
- Secure computing environments
- Data masking and anonymization
- Audit logging of data access
- Principle of least privilege

**Archiving & Destruction**:
- Secure archival with encryption
- Certified data destruction methods
- Audit trails of deletion
- Compliance with retention policies

**Privacy Principles**:
- **Data Minimization**: Collect only what's needed
- **Purpose Limitation**: Use data only for stated purposes
- **Transparency**: Clear communication about data use
- **Consent**: Obtain and honor user consent
- **Right to Access**: Allow users to view their data
- **Right to Erasure**: Enable data deletion on request

**Benefits**:
- Reduced risk of data breaches
- Protection of sensitive customer information
- Compliance with privacy regulations
- Enhanced customer trust and loyalty
- Competitive advantage through privacy leadership

<div class="mermaid">
flowchart TB
    A[Data Lifecycle<br/>Security & Privacy] --> B[Collection<br/>ðŸ”<br/>Secure Transmission]
    A --> C[Storage<br/>ðŸ’¾<br/>Encryption at Rest]
    A --> D[Processing<br/>âš™ï¸<br/>Access Controls]
    A --> E[Destruction<br/>ðŸ—‘ï¸<br/>Secure Deletion]

    B --> F[Security<br/>Measures]
    C --> F
    D --> F
    E --> F

    F --> G[Privacy<br/>Principles]

    G --> H[Data Protection<br/>ðŸ›¡ï¸]
    G --> I[Customer Trust<br/>ðŸ¤]
    G --> J[Compliance<br/>âœ“]

    H --> K[Reduced Risk<br/>+ Reputation]
    I --> K
    J --> K

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style F fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style G fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style H fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style I fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style J fill:#FFCDD2,stroke:#E57373,stroke-width:2px
    style K fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

#### 7. Maximising Value from Data ðŸ“ˆ

Making the most of their data is something organisations can do by **understanding the data lifecycle**. For instance, it can assist in locating opportunities for **data reuse** since information gathered for one purpose may also be valuable for other purposes within the organisation.

**Value Maximization Strategies**:

**Data Reuse**:
- Customer data collected for sales can inform marketing
- Transaction data used for both accounting and analytics
- Sensor data for both monitoring and predictive maintenance
- Feedback data for product development and customer service

**Cross-Functional Insights**:
- Sales data informs inventory management
- Customer service data improves product design
- Marketing data optimizes pricing strategies
- HR data enhances workforce planning

**Advanced Analytics**:
- Machine learning models for prediction
- AI-powered personalization
- Real-time decision-making
- Automated optimization

**Data Monetization**:
- Internal value: Better decisions, efficiency, innovation
- External value: Data products, insights-as-a-service
- Ecosystem value: Partner data sharing, industry benchmarks

**Innovation Opportunities**:
- Discover new customer segments
- Identify untapped markets
- Develop new products and services
- Create competitive differentiation

**Examples**:
- **Amazon**: Uses purchase data for recommendations, inventory, pricing, and new product development
- **Netflix**: Viewing data drives content recommendations, production decisions, and personalization
- **Uber**: Trip data optimizes routing, pricing, driver allocation, and city planning partnerships
- **Healthcare**: Patient data improves treatments, drug development, and population health management

<div class="mermaid">
graph TB
    A[Single Data<br/>Source] --> B[Primary<br/>Purpose<br/>ðŸ“Š]
    A --> C[Secondary<br/>Uses<br/>ðŸ”„]
    A --> D[Advanced<br/>Analytics<br/>ðŸ¤–]
    A --> E[Innovation<br/>ðŸ’¡]

    B --> F[Initial Value]
    C --> G[Reuse Value]
    D --> H[Predictive Value]
    E --> I[Strategic Value]

    F --> J[Maximised<br/>Data Value<br/>ðŸ“ˆ]
    G --> J
    H --> J
    I --> J

    J --> K[Competitive<br/>Advantage<br/>ðŸ†]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style G fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px
    style H fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style I fill:#FFE0B2,stroke:#FB8C00,stroke-width:2px
    style J fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style K fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
</div>

---

### ðŸŽ¯ Summary: The Power of Data Life Cycle Management

<div class="key-concept">
**The use of a data life cycle approach, in conclusion, enables organisations to systematically manage their data from creation and collection through to archiving or disposal.**

This systematic approach results in:

âœ… **Wiser Choices**: Data-driven decisions based on reliable, high-quality information

âœ… **Greater Effectiveness**: Streamlined processes, reduced waste, optimized resources

âœ… **Greater Adherence to Rules**: Compliance with GDPR, HIPAA, POPI Act, and other regulations

âœ… **Cost Savings**: Reduced storage, operational, and compliance costs

âœ… **Enhanced Security**: Protection of sensitive data at every stage

âœ… **Maximized Value**: Reuse data for multiple purposes, drive innovation

âœ… **Competitive Advantage**: Better insights, faster decisions, superior customer experiences
</div>

<div class="mermaid">
graph TB
    A[Data Life Cycle<br/>Management] --> B[7 Key Benefits]

    B --> C1[Enhanced<br/>Decision-Making<br/>ðŸŽ¯]
    B --> C2[Improved<br/>Data Quality<br/>âœ“]
    B --> C3[Efficient<br/>Management<br/>âš™ï¸]
    B --> C4[Regulatory<br/>Compliance<br/>ðŸ“‹]
    B --> C5[Cost<br/>Savings<br/>ðŸ’°]
    B --> C6[Security &<br/>Privacy<br/>ðŸ”’]
    B --> C7[Maximise<br/>Value<br/>ðŸ“ˆ]

    C1 --> D[Business<br/>Success<br/>ðŸ†]
    C2 --> D
    C3 --> D
    C4 --> D
    C5 --> D
    C6 --> D
    C7 --> D

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C2 fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style C3 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C4 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style C5 fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C6 fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style C7 fill:#FFC107,stroke:#F57C00,stroke-width:2px
    style D fill:#E74C3C,stroke:#C0392B,stroke-width:4px,color:#fff
</div>

---

### âš–ï¸ Relational vs Big Data Databases

Based on the kind and volume of data they're intended to handle, **traditional relational databases** and **big data databases** (also frequently referred to as **NoSQL databases**) serve different purposes, and each has its strengths. The structure, data consistency, scale, and query language of these two types of data storage can be contrasted.

<div class="mermaid">
graph TB
    A[Database Types] --> B[Relational Databases<br/>ðŸ—„ï¸<br/>Traditional SQL]
    A --> C[Big Data Databases<br/>ðŸŒ<br/>NoSQL]

    B --> B1[MySQL<br/>PostgreSQL<br/>Oracle]
    C --> C1[MongoDB<br/>Cassandra<br/>Hadoop]

    B --> B2[Structured Data<br/>ACID Properties]
    C --> C2[Unstructured/Semi-structured<br/>BASE Properties]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style B2 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C2 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
</div>

#### ðŸ“Š Detailed Comparison

##### 1. Structure ðŸ—ï¸

**Relational Databases**
- The **relational model**, which arranges data into **tables with rows and columns**, is the foundation
- Examples: MySQL, Oracle, PostgreSQL
- **Keys are used to define relationships** between tables
- Rigid schema that must be defined upfront
- Data normalization to reduce redundancy

**Big Data (NoSQL) Databases**
- Made to **store, process, and analyse data** that is too large or complex for conventional databases
- Examples: MongoDB, Cassandra, Hadoop
- Can effectively manage **unstructured or partially structured data**
- Flexible schema that can evolve over time
- Various data models: document, key-value, column-family, graph

<div class="mermaid">
graph LR
    subgraph "Relational Database Structure"
        A1[Tables] --> A2[Rows & Columns]
        A2 --> A3[Primary Keys]
        A2 --> A4[Foreign Keys]
        A3 --> A5[Relationships]
        A4 --> A5
    end

    subgraph "Big Data Database Structure"
        B1[Flexible Schema] --> B2[Documents/Collections]
        B1 --> B3[Key-Value Pairs]
        B1 --> B4[Column Families]
        B1 --> B5[Graph Nodes]
    end

    style A1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style A5 fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style B1 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B2 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style B3 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style B4 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style B5 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
</div>

##### 2. Data Consistency ðŸ”’

**Relational Databases - ACID Properties**

Based on the **ACID** (Atomicity, Consistency, Isolation, Durability) properties, they offer **robust consistency guarantees**.

- **Atomicity**: Transactions are all-or-nothing
- **Consistency**: Data remains in a valid state
- **Isolation**: Concurrent transactions don't interfere
- **Durability**: Committed data is permanently saved

**Big Data Databases - BASE Properties**

Instead of using the ACID model, they are typically based on the **BASE** (Basically Available, Soft state, Eventually consistent) model, allowing for **better performance and scalability** at the expense of consistency.

- **Basically Available**: System guarantees availability
- **Soft state**: State may change over time, even without input
- **Eventually consistent**: System will become consistent over time

<div class="mermaid">
graph TB
    subgraph "ACID - Relational Databases"
        A1[Atomicity<br/>All or Nothing] --> A5[Strong Consistency<br/>âœ“]
        A2[Consistency<br/>Valid State] --> A5
        A3[Isolation<br/>No Interference] --> A5
        A4[Durability<br/>Permanent Storage] --> A5
    end

    subgraph "BASE - Big Data Databases"
        B1[Basically Available<br/>Always Accessible] --> B4[Eventual Consistency<br/>â±ï¸]
        B2[Soft State<br/>May Change] --> B4
        B3[Eventually Consistent<br/>Over Time] --> B4
    end

    A5 --> Trade[Trade-offs]
    B4 --> Trade

    Trade --> T1[ACID: Consistency<br/>vs Performance]
    Trade --> T2[BASE: Performance<br/>vs Consistency]

    style A5 fill:#50C878,stroke:#2E7D32,stroke-width:3px,color:#fff
    style B4 fill:#FF9800,stroke:#E65100,stroke-width:3px,color:#fff
    style Trade fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style T1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style T2 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
</div>

##### 3. Scale ðŸ“ˆ

**Relational Databases**
- Frequently employed in **business applications and transactions** where the volume of data is not excessive
- Great at handling **structured data**
- May find it difficult to **scale horizontally** (across many servers) when the amount of data is considerable
- Typically scale **vertically** (more powerful single server)

**Big Data Databases**
- Made to **scale across numerous servers horizontally**
- Can **process and analyse data on a much larger scale** than traditional databases
- Designed to handle **large volumes of data** (terabytes to petabytes)
- Distributed architecture for parallel processing

<div class="mermaid">
flowchart TD
    subgraph "Relational Database Scaling"
        R1[Single Server] --> R2[Vertical Scaling<br/>â¬†ï¸<br/>More CPU/RAM/Storage]
        R2 --> R3[Limited by<br/>Hardware Capacity]
    end

    subgraph "Big Data Database Scaling"
        B1[Multiple Servers] --> B2[Horizontal Scaling<br/>âž¡ï¸<br/>Add More Servers]
        B2 --> B3[Distributed Processing]
        B3 --> B4[Virtually Unlimited<br/>Scalability]
    end

    R3 -.Limitation.-> L1[Scaling Challenges]
    B4 -.Advantage.-> L2[Massive Scale]

    style R1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style R2 fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style R3 fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B1 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B2 fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B3 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style B4 fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
</div>

##### 4. Query Language ðŸ’¬

**Relational Databases - SQL**
- Define and manipulate the data using **Structured Query Language (SQL)**
- SQL is **effective but complicated** for more complex queries
- Standardized language across different databases
- Powerful for joins, aggregations, and complex queries
- Declarative approach (specify what, not how)

**Big Data Databases - Varied Approaches**
- **Hive for Hadoop** is an example of a NoSQL database that uses a form of SQL
- Other NoSQL databases use their **own query languages or APIs**
- Can be **easier to use** and provide **more flexibility** when working with unstructured data
- Examples: MongoDB Query Language (MQL), Cassandra Query Language (CQL)

<div class="mermaid">
graph TB
    subgraph "Relational - SQL"
        S1[SQL Language] --> S2[SELECT, INSERT<br/>UPDATE, DELETE]
        S2 --> S3[JOIN Operations]
        S2 --> S4[Complex Queries]
        S3 --> S5[Standardized<br/>Across Databases]
        S4 --> S5
    end

    subgraph "Big Data - Multiple Languages"
        N1[Query Options] --> N2[SQL-like<br/>Hive, CQL]
        N1 --> N3[Native APIs<br/>MongoDB MQL]
        N1 --> N4[MapReduce<br/>Hadoop]
        N1 --> N5[Graph Queries<br/>Cypher, Gremlin]
    end

    S5 --> Result[Query Execution]
    N2 --> Result
    N3 --> Result
    N4 --> Result
    N5 --> Result

    style S1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style S5 fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style N1 fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style Result fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
</div>

#### ðŸ“‹ Comprehensive Comparison Table

| Aspect | Relational Databases | Big Data (NoSQL) Databases |
|--------|---------------------|---------------------------|
| **Structure** | Relational model with tables, rows, and columns. Keys define relationships between tables. | Flexible schema designed for large/complex data. Can handle unstructured or semi-structured data. |
| **Examples** | MySQL, Oracle, PostgreSQL, SQL Server | MongoDB, Cassandra, Hadoop, Redis, Neo4j |
| **Data Consistency** | ACID properties (Atomicity, Consistency, Isolation, Durability) - robust consistency guarantees | BASE model (Basically Available, Soft state, Eventually consistent) - better performance and scalability |
| **Scale** | Vertical scaling (more powerful server). Great for structured data but difficult to scale horizontally. | Horizontal scaling (add more servers). Designed to handle massive volumes across distributed systems. |
| **Query Language** | SQL (Structured Query Language) - standardized, powerful but complex for advanced queries | Varied: SQL-like (Hive, CQL), native APIs (MQL), MapReduce, or graph queries. More flexible for unstructured data. |
| **Best For** | Business applications, transactions, structured data, complex relationships | Big data analytics, unstructured data, high scalability, real-time processing |
| **Data Volume** | Small to medium datasets | Large to massive datasets (terabytes to petabytes) |
| **Schema** | Rigid, predefined schema | Flexible, evolving schema |
| **Performance** | Optimized for complex queries and transactions | Optimized for high throughput and availability |
| **Use Cases** | Banking, ERP, CRM, e-commerce transactions | Social media analytics, IoT data, real-time recommendations, log analysis |

#### ðŸŽ¯ When to Use Which?

<div class="mermaid">
flowchart TD
    Start{What type of<br/>data do you have?} --> Structured{Structured &<br/>Relational?}
    Start --> Volume{Large Volume<br/>& Variety?}

    Structured -->|Yes| Consistency{Need Strong<br/>Consistency?}
    Structured -->|No| Volume

    Consistency -->|Yes| Relational[Use Relational<br/>Database<br/>ðŸ—„ï¸<br/>MySQL, PostgreSQL]
    Consistency -->|No| Volume

    Volume -->|Yes| Scale{Need Horizontal<br/>Scaling?}
    Volume -->|No| Relational

    Scale -->|Yes| BigData[Use Big Data<br/>Database<br/>ðŸŒ<br/>MongoDB, Cassandra]
    Scale -->|No| Relational

    Start --> Unstructured{Unstructured<br/>Data?}
    Unstructured -->|Yes| BigData
    Unstructured -->|No| Structured

    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style Relational fill:#50C878,stroke:#2E7D32,stroke-width:3px,color:#fff
    style BigData fill:#FF9800,stroke:#E65100,stroke-width:3px,color:#fff
    style Structured fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Volume fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Consistency fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Scale fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Unstructured fill:#FFD700,stroke:#B8860B,stroke-width:2px
</div>

#### ðŸ’¡ Key Decision Factors

<div class="key-concept">
**Choose Relational Databases when:**
- You have structured, relational data
- You need ACID compliance and strong consistency
- Your data volume is manageable (not massive scale)
- You require complex joins and transactions
- Schema is well-defined and stable

**Choose Big Data (NoSQL) Databases when:**
- You have unstructured or semi-structured data
- You need to scale horizontally across many servers
- You're dealing with massive data volumes (big data)
- You can accept eventual consistency
- Schema needs to be flexible and evolve
- You need high availability and performance
</div>

#### ðŸ”„ Hybrid Approaches

<div class="important-info">
**Modern Data Architecture**: Many organizations use a **hybrid approach**, combining both relational and big data databases:

- **Relational databases** for transactional data, user accounts, financial records
- **Big data databases** for analytics, logs, user behavior, real-time data
- **Data lakes** to store raw data from both sources
- **ETL pipelines** to move data between systems as needed

This approach leverages the strengths of each technology to build robust, scalable data systems.
</div>

<div class="mermaid">
graph TB
    subgraph "Hybrid Data Architecture"
        A[Data Sources] --> B[Relational DB<br/>Transactions]
        A --> C[NoSQL DB<br/>Analytics]
        A --> D[Data Lake<br/>Raw Storage]

        B --> E[ETL Pipeline]
        C --> E
        D --> E

        E --> F[Data Warehouse]
        F --> G[Business Intelligence<br/>& Analytics]

        G --> H[Decision Making<br/>ðŸ’¡]
    end

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style F fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style G fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

### ðŸ“š What did I Learn in This Lesson?

This lesson provided comprehensive insights into the fundamentals of data and its management. Let's recap the key learnings:

<div class="mermaid">
graph TB
    A[Lesson 1.2:<br/>It's All About Data<br/>ðŸ“Š] --> B[Data Types &<br/>Representation<br/>ðŸ”¢]
    A --> C[Data Sources &<br/>Storage<br/>ðŸ’¾]
    A --> D[Database<br/>Technologies<br/>ðŸ—„ï¸]
    A --> E[Data Life Cycle<br/>Management<br/>ðŸ”„]

    B --> B1[Numeric, String,<br/>Boolean, Date/Time]
    B --> B2[Structured vs<br/>Unstructured]

    C --> C1[Internal & External<br/>Sources]
    C --> C2[Multiple Storage<br/>Options]

    D --> D1[Relational<br/>Databases]
    D --> D2[Big Data/<br/>NoSQL]

    E --> E1[8 Lifecycle<br/>Stages]
    E --> E2[7 Key<br/>Benefits]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style B1 fill:#50C878,stroke:#2E7D32,stroke-width:1px,color:#fff
    style B2 fill:#50C878,stroke:#2E7D32,stroke-width:1px,color:#fff
    style C1 fill:#FF9800,stroke:#E65100,stroke-width:1px,color:#fff
    style C2 fill:#FF9800,stroke:#E65100,stroke-width:1px,color:#fff
    style D1 fill:#9B59B6,stroke:#6C3483,stroke-width:1px,color:#fff
    style D2 fill:#9B59B6,stroke:#6C3483,stroke-width:1px,color:#fff
    style E1 fill:#E74C3C,stroke:#C0392B,stroke-width:1px,color:#fff
    style E2 fill:#E74C3C,stroke:#C0392B,stroke-width:1px,color:#fff
</div>

#### ðŸŽ¯ Core Learnings Summary

<div class="key-concept">
**1. Different Types of Data**

You learned about the fundamental data types used in data storage and analysis:

- **Numeric Data**: Integer (whole numbers), Float (decimals), Complex (real + imaginary)
- **String/Character Data**: Text data, from single characters to long sequences
- **Boolean Data**: True/False values for flags and conditions
- **Date/Time Data**: Dates, times, datetimes, and time spans for temporal information

Each data type has specific use cases, storage characteristics, and performance implications. Choosing the right data type is crucial for storage efficiency, accuracy, and proper analysis.
</div>

<div class="key-concept">
**2. Data Sources in the Modern World**

Data can be sourced from numerous locations:

**Internal Sources:**
- Transactional systems (sales, invoices, orders)
- Log data (web logs, application logs, error logs)
- CRM systems (customer information and interactions)
- HR systems (employee data and performance)

**External Sources:**
- Social media platforms (user-generated content, sentiment)
- Public datasets (government, research, open data)
- Web scraping (extracting data from websites)
- Third-party data providers (market research, demographics)
- IoT devices and sensors (real-time environmental data)
- Satellite and imagery data (geospatial information)

Understanding where data comes from helps you assess its quality, reliability, and appropriate use.
</div>

<div class="key-concept">
**3. Variety of Storage Options for Data**

You explored multiple ways to store data, each with different characteristics:

| Storage Type | Best For | Key Characteristics |
|--------------|----------|---------------------|
| **Spreadsheets/CSV** | Small datasets, human readability | Portable, simple, limited scalability |
| **Relational Databases** | Structured data, transactions | ACID properties, SQL queries, vertical scaling |
| **NoSQL Databases** | Unstructured data, flexibility | Horizontal scaling, flexible schema, BASE model |
| **Data Warehouses** | Business intelligence, reporting | Optimized for analytics, historical data |
| **Data Lakes** | Raw data storage, big data | Store any format, schema-on-read |
| **Cloud Object Storage** | Scalable storage, backups | S3, Azure Blob, cost-effective |
| **In-Memory Stores** | High-speed access, caching | Redis, Memcached, fast but volatile |

The choice of storage depends on data volume, access patterns, query requirements, and scalability needs.
</div>

<div class="key-concept">
**4. Data Representation Methods**

Data can be represented in different ways:

**Structured Data:**
- Organized in tables with rows (records) and columns (attributes)
- Relational databases, CSV files, data frames
- Easy to query, analyze, and process
- Fixed schema provides consistency

**Unstructured Data:**
- Free-form format without predefined structure
- Text documents, images, videos, audio, sensor data
- Rich in information but more challenging to analyze
- Requires specialized techniques (NLP, computer vision)

**Hybrid Approaches:**
- Combining structured and unstructured data provides comprehensive insights
- Example: Customer demographics (structured) + customer reviews (unstructured)
</div>

<div class="key-concept">
**5. Relational vs Big Data Storage**

You learned the fundamental differences between traditional and modern database approaches:

**Relational Databases (MySQL, PostgreSQL, Oracle):**
- âœ… Structured data with fixed schema
- âœ… ACID properties for strong consistency
- âœ… Complex joins and relationships
- âœ… SQL for powerful querying
- âš ï¸ Vertical scaling (limited by single server)
- âš ï¸ Difficult to handle unstructured data

**Big Data/NoSQL Databases (MongoDB, Cassandra, Hadoop):**
- âœ… Flexible schema for varied data
- âœ… Horizontal scaling (add more servers)
- âœ… Handle massive volumes (terabytes to petabytes)
- âœ… BASE model for better performance
- âš ï¸ Eventual consistency (not immediate)
- âš ï¸ More complex query patterns

**Modern Approach:** Many organizations use **hybrid architectures** combining both:
- Relational databases for transactions and structured data
- NoSQL databases for analytics and unstructured data
- Data lakes for raw data storage
- ETL pipelines to move data between systems
</div>

<div class="key-concept">
**6. Data Types for Individual Entities**

Within data storage systems, individual data entities are stored using specific data types:

- **Integer**: Counting, IDs, indexing (e.g., customer_id, quantity)
- **Float**: Measurements, prices, scientific calculations (e.g., price, temperature)
- **String**: Names, addresses, descriptions (e.g., customer_name, email)
- **Boolean**: Flags, status indicators (e.g., is_active, has_subscription)
- **Date/Time**: Timestamps, scheduling (e.g., order_date, created_at)

Proper data type selection ensures:
- âœ… Storage efficiency (right amount of space)
- âœ… Performance optimization (faster queries)
- âœ… Data accuracy (appropriate precision)
- âœ… Validation (prevent invalid data)
</div>

<div class="key-concept">
**7. Data Life Cycle Management Plan**

Organizations need a systematic plan for handling data throughout its lifecycle:

**The 8 Stages:**

1. **Collection** ðŸ“¥: Gather data from various sources (IoT, transactions, APIs)
2. **Pre-processing** ðŸ§¹: Clean, validate, transform, and prepare data
3. **Storage** ðŸ’¾: Store in appropriate databases or data lakes
4. **Processing** âš™ï¸: Aggregate, integrate, and engineer features
5. **Analysis** ðŸ“Š: Apply statistical methods, ML, and exploratory techniques
6. **Visualisation** ðŸ“ˆ: Create charts, dashboards, and reports
7. **Action** ðŸŽ¯: Make decisions and implement changes based on insights
8. **Archiving/Destruction** ðŸ—„ï¸: Archive for compliance or securely delete obsolete data

**The 7 Benefits of Lifecycle Management:**

1. **Enhanced Decision-Making** ðŸŽ¯: Reliable data leads to better insights
2. **Improved Data Quality** âœ“: Systematic processes ensure accuracy and completeness
3. **Efficient Management** âš™ï¸: Optimized storage and automated processes
4. **Regulatory Compliance** ðŸ“‹: Meet GDPR, HIPAA, PCI DSS requirements
5. **Cost Savings** ðŸ’°: Reduce storage, operational, and infrastructure costs
6. **Security & Privacy** ðŸ”’: Protection at every stage builds trust
7. **Maximizing Value** ðŸ“ˆ: Data reuse, innovation, and competitive advantage

**Key Principle:** Data lifecycle management is not a one-time activity but an ongoing, iterative process that requires governance, quality controls, and compliance awareness at every stage.
</div>

#### ðŸŽ“ Putting It All Together

<div class="important-info">
**The Complete Picture:**

This lesson has equipped you with foundational knowledge about data - the lifeblood of modern organizations:

1. **Understanding Data Types** helps you choose appropriate storage and analysis methods
2. **Knowing Data Sources** enables you to gather comprehensive information for analysis
3. **Selecting Storage Options** ensures your data is accessible, scalable, and performant
4. **Choosing Representation Methods** (structured vs unstructured) matches data to use cases
5. **Comparing Database Technologies** (relational vs big data) guides architectural decisions
6. **Applying Data Types Correctly** prevents errors and optimizes performance
7. **Implementing Lifecycle Management** creates systematic, compliant, and valuable data practices

**Real-World Application:**

Consider a modern e-commerce company:
- **Data Types**: Customer IDs (integer), prices (float), names (string), active status (boolean), order dates (datetime)
- **Data Sources**: Website transactions (internal), social media sentiment (external), IoT sensors in warehouses
- **Storage**: Relational DB for orders, NoSQL for product catalog, data lake for clickstream data
- **Representation**: Structured customer data, unstructured product reviews
- **Databases**: MySQL for transactions (ACID), MongoDB for product catalog (flexibility), Hadoop for analytics
- **Lifecycle**: Collect orders â†’ Clean duplicates â†’ Store in DB â†’ Process for insights â†’ Analyze trends â†’ Visualize KPIs â†’ Optimize inventory â†’ Archive old orders

This comprehensive approach to data management enables organizations to make informed decisions, comply with regulations, reduce costs, and gain competitive advantages in the data-driven economy.
</div>

<div class="mermaid">
graph LR
    A[Raw Data<br/>ðŸ“¥] --> B[Proper Types &<br/>Storage<br/>ðŸ’¾]
    B --> C[Appropriate<br/>Database<br/>ðŸ—„ï¸]
    C --> D[Lifecycle<br/>Management<br/>ðŸ”„]
    D --> E[Quality<br/>Insights<br/>ðŸ’¡]
    E --> F[Better<br/>Decisions<br/>ðŸŽ¯]
    F --> G[Business<br/>Success<br/>ðŸš€]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style F fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ’¡ Key Takeaways for Exam Preparation

1. âœ… **Data types** (Integer, Float, String, Boolean, Date/Time) have specific use cases and storage characteristics
2. âœ… **Data sources** can be internal (transactional, CRM, HR) or external (social media, public datasets, IoT)
3. âœ… **Storage options** range from spreadsheets to data lakes, each with different scalability and performance
4. âœ… **Structured data** is organized in tables; **unstructured data** is free-form (text, images, videos)
5. âœ… **Relational databases** use ACID properties and SQL; **NoSQL databases** use BASE model and horizontal scaling
6. âœ… **Data lifecycle** has 8 stages from collection to archiving/destruction
7. âœ… **Lifecycle benefits** include better decisions, quality, efficiency, compliance, cost savings, security, and value
8. âœ… **Hybrid approaches** combine relational and NoSQL databases for optimal performance
9. âœ… **Data governance** and **compliance** (GDPR, HIPAA, PCI DSS) are essential throughout the lifecycle
10. âœ… **Proper data type selection** prevents errors and optimizes storage, performance, and accuracy

**Remember:** Data is only valuable when it's properly collected, stored, managed, and analyzed. The systematic approach you've learned in this lesson forms the foundation for all data analysis work you'll do in your career.
</div>

---

<svg width="680" height="82" viewBox="0 0 680 82" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Lesson task banner">
    <rect x="1" y="1" width="678" height="80" rx="10" fill="#FFF4E8" stroke="#E58E26" stroke-width="2"/>
    <text x="20" y="34" fill="#8A4A00" font-size="18" font-weight="700">Lesson Task: Internal vs External Data Sources</text>
    <text x="20" y="56" fill="#8A4A00" font-size="13">Classify data sources and design clear feature tables with types and storage.</text>
</svg>

### The Task: Internal vs External Data Sources

In this lesson, you learned about various considerations related to data, including **internal and external sources**. Let's complete a practical exercise to solidify your understanding.

#### Task Requirements

**Select:**
- **2 types of internal data**
- **2 types of external data**

**For each data type, create a table listing:**
1. **Feature name**: The specific attribute or field
2. **Description**: What this feature represents (helpful for future users)
3. **Data type**: Numeric, String, Boolean, Date/Time, Audio, Video, etc.
4. **Storage**: Structured or Unstructured

---

### âœ… Complete Solution with Examples

#### Internal Data Source 1: Human Resources (HR) Data

<div class="key-concept">
**About HR Data:**
Human Resources data contains information about employees within an organization. This is critical internal data used for payroll, performance management, compliance, and workforce planning.
</div>

| Feature Name | Description | Data Type | Storage |
|--------------|-------------|-----------|---------|
| `employee_id` | Unique identifier for each staff member | Integer | Structured |
| `staff_surname` | Employee's last name/family name | String | Structured |
| `staff_firstname` | Employee's first name/given name | String | Structured |
| `email_address` | Corporate email address | String | Structured |
| `phone_number` | Contact phone number | String | Structured |
| `date_of_birth` | Employee's birth date | Date | Structured |
| `hire_date` | Date employee joined the company | Date | Structured |
| `department` | Department where employee works (e.g., IT, Sales, HR) | String | Structured |
| `job_title` | Current position/role title | String | Structured |
| `employment_type` | Full-time, Part-time, Contract, Intern | String | Structured |
| `salary` | Annual salary amount | Float | Structured |
| `currency` | Currency code for salary (e.g., USD, EUR, GBP) | String | Structured |
| `manager_id` | Employee ID of direct manager | Integer | Structured |
| `is_active` | Whether employee is currently employed | Boolean | Structured |
| `termination_date` | Date employment ended (if applicable) | Date | Structured |
| `performance_rating` | Latest performance review score (1-5) | Integer | Structured |
| `skills` | List of employee skills and competencies | String (comma-separated) | Structured |
| `emergency_contact_name` | Name of emergency contact person | String | Structured |
| `emergency_contact_phone` | Phone number of emergency contact | String | Structured |
| `profile_photo` | Employee photo for ID badge | Image (JPEG/PNG) | Unstructured |
| `resume_document` | Employee's CV/resume file | Document (PDF) | Unstructured |
| `performance_review_notes` | Detailed feedback from performance reviews | Text | Unstructured |

---

#### Internal Data Source 2: Transactional/Sales Data

<div class="key-concept">
**About Transactional Data:**
Transactional data captures business transactions such as sales, purchases, and payments. This is essential for financial reporting, revenue analysis, and customer behavior understanding.
</div>

| Feature Name | Description | Data Type | Storage |
|--------------|-------------|-----------|---------|
| `transaction_id` | Unique identifier for each transaction | Integer | Structured |
| `order_number` | Human-readable order reference number | String | Structured |
| `customer_id` | Unique identifier for the customer | Integer | Structured |
| `transaction_date` | Date and time when transaction occurred | DateTime | Structured |
| `product_id` | Unique identifier for the product sold | Integer | Structured |
| `product_name` | Name/description of the product | String | Structured |
| `product_category` | Category of product (e.g., Electronics, Clothing) | String | Structured |
| `quantity` | Number of units purchased | Integer | Structured |
| `unit_price` | Price per single unit | Float | Structured |
| `discount_percentage` | Discount applied to the transaction (0-100) | Float | Structured |
| `tax_amount` | Tax charged on the transaction | Float | Structured |
| `total_amount` | Final total amount paid | Float | Structured |
| `payment_method` | How customer paid (Credit Card, PayPal, Cash) | String | Structured |
| `payment_status` | Status of payment (Completed, Pending, Failed) | String | Structured |
| `shipping_address` | Delivery address for the order | String | Structured |
| `shipping_cost` | Cost of shipping/delivery | Float | Structured |
| `is_refunded` | Whether transaction was refunded | Boolean | Structured |
| `refund_date` | Date refund was processed (if applicable) | Date | Structured |
| `sales_channel` | Where sale occurred (Online, In-Store, Mobile App) | String | Structured |
| `salesperson_id` | Employee who processed the sale | Integer | Structured |
| `customer_notes` | Special instructions or comments from customer | Text | Unstructured |
| `receipt_image` | Scanned copy of physical receipt | Image (JPEG/PDF) | Unstructured |

---

#### External Data Source 1: Social Media Data

<div class="key-concept">
**About Social Media Data:**
Social media data comes from platforms like Twitter, Facebook, Instagram, and LinkedIn. This external data provides insights into customer sentiment, brand perception, and market trends.
</div>

| Feature Name | Description | Data Type | Storage |
|--------------|-------------|-----------|---------|
| `post_id` | Unique identifier for the social media post | String | Structured |
| `platform` | Social media platform (Twitter, Facebook, Instagram) | String | Structured |
| `username` | User's handle or username | String | Structured |
| `user_id` | Unique identifier for the user on the platform | String | Structured |
| `post_timestamp` | Date and time when post was created | DateTime | Structured |
| `post_text` | Text content of the post | Text | Unstructured |
| `hashtags` | Hashtags used in the post | String (comma-separated) | Structured |
| `mentions` | Other users mentioned in the post | String (comma-separated) | Structured |
| `likes_count` | Number of likes/reactions | Integer | Structured |
| `shares_count` | Number of shares/retweets | Integer | Structured |
| `comments_count` | Number of comments/replies | Integer | Structured |
| `sentiment_score` | Sentiment analysis score (-1 to +1) | Float | Structured |
| `sentiment_label` | Sentiment classification (Positive, Negative, Neutral) | String | Structured |
| `language` | Language of the post (e.g., en, es, fr) | String | Structured |
| `location` | Geographic location where post was made | String | Structured |
| `is_verified` | Whether user account is verified | Boolean | Structured |
| `follower_count` | Number of followers the user has | Integer | Structured |
| `post_url` | Direct link to the post | String | Structured |
| `media_type` | Type of media attached (Image, Video, None) | String | Structured |
| `image_url` | URL to attached image | String | Structured |
| `video_url` | URL to attached video | String | Structured |
| `image_file` | Downloaded image file | Image (JPEG/PNG) | Unstructured |
| `video_file` | Downloaded video file | Video (MP4/MOV) | Unstructured |
| `audio_file` | Audio extracted from video or voice post | Audio (MP3/WAV) | Unstructured |

---

#### External Data Source 2: Weather/IoT Sensor Data

<div class="key-concept">
**About Weather/IoT Data:**
Weather and IoT sensor data comes from external sources like weather APIs, environmental sensors, and public data feeds. This data is valuable for industries like agriculture, logistics, retail, and energy.
</div>

| Feature Name | Description | Data Type | Storage |
|--------------|-------------|-----------|---------|
| `sensor_id` | Unique identifier for the sensor device | String | Structured |
| `reading_timestamp` | Date and time of the sensor reading | DateTime | Structured |
| `location_latitude` | Geographic latitude coordinate | Float | Structured |
| `location_longitude` | Geographic longitude coordinate | Float | Structured |
| `location_name` | Human-readable location name (e.g., "London, UK") | String | Structured |
| `temperature_celsius` | Temperature reading in Celsius | Float | Structured |
| `temperature_fahrenheit` | Temperature reading in Fahrenheit | Float | Structured |
| `humidity_percentage` | Relative humidity (0-100%) | Float | Structured |
| `pressure_hpa` | Atmospheric pressure in hectopascals | Float | Structured |
| `wind_speed_kmh` | Wind speed in kilometers per hour | Float | Structured |
| `wind_direction_degrees` | Wind direction in degrees (0-360) | Integer | Structured |
| `precipitation_mm` | Rainfall amount in millimeters | Float | Structured |
| `visibility_km` | Visibility distance in kilometers | Float | Structured |
| `uv_index` | UV radiation index (0-11+) | Integer | Structured |
| `cloud_cover_percentage` | Cloud coverage (0-100%) | Integer | Structured |
| `weather_condition` | General weather description (Sunny, Rainy, Cloudy) | String | Structured |
| `air_quality_index` | Air quality measurement (0-500) | Integer | Structured |
| `pollution_pm25` | PM2.5 particulate matter concentration | Float | Structured |
| `pollution_pm10` | PM10 particulate matter concentration | Float | Structured |
| `is_daytime` | Whether reading was taken during daylight | Boolean | Structured |
| `sunrise_time` | Time of sunrise | Time | Structured |
| `sunset_time` | Time of sunset | Time | Structured |
| `forecast_text` | Detailed weather forecast description | Text | Unstructured |
| `satellite_image` | Satellite weather imagery | Image (JPEG/PNG) | Unstructured |
| `radar_image` | Weather radar image | Image (JPEG/PNG) | Unstructured |

---

### ðŸ’» Practical Implementation Examples

Now let's see how to work with this data in practice using different tools and technologies.

#### Example 1: Creating HR Data in Excel/CSV

<div class="important-info">
**Scenario:** You need to create a structured HR database for a small company with 10 employees.

**Tool:** Microsoft Excel or Google Sheets

**Steps:**
1. Create column headers matching the feature names
2. Enter data for each employee in rows
3. Apply data validation to ensure correct data types
4. Save as CSV for portability or XLSX for Excel features
</div>

**Sample CSV Structure:**

```csv
employee_id,staff_surname,staff_firstname,email_address,phone_number,date_of_birth,hire_date,department,job_title,employment_type,salary,currency,manager_id,is_active,performance_rating
1001,Smith,John,john.smith@company.com,+44-20-1234-5678,1985-03-15,2020-01-10,IT,Software Engineer,Full-time,65000.00,GBP,1005,TRUE,4
1002,Johnson,Sarah,sarah.johnson@company.com,+44-20-1234-5679,1990-07-22,2019-05-15,Sales,Sales Manager,Full-time,75000.00,GBP,1010,TRUE,5
1003,Williams,Michael,michael.williams@company.com,+44-20-1234-5680,1988-11-30,2021-03-01,Marketing,Marketing Specialist,Full-time,55000.00,GBP,1008,TRUE,4
1004,Brown,Emma,emma.brown@company.com,+44-20-1234-5681,1992-02-14,2022-06-20,HR,HR Coordinator,Part-time,35000.00,GBP,1009,TRUE,3
1005,Jones,David,david.jones@company.com,+44-20-1234-5682,1980-09-05,2015-08-12,IT,IT Director,Full-time,95000.00,GBP,NULL,TRUE,5
```

**Excel Data Validation Examples:**

```
Column A (employee_id): Data Type = Whole Number, Minimum = 1000
Column F (date_of_birth): Data Type = Date, Format = YYYY-MM-DD
Column K (salary): Data Type = Decimal, Minimum = 0
Column N (is_active): Data Type = List, Values = TRUE, FALSE
Column O (performance_rating): Data Type = Whole Number, Between 1 and 5
```

---

#### Example 2: Working with Transactional Data in Python (Pandas)

<div class="important-info">
**Scenario:** Analyze sales transactions to find top-selling products and revenue by category.

**Tool:** Python with Pandas library

**Objective:** Load, clean, and analyze transactional data
</div>

```python
import pandas as pd
import numpy as np
from datetime import datetime

# Create sample transactional data
transactions_data = {
    'transaction_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008],
    'order_number': ['ORD-2025-001', 'ORD-2025-002', 'ORD-2025-003', 'ORD-2025-004',
                     'ORD-2025-005', 'ORD-2025-006', 'ORD-2025-007', 'ORD-2025-008'],
    'customer_id': [5001, 5002, 5001, 5003, 5004, 5002, 5005, 5003],
    'transaction_date': ['2025-02-01 10:30:00', '2025-02-01 11:15:00', '2025-02-01 14:20:00',
                         '2025-02-02 09:45:00', '2025-02-02 13:30:00', '2025-02-03 10:00:00',
                         '2025-02-03 15:45:00', '2025-02-04 11:30:00'],
    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop', 'Headphones', 'Webcam', 'Monitor'],
    'product_category': ['Electronics', 'Electronics', 'Electronics', 'Electronics',
                         'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'quantity': [1, 2, 1, 1, 1, 1, 2, 1],
    'unit_price': [899.99, 25.99, 79.99, 299.99, 899.99, 149.99, 89.99, 299.99],
    'discount_percentage': [10, 0, 5, 15, 10, 0, 10, 15],
    'payment_method': ['Credit Card', 'PayPal', 'Credit Card', 'Debit Card',
                       'Credit Card', 'PayPal', 'Credit Card', 'Debit Card'],
    'payment_status': ['Completed', 'Completed', 'Completed', 'Completed',
                       'Completed', 'Completed', 'Completed', 'Completed'],
    'is_refunded': [False, False, False, False, False, False, False, False]
}

# Create DataFrame
df_transactions = pd.DataFrame(transactions_data)

# Convert data types
df_transactions['transaction_date'] = pd.to_datetime(df_transactions['transaction_date'])
df_transactions['is_refunded'] = df_transactions['is_refunded'].astype(bool)

# Calculate total amount for each transaction
df_transactions['discount_amount'] = (df_transactions['unit_price'] *
                                      df_transactions['quantity'] *
                                      df_transactions['discount_percentage'] / 100)
df_transactions['total_amount'] = (df_transactions['unit_price'] *
                                   df_transactions['quantity'] -
                                   df_transactions['discount_amount'])

# Display the data
print("=" * 80)
print("TRANSACTIONAL DATA SAMPLE")
print("=" * 80)
print(df_transactions[['order_number', 'product_name', 'quantity', 'unit_price', 'total_amount']].head())
print()

# Analysis 1: Total revenue
total_revenue = df_transactions['total_amount'].sum()
print(f"Total Revenue: Â£{total_revenue:,.2f}")
print()

# Analysis 2: Top-selling products by quantity
print("=" * 80)
print("TOP-SELLING PRODUCTS BY QUANTITY")
print("=" * 80)
top_products = df_transactions.groupby('product_name').agg({
    'quantity': 'sum',
    'total_amount': 'sum'
}).sort_values('quantity', ascending=False)
print(top_products)
print()

# Analysis 3: Revenue by payment method
print("=" * 80)
print("REVENUE BY PAYMENT METHOD")
print("=" * 80)
payment_revenue = df_transactions.groupby('payment_method')['total_amount'].sum().sort_values(ascending=False)
print(payment_revenue)
print()

# Analysis 4: Daily sales trend
print("=" * 80)
print("DAILY SALES TREND")
print("=" * 80)
df_transactions['date'] = df_transactions['transaction_date'].dt.date
daily_sales = df_transactions.groupby('date')['total_amount'].sum()
print(daily_sales)
print()

# Save to CSV
df_transactions.to_csv('transactions_data.csv', index=False)
print("âœ… Data saved to 'transactions_data.csv'")
```

**Expected Output:**

```
================================================================================
TRANSACTIONAL DATA SAMPLE
================================================================================
    order_number product_name  quantity  unit_price  total_amount
0  ORD-2025-001       Laptop         1      899.99        809.99
1  ORD-2025-002        Mouse         2       25.99         51.98
2  ORD-2025-003     Keyboard         1       79.99         75.99
3  ORD-2025-004      Monitor         1      299.99        254.99
4  ORD-2025-005       Laptop         1      899.99        809.99

Total Revenue: Â£3,632.88

================================================================================
TOP-SELLING PRODUCTS BY QUANTITY
================================================================================
              quantity  total_amount
product_name
Laptop               2       1619.98
Monitor              2        509.98
Webcam               2        161.98
Mouse                2         51.98
Headphones           1        149.99
Keyboard             1         75.99

================================================================================
REVENUE BY PAYMENT METHOD
================================================================================
payment_method
Credit Card    2727.93
Debit Card      554.98
PayPal          201.97
Name: total_amount, dtype: float64

================================================================================
DAILY SALES TREND
================================================================================
date
2025-02-01    937.96
2025-02-02    1064.98
2025-02-03    311.97
2025-02-04    254.99
Name: total_amount, dtype: float64

âœ… Data saved to 'transactions_data.csv'
```

---

#### Example 3: Storing HR Data in SQL Database

<div class="important-info">
**Scenario:** Create a relational database for HR data with proper data types and constraints.

**Tool:** SQL (MySQL/PostgreSQL)

**Objective:** Design and populate a structured HR database
</div>

```sql
-- Create HR database
CREATE DATABASE company_hr;
USE company_hr;

-- Create employees table with proper data types
CREATE TABLE employees (
    employee_id INT PRIMARY KEY AUTO_INCREMENT,
    staff_surname VARCHAR(100) NOT NULL,
    staff_firstname VARCHAR(100) NOT NULL,
    email_address VARCHAR(255) UNIQUE NOT NULL,
    phone_number VARCHAR(20),
    date_of_birth DATE NOT NULL,
    hire_date DATE NOT NULL,
    department VARCHAR(50) NOT NULL,
    job_title VARCHAR(100) NOT NULL,
    employment_type ENUM('Full-time', 'Part-time', 'Contract', 'Intern') NOT NULL,
    salary DECIMAL(10, 2) NOT NULL,
    currency CHAR(3) DEFAULT 'GBP',
    manager_id INT,
    is_active BOOLEAN DEFAULT TRUE,
    termination_date DATE NULL,
    performance_rating INT CHECK (performance_rating BETWEEN 1 AND 5),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (manager_id) REFERENCES employees(employee_id)
);

-- Insert sample data
INSERT INTO employees (employee_id, staff_surname, staff_firstname, email_address, phone_number,
                       date_of_birth, hire_date, department, job_title, employment_type,
                       salary, currency, manager_id, is_active, performance_rating)
VALUES
(1001, 'Smith', 'John', 'john.smith@company.com', '+44-20-1234-5678',
 '1985-03-15', '2020-01-10', 'IT', 'Software Engineer', 'Full-time',
 65000.00, 'GBP', 1005, TRUE, 4),

(1002, 'Johnson', 'Sarah', 'sarah.johnson@company.com', '+44-20-1234-5679',
 '1990-07-22', '2019-05-15', 'Sales', 'Sales Manager', 'Full-time',
 75000.00, 'GBP', 1010, TRUE, 5),

(1003, 'Williams', 'Michael', 'michael.williams@company.com', '+44-20-1234-5680',
 '1988-11-30', '2021-03-01', 'Marketing', 'Marketing Specialist', 'Full-time',
 55000.00, 'GBP', 1008, TRUE, 4),

(1004, 'Brown', 'Emma', 'emma.brown@company.com', '+44-20-1234-5681',
 '1992-02-14', '2022-06-20', 'HR', 'HR Coordinator', 'Part-time',
 35000.00, 'GBP', 1009, TRUE, 3),

(1005, 'Jones', 'David', 'david.jones@company.com', '+44-20-1234-5682',
 '1980-09-05', '2015-08-12', 'IT', 'IT Director', 'Full-time',
 95000.00, 'GBP', NULL, TRUE, 5);

-- Query 1: Get all active employees
SELECT employee_id, staff_firstname, staff_surname, department, job_title, salary
FROM employees
WHERE is_active = TRUE
ORDER BY department, staff_surname;

-- Query 2: Calculate average salary by department
SELECT department,
       COUNT(*) as employee_count,
       AVG(salary) as avg_salary,
       MIN(salary) as min_salary,
       MAX(salary) as max_salary
FROM employees
WHERE is_active = TRUE
GROUP BY department
ORDER BY avg_salary DESC;

-- Query 3: Find employees and their managers
SELECT e.employee_id,
       CONCAT(e.staff_firstname, ' ', e.staff_surname) as employee_name,
       e.job_title,
       CONCAT(m.staff_firstname, ' ', m.staff_surname) as manager_name,
       m.job_title as manager_title
FROM employees e
LEFT JOIN employees m ON e.manager_id = m.employee_id
WHERE e.is_active = TRUE;

-- Query 4: Calculate tenure for each employee
SELECT employee_id,
       CONCAT(staff_firstname, ' ', staff_surname) as full_name,
       hire_date,
       DATEDIFF(CURDATE(), hire_date) as days_employed,
       ROUND(DATEDIFF(CURDATE(), hire_date) / 365.25, 1) as years_employed
FROM employees
WHERE is_active = TRUE
ORDER BY hire_date;
```

---

#### Example 4: Analyzing Social Media Data with Python

<div class="important-info">
**Scenario:** Collect and analyze social media sentiment about your brand.

**Tool:** Python with TextBlob for sentiment analysis

**Objective:** Process unstructured text data and extract insights
</div>

```python
import pandas as pd
from textblob import TextBlob
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import random

# Sample social media data
social_media_data = {
    'post_id': ['TW001', 'FB002', 'IG003', 'TW004', 'FB005', 'IG006', 'TW007', 'FB008'],
    'platform': ['Twitter', 'Facebook', 'Instagram', 'Twitter', 'Facebook', 'Instagram', 'Twitter', 'Facebook'],
    'username': ['@user1', 'user2', '@user3', '@user4', 'user5', '@user6', '@user7', 'user8'],
    'post_timestamp': [
        '2025-02-01 10:30:00', '2025-02-01 14:20:00', '2025-02-02 09:15:00',
        '2025-02-02 16:45:00', '2025-02-03 11:30:00', '2025-02-03 18:00:00',
        '2025-02-04 08:45:00', '2025-02-04 15:20:00'
    ],
    'post_text': [
        'Just bought the new laptop from @Company! Absolutely love it! Best purchase ever! ðŸ˜',
        'The customer service at Company was terrible. Very disappointed with my experience.',
        'Amazing product quality! The monitor is crystal clear. Highly recommend! â­â­â­â­â­',
        'Ordered a keyboard but it arrived damaged. Waiting for replacement. Not happy.',
        'Company has the best prices and fast shipping. Will definitely buy again!',
        'The webcam quality is okay, nothing special. Expected better for the price.',
        'Fantastic experience! The staff was helpful and the product exceeded expectations!',
        'Product is good but delivery took too long. Improve your logistics please.'
    ],
    'likes_count': [245, 12, 389, 45, 156, 78, 312, 89],
    'shares_count': [34, 3, 67, 8, 23, 12, 45, 15],
    'comments_count': [18, 7, 42, 15, 28, 9, 31, 19]
}

# Create DataFrame
df_social = pd.DataFrame(social_media_data)

# Convert timestamp to datetime
df_social['post_timestamp'] = pd.to_datetime(df_social['post_timestamp'])

# Perform sentiment analysis
def analyze_sentiment(text):
    # Analyze sentiment of text using TextBlob
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity  # -1 (negative) to +1 (positive)

    # Classify sentiment
    if polarity > 0.1:
        label = 'Positive'
    elif polarity < -0.1:
        label = 'Negative'
    else:
        label = 'Neutral'

    return polarity, label

# Apply sentiment analysis
df_social[['sentiment_score', 'sentiment_label']] = df_social['post_text'].apply(
    lambda x: pd.Series(analyze_sentiment(x))
)

# Calculate engagement score
df_social['engagement_score'] = (
    df_social['likes_count'] +
    df_social['shares_count'] * 2 +
    df_social['comments_count'] * 3
)

# Display results
print("=" * 100)
print("SOCIAL MEDIA SENTIMENT ANALYSIS")
print("=" * 100)
print(df_social[['platform', 'post_text', 'sentiment_label', 'sentiment_score', 'engagement_score']])
print()

# Summary statistics
print("=" * 100)
print("SENTIMENT SUMMARY")
print("=" * 100)
sentiment_summary = df_social['sentiment_label'].value_counts()
print(sentiment_summary)
print()

print(f"Average Sentiment Score: {df_social['sentiment_score'].mean():.3f}")
print(f"Most Positive Post: {df_social.loc[df_social['sentiment_score'].idxmax(), 'post_text'][:80]}...")
print(f"Most Negative Post: {df_social.loc[df_social['sentiment_score'].idxmin(), 'post_text'][:80]}...")
print()

# Platform analysis
print("=" * 100)
print("SENTIMENT BY PLATFORM")
print("=" * 100)
platform_sentiment = df_social.groupby('platform').agg({
    'sentiment_score': 'mean',
    'engagement_score': 'sum',
    'post_id': 'count'
}).rename(columns={'post_id': 'post_count'})
print(platform_sentiment)
print()

# Engagement analysis
print("=" * 100)
print("TOP ENGAGING POSTS")
print("=" * 100)
top_posts = df_social.nlargest(3, 'engagement_score')[['platform', 'post_text', 'engagement_score', 'sentiment_label']]
print(top_posts)
```

**Expected Output:**

```
====================================================================================================
SOCIAL MEDIA SENTIMENT ANALYSIS
====================================================================================================
   platform                                          post_text sentiment_label  sentiment_score  engagement_score
0   Twitter  Just bought the new laptop from @Company! A...        Positive            0.650               349
1  Facebook  The customer service at Company was terribl...        Negative           -0.700                36
2 Instagram  Amazing product quality! The monitor is cry...        Positive            0.800               590
3   Twitter  Ordered a keyboard but it arrived damaged. ...        Negative           -0.400               109
4  Facebook  Company has the best prices and fast shippi...        Positive            0.500               272
5 Instagram  The webcam quality is okay, nothing special...         Neutral            0.050               129
6   Twitter  Fantastic experience! The staff was helpful...        Positive            0.750               483
7  Facebook  Product is good but delivery took too long....         Neutral            0.100               161

====================================================================================================
SENTIMENT SUMMARY
====================================================================================================
Positive    4
Negative    2
Neutral     2
Name: sentiment_label, dtype: int64

Average Sentiment Score: 0.219
Most Positive Post: Amazing product quality! The monitor is crystal clear. Highly recommend! â­â­â­â­â­...
Most Negative Post: The customer service at Company was terrible. Very disappointed with my experie...

====================================================================================================
SENTIMENT BY PLATFORM
====================================================================================================
           sentiment_score  engagement_score  post_count
platform
Facebook             -0.033               469           3
Instagram             0.425               719           2
Twitter               0.167               941           3

====================================================================================================
TOP ENGAGING POSTS
====================================================================================================
   platform                                          post_text  engagement_score sentiment_label
2 Instagram  Amazing product quality! The monitor is cry...               590        Positive
6   Twitter  Fantastic experience! The staff was helpful...               483        Positive
0   Twitter  Just bought the new laptop from @Company! A...               349        Positive
```

---

### Key Insights from the Task

<div class="key-concept">
**What You've Learned:**

1. **Internal Data** (HR, Transactional) is typically **structured** and stored in relational databases
2. **External Data** (Social Media, IoT) can be **mixed** - some structured (metrics), some unstructured (text, images)
3. **Data types matter** - choosing the right type ensures efficiency, accuracy, and proper analysis
4. **Documentation is critical** - clear descriptions help future users understand the data
5. **Different tools for different data** - SQL for structured, Python for mixed, specialized tools for unstructured
6. **Storage choice depends on data characteristics** - structured data â†’ databases, unstructured â†’ NoSQL/data lakes
7. **Real-world data requires cleaning and transformation** before analysis
8. **Combining data sources** (internal + external) provides comprehensive business insights

**Best Practices:**

âœ… Always document your data features with clear descriptions
âœ… Choose appropriate data types to optimize storage and performance
âœ… Use structured storage for tabular data, unstructured for media/text
âœ… Validate data types and constraints to maintain data quality
âœ… Consider future users when designing your data structure
âœ… Combine internal and external data for richer insights
âœ… Use the right tool for the job (Excel, Python, SQL, etc.)
âœ… Perform exploratory analysis to understand your data before deep analysis
</div>

---

### ðŸ“š Summary Table: All Four Data Sources

| Data Source | Type | Primary Storage | Key Features | Common Use Cases |
|-------------|------|-----------------|--------------|------------------|
| **HR Data** | Internal | Structured (SQL Database) | Employee IDs, names, salaries, dates, performance ratings | Payroll, workforce planning, compliance, performance management |
| **Transactional Data** | Internal | Structured (SQL Database) | Transaction IDs, amounts, dates, customer IDs, product details | Revenue analysis, customer behavior, inventory management, financial reporting |
| **Social Media Data** | External | Mixed (Structured metrics + Unstructured text/media) | Post text, sentiment, engagement metrics, images, videos | Brand monitoring, sentiment analysis, customer feedback, marketing insights |
| **Weather/IoT Data** | External | Mixed (Structured readings + Unstructured images) | Temperature, humidity, sensor readings, timestamps, satellite images | Demand forecasting, logistics optimization, agriculture, energy management |

This comprehensive task solution demonstrates how to identify, document, structure, and work with both internal and external data sources using industry-standard tools and best practices! ðŸš€
            """,
            "key_points": [
                "Data can be stored in structured (tabular) or unstructured (free-form) formats",
                "Structured data is organized in tables with rows (records/entities) and columns (attributes)",
                "Relational databases (MySQL, PostgreSQL) store data in related tables using primary and foreign keys, enabling powerful SQL querying",
                "CSV/Excel files are human-readable and portable but lack scalability and performance for large datasets",
                "Data frames (pandas, R) are mutable two-dimensional structures that can hold mixed data types with powerful manipulation capabilities",
                "Unstructured data includes text, images, videos, audio, and sensor data - rich in information but more challenging to analyze",
                "Text data is analyzed using Natural Language Processing (NLP) techniques like sentiment analysis, topic modeling, and entity recognition",
                "Images are represented as multi-dimensional arrays (height Ã— width Ã— color channels), videos add a frames dimension",
                "Audio data is represented as time series or frequency domain using Fourier transformation",
                "NoSQL databases (MongoDB, Cassandra) are flexible and scalable, storing documents (JSON/XML), key-value pairs, wide-columns, or graphs",
                "Big data technologies (Hadoop, Spark) enable distributed processing and analysis of massive datasets at scale",
                "Combining structured and unstructured data provides comprehensive insights (e.g., demographics + customer reviews)",
                "Natural Language Processing (NLP) has emerged as crucial for analyzing textual data and understanding human language",
                "Machine learning is increasingly important for handling unstructured data: image classification, speech-to-text, sentiment analysis",
                "Data is only as valuable as the conclusions drawn from it - the goal is actionable insights for decision-making",
                "Structured data advantages: organized, queryable, consistent schema, good for transactions and reporting",
                "Unstructured data advantages: rich information, captures real-world complexity, enables advanced AI/ML applications",
                "Key structured technologies: SQL databases, spreadsheets, data frames (pandas, R)",
                "Key unstructured technologies: NLP tools (NLTK, spaCy), computer vision (OpenCV), NoSQL databases, big data platforms",
                "The ultimate objective of data analysis is to derive valuable and practical insights to inform decision-making, regardless of data type",
                "Relational databases use the relational model with tables, rows, columns, and keys to define relationships between tables",
                "Relational databases provide ACID properties (Atomicity, Consistency, Isolation, Durability) for robust consistency guarantees",
                "Relational databases scale vertically (more powerful server) and are great for structured data but difficult to scale horizontally",
                "Relational databases use SQL (Structured Query Language) - standardized and powerful but complex for advanced queries",
                "Big data (NoSQL) databases are designed to store, process, and analyze data too large or complex for conventional databases",
                "Big data databases use BASE model (Basically Available, Soft state, Eventually consistent) for better performance and scalability",
                "Big data databases scale horizontally (add more servers) and can handle massive volumes (terabytes to petabytes)",
                "Big data databases use varied query approaches: SQL-like (Hive, CQL), native APIs (MQL), MapReduce, or graph queries",
                "Choose relational databases for: structured data, ACID compliance, complex joins, transactions, stable schema",
                "Choose big data databases for: unstructured data, horizontal scaling, massive volumes, eventual consistency, flexible schema",
                "Modern data architecture often uses hybrid approach: relational for transactions, NoSQL for analytics, data lakes for raw storage",
                "Relational databases best for: banking, ERP, CRM, e-commerce transactions with structured data",
                "Big data databases best for: social media analytics, IoT data, real-time recommendations, log analysis with unstructured data",
                "Fundamental data types include: Numeric (Integer, Float, Complex), String/Character, Boolean, and Date/Time",
                "Integer data type represents whole numbers (positive/negative) used for counting, IDs, and indexing",
                "Float data type represents decimal numbers used for prices, measurements, and scientific calculations",
                "Complex numbers combine real and imaginary components (a+bj) used in engineering and signal processing",
                "Character represents a single symbol while String represents a sequence of characters (text)",
                "String data includes any character in the encoding format: letters, punctuation, special characters, and non-calculation numbers",
                "Boolean data type has only two values (True/False, 1/0) used for flags, conditions, and status indicators",
                "Date represents calendar dates (2023-06-20), Time represents time of day (13:45:30)",
                "DateTime combines date and time (2023-06-20T13:45:30) used for timestamps and logs",
                "TimeSpan/Duration represents time intervals used for calculating durations and time differences",
                "Choosing the right data type affects storage efficiency, performance, accuracy, and validation",
                "Common pitfalls: storing phone numbers as integers, using float for currency, storing dates as strings",
                "Data type selection should consider: storage space, performance needs, accuracy requirements, and compatibility",
                "The data lifecycle consists of 8 stages: Collection, Pre-processing, Storage, Processing, Analysis, Visualisation, Action, Archiving/Destruction",
                "Data collection involves gathering raw data from IoT devices, user content, transactions, APIs, and other sources",
                "Pre-processing includes data cleaning, handling missing values, removing duplicates, type conversion, and normalization",
                "Storage options include relational databases, NoSQL databases, data lakes, and data warehouses based on data characteristics",
                "Processing stage involves aggregation, feature engineering, data integration, and transformation for analysis",
                "Analysis techniques include exploratory data analysis, statistical analysis, machine learning, and time series forecasting",
                "Visualisation presents insights through charts, dashboards, reports, and infographics for decision-makers",
                "Action stage translates insights into strategic decisions, operational improvements, marketing campaigns, and product development",
                "Archiving preserves data for compliance and future reference; destruction securely deletes obsolete data per retention policies",
                "Each lifecycle stage has considerations for data quality, governance, privacy (GDPR, CCPA), and security",
                "Data quality involves accuracy, completeness, consistency, and timeliness with validation at each stage",
                "Data governance requires policies, roles, standards, and compliance frameworks throughout the lifecycle",
                "RetailCo example demonstrates lifecycle: collecting e-commerce/POS data, cleaning duplicates, storing in hybrid databases",
                "RetailCo combines transaction and feedback data, uses ML for forecasting, creates KPI dashboards, and takes targeted actions",
                "Organizations must comply with data protection laws (GDPR, PCI DSS) and maintain strong governance structures throughout the lifecycle",
                "Data lifecycle approach provides 7 key benefits: enhanced decision-making, improved quality, efficiency, compliance, cost savings, security, and value maximization",
                "Enhanced decision-making: Structured lifecycle ensures organized collection, reliable storage, and systematic analysis for better insights",
                "Improved data quality: Systematic cleaning, validation, and transformation ensure accuracy, completeness, consistency, and timeliness",
                "Efficient data management: Optimized storage (hot/warm/cold), easy access, automated processes, and proper archiving/disposal",
                "Regulatory compliance: Lifecycle approach helps comply with GDPR, HIPAA, POPI Act, PCI DSS, avoiding fines and legal issues",
                "Cost savings: Reduced storage costs (archiving/deletion), operational efficiency (automation), fewer errors, and better resource allocation",
                "Security and privacy: Protection at every stage (encryption, access controls, secure deletion) maintains customer trust and reduces breach risk",
                "Maximizing value: Data reuse for multiple purposes, cross-functional insights, advanced analytics, and innovation opportunities",
                "Real-world examples: Amazon (recommendations), Netflix (content decisions), Uber (routing optimization), Healthcare (treatment improvements)",
                "Data lifecycle management enables systematic data handling from creation to disposal, resulting in wiser choices and competitive advantage",
                "Organizations using lifecycle approach achieve better ROI through reduced costs, improved efficiency, and enhanced decision-making capabilities",
                "Lesson summary: Understanding different data types (numeric, string, boolean, date/time) is fundamental to proper data handling",
                "Lesson summary: Data can be sourced from internal systems (transactional, CRM, HR) and external sources (social media, IoT, public datasets)",
                "Lesson summary: Multiple storage options exist (spreadsheets, databases, data warehouses, data lakes, cloud storage) with different characteristics",
                "Lesson summary: Data representation can be structured (tabular, organized) or unstructured (free-form, flexible)",
                "Lesson summary: Relational databases (ACID, SQL, vertical scaling) differ from big data databases (BASE, horizontal scaling, flexible schema)",
                "Lesson summary: Individual data entities use specific data types (integer for IDs, float for prices, string for text, boolean for flags, datetime for timestamps)",
                "Lesson summary: Organizations need systematic data lifecycle plans covering all 8 stages from collection to archiving/destruction",
                "Lesson summary: Data lifecycle approach provides 7 key benefits: enhanced decisions, quality, efficiency, compliance, cost savings, security, and value",
                "Lesson summary: Hybrid database architectures combine relational and NoSQL databases for optimal performance and scalability",
                "Lesson summary: Proper data type selection ensures storage efficiency, performance optimization, data accuracy, and validation",
                "Lesson summary: Data governance and compliance (GDPR, HIPAA, PCI DSS) are essential throughout the entire data lifecycle",
                "Lesson summary: Data is only valuable when properly collected, stored, managed, and analyzed using systematic approaches",
                "Lesson summary: Real-world applications combine multiple concepts (e.g., e-commerce using various data types, sources, storage, and lifecycle management)",
                "Task solution: Internal data sources (HR, Transactional) are typically structured and stored in relational databases",
                "Task solution: External data sources (Social Media, IoT/Weather) can be mixed - structured metrics and unstructured content",
                "Task solution: HR data features include employee_id (integer), names (string), salary (float), hire_date (date), is_active (boolean)",
                "Task solution: Transactional data features include transaction_id (integer), amounts (float), dates (datetime), payment_status (string)",
                "Task solution: Social Media data includes structured metrics (likes, shares) and unstructured content (post text, images, videos)",
                "Task solution: Weather/IoT data includes structured readings (temperature, humidity) and unstructured imagery (satellite, radar)",
                "Task solution: Documentation with clear descriptions is critical for future users and data maintainability",
                "Task solution: Proper data type selection (integer for IDs, float for measurements, string for text, boolean for flags) optimizes storage and performance",
                "Task solution: Excel/CSV works for small datasets, SQL databases for structured data, Python/Pandas for analysis, NoSQL for unstructured data",
                "Task solution: Combining internal and external data sources provides comprehensive business insights (e.g., sales data + weather data for demand forecasting)",
                "Task solution: Real-world implementation requires data validation, type conversion, cleaning, and transformation before analysis",
                "Task solution: Sentiment analysis on social media text (unstructured) can be quantified into sentiment scores (structured) for analysis",
                "Task solution: Best practices include documenting features, choosing appropriate data types, validating constraints, and using the right tool for each data type"
            ],
            "visual_elements": {
                "diagrams": True,
                "tables": True,
                "highlighted_sections": True
            }
        },
        {
            "lesson_number": "1.3",
            "title": "Data Collection",
            "content": """
### Primary and Secondary Data

Even though a wealth of data is already available, some studies require collecting 'new' data. These two broad categories for data collection can be used to categorise data as **primary** or **secondary**.

#### ðŸŽ¯ Core Concept: Primary vs Secondary Data

<div class="mermaid">
flowchart LR
    A[Data Collection] --> B[Primary Data<br/>ðŸ“<br/>First-hand Information]
    A --> C[Secondary Data<br/>ðŸ“š<br/>Pre-existing Information]
    
    B --> B1[Collected by<br/>Researcher]
    B --> B2[For Specific<br/>Purpose]
    
    C --> C1[Collected by<br/>Others]
    C --> C2[For Different<br/>Purpose]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style B2 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style C2 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
</div>

---

### ðŸ“ Primary Data (First-Order Data)

First-hand information gathered by the researcher or analysts for a particular research project or goal is **primary** or **first-order data**. Primary data is collected from the source and is intended to respond to specific research questions.

#### ðŸŽ¨ Primary Data Collection Methods

<div class="mermaid">
graph TB
    A[Primary Data<br/>Collection Methods] --> B[Surveys<br/>ðŸ“‹<br/>Questionnaires]
    A --> C[Interviews<br/>ðŸŽ¤<br/>One-on-one]
    A --> D[Experiments<br/>ðŸ”¬<br/>Controlled Tests]
    A --> E[Direct Observation<br/>ðŸ‘ï¸<br/>Recording Behavior]
    A --> F[Focus Groups<br/>ðŸ‘¥<br/>Group Discussions]
    
    B --> B1[Online/Paper<br/>Structured Questions]
    C --> C1[In-depth<br/>Qualitative Insights]
    D --> D1[Controlled<br/>Variables]
    E --> E1[Natural<br/>Settings]
    F --> F1[Interactive<br/>Feedback]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
</div>

There are various techniques for gathering primary data, and the choice of method frequently depends on the **purpose of the study**, the **resources at hand**, and the **kind of data** being sought. Here are typical primary data collection methods:

##### 1. Surveys / Questionnaires ðŸ“‹

**Description**: Structured questionnaires distributed to a target audience to gather quantitative and qualitative data.

**Delivery Methods**:
- **In-person**: Face-to-face administration in specific locations
- **Mail**: Postal surveys sent to respondents' addresses
- **Phone**: Telephone interviews using standardized questions
- **Online**: Web-based surveys via email, websites, or social media

**Characteristics**:
- Structured questions ensure consistency across respondents
- Can reach large audiences quickly
- Standardized format for easy analysis
- Collect information from a sample of people
- Mix of closed-ended (quantitative) and open-ended (qualitative) questions

**Examples**:
- Customer satisfaction surveys
- Market research questionnaires
- Employee engagement surveys
- Product feedback forms
- Net Promoter Score (NPS) surveys

**Advantages**:
- Cost-effective for large samples
- Easy to analyze statistically
- Respondents can answer at their convenience
- Anonymity can encourage honest responses
- Standardized data collection
- Can cover wide geographic areas

**Disadvantages**:
- Low response rates (especially mail/online)
- Limited depth of responses
- Potential for biased or misunderstood questions
- No opportunity for follow-up clarification
- Response bias (only motivated people respond)

##### 2. Interviews ðŸŽ¤

**Description**: One-on-one conversations between researcher and participant to gather in-depth qualitative information.

**Delivery Methods**:
- **In-person**: Face-to-face interviews
- **Phone**: Telephone interviews
- **Online**: Video conferencing (Zoom, Teams, Skype)

**Types of Interviews**:

**Structured Interviews** ðŸ“‹
- Predetermined list of questions in fixed order
- Same questions asked to all participants
- Limited flexibility for follow-up
- Easier to analyze and compare responses
- Good for quantitative analysis

**Semi-Structured Interviews** ðŸ”„
- Some predetermined questions as framework
- Freedom to delve deeper into interesting topics
- Can probe for more details
- Flexible order of questions
- Balance between consistency and exploration

**Unstructured Interviews** ðŸ’¬
- More like a guided conversation
- Broad topics rather than specific questions
- Maximum flexibility to explore emerging themes
- Participant-led discussion
- Rich, detailed qualitative data

**Examples**:
- Customer interviews about product experiences
- Expert interviews for industry insights
- User research interviews
- Stakeholder interviews
- Job interviews
- Clinical interviews

**Advantages**:
- Rich, detailed information
- Can explore complex topics in depth
- Flexibility to follow interesting leads
- Build rapport with participants
- Clarify misunderstandings immediately
- Observe non-verbal cues (in-person)

**Disadvantages**:
- Time-consuming and expensive
- Small sample sizes
- Interviewer bias can influence responses
- Difficult to analyze and quantify
- Requires skilled interviewers
- Cannot guarantee anonymity

##### 3. Observations ðŸ‘ï¸

**Description**: Systematically observing and documenting phenomena or behavior as it arises naturally in the wild.

**Types of Observation**:

**Overt Observation** ðŸ‘€
- Participants are **aware** they are being watched
- Researcher's presence is known
- May affect natural behavior (Hawthorne effect)
- More ethical (informed consent)
- Examples: Store managers observing customer service

**Covert Observation** ðŸ•µï¸
- Participants are **unaware** they are being watched
- Hidden or disguised observation
- More natural behavior
- Ethical concerns (privacy, consent)
- Examples: Mystery shoppers, hidden cameras

**Participant vs Non-Participant**:
- **Participant observation**: Researcher joins the activity
- **Non-participant observation**: Researcher observes from outside

**Characteristics**:
- No manipulation of environment
- Real-world settings
- Objective recording of behaviors
- Can be structured (specific behaviors) or unstructured (general observation)

**Examples**:
- Observing customer behavior in stores
- Watching how users interact with software
- Studying workplace processes
- Recording traffic patterns
- Classroom observations
- Wildlife behavior studies

**Advantages**:
- Natural behavior in real settings
- First-hand data collection
- Can capture non-verbal cues
- Rich contextual information
- Good for behaviors people can't articulate
- No reliance on self-reporting

**Disadvantages**:
- Observer may influence behavior (Hawthorne effect)
- Time-intensive
- Limited to observable behaviors (not thoughts/feelings)
- Difficult to record everything
- Ethical concerns with covert observation
- Observer bias in interpretation

##### 4. Experiments ðŸ”¬

**Description**: Controlled studies in which one variable is changed to track how it affects another variable.

**Characteristics**:
- Control over variables (independent and dependent)
- Random assignment of participants to conditions
- Establish cause-and-effect relationships
- Replicable under same conditions
- Manipulation of independent variable
- Measurement of dependent variable

**Types**:
- **Laboratory experiments**: Controlled environment
- **Field experiments**: Natural settings with controls
- **Natural experiments**: Observe naturally occurring conditions

**Examples**:
- A/B testing website designs
- Product testing in controlled environments
- Clinical trials for new treatments
- Usability testing
- Marketing campaign experiments
- Psychology experiments

**Advantages**:
- Can establish causality (cause-and-effect)
- High level of control over variables
- Replicable and verifiable
- Precise measurements
- Can isolate specific factors
- Scientific rigor

**Disadvantages**:
- Artificial settings may not reflect reality
- Expensive and time-consuming
- Ethical constraints in some contexts
- May not be feasible for all research questions
- External validity concerns (generalizability)
- Resource-intensive

**Note**: Even though they can be resource-intensive and inappropriate for some research questions, experiments can be a **powerful tool for demonstrating cause-and-effect relationships**.

##### 5. Focus Groups ðŸ‘¥

**Description**: A moderator guides a group discussion on a specific subject, exploring people's experiences, beliefs, or attitudes.

**Characteristics**:
- Moderated group interaction (typically 6-12 participants)
- Participants build on each other's ideas
- Explore attitudes, perceptions, and opinions
- Generate qualitative insights through discussion
- Interactive and dynamic
- Typically 60-90 minutes duration

**Process**:
1. Moderator introduces topic
2. Guided discussion with open-ended questions
3. Participants share perspectives
4. Group interaction reveals consensus and differences
5. Moderator probes deeper on key themes

**Examples**:
- Testing new product concepts
- Gathering feedback on advertising campaigns
- Understanding customer preferences
- Exploring user needs
- Political opinion research
- Brand perception studies

**Advantages**:
- Rich discussion and idea generation
- Participants stimulate each other's thinking
- Cost-effective for multiple participants simultaneously
- Can reveal consensus and differences
- Group dynamics can uncover insights
- Observe interactions and reactions

**Disadvantages**:
- Group dynamics may inhibit some participants
- Dominant voices can skew results
- Moderator skill critically important
- Results not statistically generalizable
- Groupthink may limit diverse opinions
- Difficult to schedule and coordinate

**Helpful Tip**: Focus groups can be helpful in **thoroughly examining people's experiences, beliefs, or attitudes** through interactive discussion.

##### 6. Case Studies ðŸ“

**Description**: Thorough investigation of a single subject (such as a person, group, organization, or event) using various data sources.

**Characteristics**:
- In-depth, comprehensive examination
- Multiple data sources (triangulation)
- Contextual analysis
- Holistic understanding
- Longitudinal or cross-sectional
- Rich qualitative data

**Data Sources Used**:
- Interviews with key individuals
- Document analysis
- Observations
- Archival records
- Physical artifacts
- Quantitative data

**Examples**:
- Business case studies (company success/failure)
- Medical case studies (patient treatment)
- Educational case studies (teaching methods)
- Legal case studies (court cases)
- Organizational change case studies
- Individual biography studies

**Advantages**:
- Thorough understanding of one situation
- Rich, detailed, contextual data
- Can explore complex phenomena
- Reveals interconnections and processes
- Real-world context preserved
- Multiple perspectives

**Disadvantages**:
- Results might not apply to others (limited generalizability)
- Time and resource intensive
- Potential for researcher bias
- Difficult to replicate
- Cannot establish causality
- Small sample (typically n=1)

**Note**: Case studies can give you a **thorough understanding of one situation**, but the results might **not apply to others** due to limited generalizability.

##### 7. Field Trials / Pilots ðŸŒ¾

**Description**: Testing a concept under actual/real-world conditions while recording results.

**Characteristics**:
- Real-world testing environment
- Controlled variables where possible
- Observation and measurement of outcomes
- Iterative testing and refinement
- Practical application focus

**Common Applications**:
- **Product Development**: Testing prototypes with real users
- **Engineering**: Testing new systems or processes
- **Agriculture**: Testing new crops, fertilizers, or farming methods
- **Medicine**: Clinical trials in real healthcare settings
- **Software**: Beta testing with actual users

**Examples**:
- Agricultural field trials of new crop varieties
- Pilot programs for new software features
- Beta testing consumer products
- Trial run of new manufacturing process
- Pilot implementation of new policy
- Test marketing in select regions

**Advantages**:
- Real-world feedback and validation
- Identifies practical problems before full rollout
- Reduces risk of large-scale failure
- Generates authentic user data
- Can refine before full launch
- Documents real-world performance

**Disadvantages**:
- Can be expensive to set up
- May have limited duration
- Results may vary in different contexts
- Potential disruption to operations
- Ethical considerations for participants
- Difficult to control all variables

**Note**: Field trials are frequently used in **product development, engineering, or agriculture** to test concepts under actual conditions while carefully recording results.

##### 8. Ethnography ðŸŒ

**Description**: Researcher immerses themselves in a community for an extended period to study the behaviors, culture, and practices of its members.

**Characteristics**:
- Long-term immersion (months to years)
- Participant observation
- Cultural context and meaning
- Holistic perspective
- Naturalistic setting
- Qualitative and interpretive

**Methods Used**:
- Participant observation
- In-depth interviews
- Field notes and journals
- Document analysis
- Photography/video recording
- Cultural artifact collection

**Examples**:
- Studying workplace culture
- Understanding consumer behavior in natural settings
- Researching community practices
- Exploring organizational rituals
- Investigating subcultures
- Cross-cultural studies

**Advantages**:
- Deep cultural understanding
- Rich contextual data
- Natural behavior observed
- Insider perspective
- Uncovers hidden patterns
- Long-term relationships build trust

**Disadvantages**:
- Extremely time-consuming (months/years)
- Very expensive
- Researcher can "go native" (lose objectivity)
- Difficult to generalize findings
- Ethical challenges of dual roles
- Language and cultural barriers

**Note**: To study behaviors, culture, and practices, the **researcher immerses themselves** in a community for an **extended period**, gaining insider perspective and deep understanding.

---

### ðŸ”„ Mixed-Methods Approach

It is **typical to combine several methods** within a single study to obtain a more thorough understanding of the research topic. A **mixed-methods approach** is a common term for this.

<div class="mermaid">
graph TB
    A[Mixed-Methods<br/>Research] --> B[Quantitative<br/>Methods<br/>ðŸ“Š]
    A --> C[Qualitative<br/>Methods<br/>ðŸ’¬]
    
    B --> B1[Surveys<br/>Experiments<br/>Structured Obs.]
    C --> C1[Interviews<br/>Focus Groups<br/>Ethnography]
    
    B1 --> D[Triangulation]
    C1 --> D
    
    D --> E[Comprehensive<br/>Understanding<br/>ðŸ’¡]
    
    E --> F[Numbers<br/>+ Context]
    E --> G[Breadth<br/>+ Depth]
    E --> H[Validation<br/>+ Exploration]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

**Benefits of Mixed-Methods**:
- **Triangulation**: Validate findings through multiple sources
- **Complementarity**: Different methods reveal different aspects
- **Development**: One method informs the design of another
- **Expansion**: Extend breadth and range of inquiry
- **Comprehensive**: Numbers provide what/how much, words provide why/how

**Example - Customer Experience Research**:
1. **Survey** (n=1,000): Quantify satisfaction levels and identify problem areas
2. **Interviews** (n=20): Understand why customers are dissatisfied
3. **Observation**: Watch actual customer interactions
4. **Analysis**: Combine all sources for complete picture

**Advantages and Disadvantages Summary**:

Each method has **advantages and disadvantages**, and the **best one will depend on**:
- Nature of the research question
- Context and setting
- Resources available (time, budget, staff)
- Researcher's skills and expertise
- Ethical considerations
- Required depth vs breadth

<div class="mermaid">
flowchart TB
    A[Choose Primary<br/>Data Method] --> B{Research Goals?}
    
    B -->|Large sample,<br/>quantitative| C[Survey<br/>ðŸ“‹]
    B -->|In-depth,<br/>qualitative| D[Interview<br/>ðŸŽ¤]
    B -->|Test causality| E[Experiment<br/>ðŸ”¬]
    B -->|Natural behavior| F[Observation<br/>ðŸ‘ï¸]
    B -->|Group insights| G[Focus Group<br/>ðŸ‘¥]
    
    C --> H[Analysis]
    D --> H
    E --> H
    F --> H
    G --> H
    
    H --> I[Insights for<br/>Decision Making<br/>ðŸ’¡]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style H fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style I fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

### â“ Primary Data Question Types

Surveys and questionnaires, interviews, and focus groups are three commonly used primary data collection methods. The data collector, interviewer, or focus group leader may use a variety of questions during interviews conducted on paper, online, or in person. The type of question used in surveys can impact the calibre of the responses and the kind of analysis you can run.

#### ðŸ“‹ Survey and Questionnaire Question Types

Here are a few examples of typical survey question types and how they influence analysis:

**Multiple choice questions**
- Offer a list of options for respondents to select one or more.
- Simple to analyze quantitatively, but predefined options may miss edge cases.

**Rating scale (Likert scale) questions**
- Use a scale such as 1-5 or 1-7 to measure agreement or satisfaction.
- Common anchors include "strongly disagree" to "strongly agree" or "very dissatisfied" to "very satisfied".

**Rank order questions**
- Ask participants to rank items by importance or preference.
- Useful for relative priorities, but can be demanding with long lists.

**Open-ended questions**
- Let respondents answer in their own words.
- Provide rich detail but require more effort to analyze.

**Dichotomous questions**
- Offer two possible responses, usually "yes" or "no".
- Easy to answer and analyze, but limited depth.

**Demographic questions**
- Capture attributes like age, gender, income, education, or occupation.
- Essential for segmentation and context.

**Matrix questions**
- Present several statements with the same response options.
- Efficient for related items, but can be overwhelming if too large.

**Pictorial questions**
- Use images as response options.
- Engaging for visual topics but can be interpreted differently.

**Semantic differential questions**
- Ask respondents to rate a concept between bipolar adjectives (e.g., "low quality" to "high quality").
- Useful for measuring attitudes, but requires careful adjective choice to avoid bias.

When designing a survey, consider the data you need, how easy the questions are to answer and analyze, and the overall respondent experience. Balancing these factors improves response quality and usability.

#### ðŸŽ¤ Interview and Focus Group Question Types

The purpose of interviews and focus groups is to explore a topic in depth and understand participants' viewpoints, experiences, or attitudes. Open-ended formats are common, but structured prompts help keep sessions focused.

**Open-ended questions**
- Encourage participants to explain in their own words.
- Example: "Can you describe a typical day at your job?"

**Probing questions**
- Follow-up prompts to clarify or expand an answer.
- Example: "Can you tell me more about that experience?"

**Experience or example questions**
- Ask for specific stories or incidents.
- Example: "Can you share a time you handled a challenging situation at work?"

**Hypothetical questions**
- Explore reactions to imagined scenarios.
- Example: "What would you change if you were the manager of this company?"

**Opinion or belief questions**
- Capture attitudes or viewpoints on a topic.
- Example: "What are your opinions on working remotely?"

**Knowledge questions**
- Gauge a participant's familiarity with a topic.
- Example: "What do you know about our sustainability initiative?"

**Sensory questions**
- Explore perceptions related to sensory experience.
- Example: "How would you describe the flavor of our new chocolate bar?"

**Demographic questions**
- Provide context such as age, gender, education, or occupation.
- Often asked at the end to avoid early drop-off.

#### âœ… Question Type Selection Tips (Quick Table)

| Goal | Recommended Question Type | Data Type |
|------|---------------------------|-----------|
| Measure agreement or satisfaction | Likert/rating scale | Quantitative |
| Prioritize options | Rank order | Quantitative |
| Capture detailed explanations | Open-ended | Qualitative |
| Confirm a yes/no condition | Dichotomous | Quantitative |
| Segment respondents | Demographic | Quantitative |
| Compare multiple items quickly | Matrix | Quantitative |
| Assess attitudes between opposites | Semantic differential | Quantitative |
| Explore experiences and context | Open-ended + probing | Qualitative |

**Tip**: Use fewer question types per survey to reduce fatigue, and place sensitive or demographic items near the end.

---

### ðŸŒ Modern Online Primary Data Collection

Modern online environments have **significantly impacted primary data collection techniques**, offering fresh approaches to information gathering, archiving, and analysis. The digital revolution has transformed how researchers collect, store, and analyze data.

<div class="mermaid">
graph TB
    A[Modern Online<br/>Data Collection] --> B[Online Surveys<br/>ðŸ“±<br/>Digital Platforms]
    A --> C[Virtual Meetings<br/>ðŸ’»<br/>Video Conferencing]
    A --> D[Web Analytics<br/>ðŸ“Š<br/>Behavior Tracking]
    A --> E[Online Experiments<br/>ðŸ”¬<br/>A/B Testing]
    A --> F[Crowdsourcing<br/>ðŸ‘¥<br/>Public Participation]
    A --> G[Mobile Collection<br/>ðŸ“²<br/>Smartphone Data]
    
    B --> H[Fresh Approaches to<br/>Data Collection<br/>ðŸŒŸ]
    C --> H
    D --> H
    E --> H
    F --> H
    G --> H
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style F fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style G fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ“± 1. Online Surveys and Questionnaires

The way surveys are conducted has been **revolutionised by digital platforms**. Today, surveys can be distributed via email or shared on social media, which makes it **quicker and less expensive** to reach a larger and more varied audience.

**Digital Survey Platforms**:
- **Google Forms**: Free, user-friendly, integrates with Google Sheets
- **SurveyMonkey**: Professional features, templates, analytics
- **Typeform**: Interactive, engaging user interface
- **Qualtrics**: Advanced research features, enterprise-level
- **Microsoft Forms**: Integrated with Office 365

**Key Benefits**:
- **Speed**: Instant distribution to thousands of participants
- **Cost**: Minimal distribution and processing costs
- **Reach**: Global audience accessibility
- **Automated**: Data entry and analysis automatically handled
- **Reduced Errors**: No manual data entry mistakes
- **Real-time**: Results available immediately
- **Skip Logic**: Dynamic questions based on previous answers
- **Multimedia**: Include images, videos, audio in questions

**Distribution Channels**:
- Email campaigns
- Social media (Facebook, Twitter, LinkedIn)
- Website pop-ups
- QR codes
- SMS text messages
- Embedded forms

**Features**:
- Automated data entry and analysis
- Lower possibility of errors
- Accelerated research process
- Branching logic and conditional questions
- Multi-language support
- Mobile-responsive design

**Example**:
A retail company sends an email survey to 10,000 customers after purchase. Within 24 hours, 2,000 responses are collected and automatically analyzed, revealing satisfaction trends by product category.

<div class="mermaid">
flowchart LR
    A[Create<br/>Survey] --> B[Digital<br/>Platform]
    B --> C[Distribution<br/>ðŸ“§ðŸ“±]
    
    C --> D1[Email]
    C --> D2[Social Media]
    C --> D3[Website]
    C --> D4[QR Code]
    
    D1 --> E[Responses<br/>Collected]
    D2 --> E
    D3 --> E
    D4 --> E
    
    E --> F[Automated<br/>Analysis<br/>ðŸ“Š]
    F --> G[Real-time<br/>Results<br/>âœ…]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ’» 2. Virtual Interviews and Focus Groups

The development of **video conferencing tools** has made it possible to conduct focus groups and interviews online. Due to this, **participants from all over the world can now be included**, and recordings can now be more thoroughly transcribed and analysed.

**Video Conferencing Platforms**:
- **Zoom**: Screen sharing, breakout rooms, recording
- **Microsoft Teams**: Integration with Office, collaborative features
- **Google Meet**: Simple, browser-based, integrated with Calendar
- **Skype**: One-on-one interviews, international calling
- **Cisco Webex**: Enterprise security, large group capacity

**Advantages**:
- **Global Reach**: Include participants worldwide
- **Cost Savings**: No travel expenses or venue rental
- **Convenience**: Participants join from home/office
- **Recording**: Automatic transcription and analysis
- **Screen Sharing**: Present stimuli, documents, prototypes
- **Breakout Rooms**: Divide focus groups for sub-discussions
- **Chat Features**: Backchannel for notes and questions
- **Accessibility**: Options for closed captions, translation

**Considerations**:
- Technical requirements (internet, camera, microphone)
- Digital literacy of participants
- Privacy and security settings
- Time zone coordination
- Non-verbal cues may be harder to read

**Example**:
A global company conducts virtual focus groups with customers in 5 countries simultaneously, using breakout rooms for regional discussions and recording sessions for detailed analysis later.

<div class="mermaid">
graph TB
    A[Virtual Research] --> B[Interviews<br/>1-on-1]
    A --> C[Focus Groups<br/>6-12 people]
    
    B --> D[Video Platform<br/>ðŸ’»]
    C --> D
    
    D --> E[Features]
    
    E --> F[Recording<br/>ðŸ“¹]
    E --> G[Screen Share<br/>ðŸ“Š]
    E --> H[Chat<br/>ðŸ’¬]
    E --> I[Breakout Rooms<br/>ðŸ‘¥]
    
    F --> J[Analysis<br/>Transcription<br/>âœ…]
    G --> J
    H --> J
    I --> J
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style J fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ“Š 3. Web Analytics and Digital Ethnography

**Online behaviour** is now a valuable source of first-hand information. With the user's permission, researchers can track users' online behaviour, including the websites they visit, the links they click, and how long they spend on each page.

**Tracking Methods**:
- **Cookies**: Track user sessions and behavior across visits
- **Server Logs**: Record all server requests and interactions
- **Specialized Software**: Google Analytics, Hotjar, Mixpanel
- **Heatmaps**: Visualize where users click, scroll, move
- **Session Recording**: Replay user interactions
- **Eye Tracking**: Advanced studies of visual attention

**Data Collected**:
- **Page Views**: Which pages users visit
- **Click Behavior**: What links/buttons users click
- **Time on Page**: Duration of engagement
- **Scroll Depth**: How far users scroll down pages
- **Navigation Paths**: User journey through website
- **Conversion Tracking**: Actions completed (purchases, signups)
- **Demographics**: Location, device type, browser

**Research Applications**:
- **Online Consumer Behavior**: Purchase patterns, product browsing
- **Social Media Use**: Engagement, sharing, interactions
- **Information-Seeking Behavior**: Search patterns, content consumption
- **User Experience (UX)**: Website usability, pain points
- **Digital Marketing**: Campaign effectiveness, attribution

**Privacy Considerations**:
- âš ï¸ Requires user consent and permission
- Must comply with GDPR, CCPA regulations
- Clear privacy policies and cookie notices
- Option to opt-out of tracking
- Anonymization of personal data

**Example**:
An e-commerce site uses heatmaps to discover that users consistently miss the "Add to Cart" button. Repositioning it increases conversions by 25%.

<div class="mermaid">
flowchart TD
    A[User Visits<br/>Website] --> B{Consent<br/>Given?}
    
    B -->|Yes| C[Tracking Tools]
    B -->|No| D[No Tracking<br/>Privacy Respected]
    
    C --> E[Cookies<br/>ðŸª]
    C --> F[Server Logs<br/>ðŸ“]
    C --> G[Analytics<br/>ðŸ“Š]
    
    E --> H[Behavior Data]
    F --> H
    G --> H
    
    H --> I[Analysis]
    
    I --> J[Insights:<br/>â€¢ Pages viewed<br/>â€¢ Click patterns<br/>â€¢ Time spent<br/>â€¢ User journey]
    
    J --> K[Improvements<br/>âœ…]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style H fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style J fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style K fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ”¬ 4. Online Experiments (A/B Testing)

**A/B tests**, in which different web page versions are shown to different visitors to see which performs better, have become **simpler to conduct** thanks to platforms like Google Optimize and Optimizely.

**A/B Testing Platforms**:
- **Google Optimize**: Free, integrates with Google Analytics
- **Optimizely**: Enterprise-level, advanced features
- **VWO (Visual Website Optimizer)**: Visual editor, easy setup
- **Adobe Target**: Comprehensive testing and personalization
- **Unbounce**: Landing page optimization

**What Can Be Tested**:
- **Headlines**: Different wording, length, tone
- **Call-to-Action (CTA) Buttons**: Color, text, placement, size
- **Images**: Different visuals, product photos, illustrations
- **Layout**: Page structure, element positioning
- **Copy**: Product descriptions, value propositions
- **Pricing**: Different price points, display formats
- **Forms**: Number of fields, required vs optional

**Process**:
1. **Hypothesis**: "Changing button color to green will increase clicks"
2. **Create Variants**: Version A (blue button) vs Version B (green button)
3. **Split Traffic**: 50% see A, 50% see B
4. **Collect Data**: Track clicks, conversions, time on page
5. **Analyze Results**: Statistical significance testing
6. **Implement Winner**: Roll out better-performing version

**Benefits**:
- **Data-Driven Decisions**: Remove guesswork
- **Quick Results**: Get answers in days or weeks
- **Continuous Improvement**: Iterative optimization
- **Measurable Impact**: Clear ROI on changes
- **Low Risk**: Test before full implementation

**Example**:
An online retailer tests two checkout button colors. The green button achieves a 15% higher conversion rate than the red button, resulting in $50,000 additional monthly revenue.

<div class="mermaid">
graph LR
    A[Original Page<br/>Version A] --> C[Split Traffic<br/>50/50]
    B[Modified Page<br/>Version B] --> C
    
    C --> D[Visitors]
    
    D --> E[Measure Results]
    
    E --> F[Version A:<br/>5% conversion]
    E --> G[Version B:<br/>7% conversion]
    
    F --> H{Winner?}
    G --> H
    
    H -->|Version B<br/>Performs Better| I[Implement<br/>Version B<br/>âœ…]
    
    style A fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style H fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style I fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ‘¥ 5. Crowdsourcing and Citizen Science

The Internet allows researchers to **collect data from the general public on a previously unimaginable scale**.

**Crowdsourcing Platforms**:
- **Amazon Mechanical Turk (MTurk)**: Micro-tasks, surveys, data labeling
- **Prolific**: Research-focused participant pool
- **Zooniverse**: Citizen science projects
- **Figure Eight (Appen)**: Data annotation and collection
- **Crowdflower**: Human-in-the-loop machine learning

**Applications**:

**Academic Research** ðŸŽ“
- Large-scale psychology experiments
- Linguistic studies
- Economic games and decision-making
- Survey research with diverse samples

**Citizen Science Projects** ðŸ”¬
- **Wildlife Studies**: Report bird sightings, animal observations
- **Astronomy**: Classify galaxies, discover exoplanets
- **Environmental Monitoring**: Track pollution, weather patterns
- **Health Research**: Track disease symptoms, medication effects

**Digital Humanities** ðŸ“š
- **Transcription**: Historical document transcription
- **Translation**: Multilingual content translation
- **Categorization**: Image tagging, content classification
- **Digitization**: Converting analog records to digital

**Examples**:
- **eBird**: Millions of bird sightings reported globally by birdwatchers
- **Galaxy Zoo**: Citizens classify galaxy images, contributing to astronomical research
- **Foldit**: Players solve protein-folding puzzles, advancing medical research
- **Old Weather**: Volunteers transcribe historical ship logs for climate research

**Benefits**:
- **Scale**: Collect massive amounts of data
- **Speed**: Parallel processing by many contributors
- **Cost**: Often lower than traditional methods
- **Diversity**: Access varied perspectives and expertise
- **Engagement**: Public participation in science

**Challenges**:
- **Quality Control**: Variable accuracy, need validation
- **Participant Motivation**: Ensuring engagement
- **Data Consistency**: Standardizing contributions
- **Attribution**: Crediting contributors appropriately

<div class="mermaid">
graph TB
    A[Crowdsourcing<br/>Platform] --> B[Research Task]
    
    B --> C1[Thousands of<br/>Contributors<br/>ðŸŒ]
    
    C1 --> D1[Participant 1<br/>Contributes Data]
    C1 --> D2[Participant 2<br/>Contributes Data]
    C1 --> D3[Participant 3<br/>Contributes Data]
    C1 --> D4[... Participant N<br/>Contributes Data]
    
    D1 --> E[Aggregated<br/>Dataset]
    D2 --> E
    D3 --> E
    D4 --> E
    
    E --> F[Quality Control<br/>& Validation]
    
    F --> G[Research<br/>Insights<br/>ðŸ’¡]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ“² 6. Mobile Data Collection

Due to the **widespread use of smartphones**, data collection has become **quicker and more effective**. Various fields, including social science, health research, environmental studies, and more, use this technology.

**Mobile Data Types**:

**GPS Location Data** ðŸ“
- Track movement patterns
- Map user journeys
- Location-based services
- Geofencing studies
- Travel behavior research

**Images and Videos** ðŸ“¸
- Photo documentation
- Visual diaries
- Product usage evidence
- Environmental observations
- Medical imaging (symptom tracking)

**Real-Time Responses** â±ï¸
- Experience sampling (ESM)
- Momentary assessments
- Ecological momentary assessment (EMA)
- Real-time mood tracking
- Activity logging

**Sensor Data** ðŸ“Š
- Accelerometer (movement, activity)
- Gyroscope (orientation)
- Proximity sensors
- Light sensors
- Biometric sensors (heart rate, steps)

**Applications by Field**:

**Social Science** ðŸ‘¥
- Time-use studies
- Social interaction patterns
- Communication behavior
- Daily activities tracking

**Health Research** ðŸ¥
- Medication adherence
- Symptom tracking
- Physical activity monitoring
- Mental health assessments
- Disease surveillance

**Environmental Studies** ðŸŒ³
- Air quality monitoring
- Noise pollution mapping
- Wildlife observations
- Citizen science data
- Climate change tracking

**Market Research** ðŸ’¼
- Shopping behavior
- Product usage patterns
- In-store experiences
- Receipt scanning
- Mobile surveys at point of purchase

**Mobile Survey Tools**:
- **SurveyMonkey Mobile**: Native apps for iOS/Android
- **Qualtrics Mobile**: Offline data collection
- **ODK (Open Data Kit)**: Open-source, works offline
- **KoBoToolbox**: Humanitarian and development research
- **CommCare**: Health and social services data

**Benefits**:
- **Real-Time**: Capture data as events occur
- **Context-Aware**: Location and environmental data
- **Multimedia**: Photos, audio, video capabilities
- **Always Available**: People carry phones constantly
- **Offline Capability**: Collect data without internet
- **Reduced Recall Bias**: Capture in-the-moment experiences

**Example**:
A health study uses a mobile app to prompt participants five times daily to report mood, stress levels, and current activity. GPS data correlates mood with locations, revealing that time in nature improves well-being.

<div class="mermaid">
flowchart TD
    A[Smartphone<br/>ðŸ“±] --> B[Data Collection<br/>Capabilities]
    
    B --> C1[GPS<br/>ðŸ“<br/>Location]
    B --> C2[Camera<br/>ðŸ“¸<br/>Images/Video]
    B --> C3[Sensors<br/>ðŸ“Š<br/>Movement/Bio]
    B --> C4[Surveys<br/>ðŸ“‹<br/>Real-time]
    
    C1 --> D[Research<br/>Applications]
    C2 --> D
    C3 --> D
    C4 --> D
    
    D --> E1[Social<br/>Science]
    D --> E2[Health<br/>Research]
    D --> E3[Environmental<br/>Studies]
    D --> E4[Market<br/>Research]
    
    E1 --> F[Quick &<br/>Effective<br/>Data Collection<br/>âœ…]
    E2 --> F
    E3 --> F
    E4 --> F
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

### âš ï¸ Challenges and Ethical Considerations of Online Data Collection

But these developments also bring **new difficulties and moral dilemmas** that researchers must carefully address.

<div class="mermaid">
graph TB
    A[Online Data<br/>Collection<br/>Challenges] --> B[Privacy &<br/>Security<br/>ðŸ”’]
    A --> C[Digital Divide<br/>& Bias<br/>âš–ï¸]
    A --> D[Data Quality<br/>ðŸ“Š]
    A --> E[Technical<br/>Issues<br/>ðŸ’»]
    
    B --> F[Mitigation<br/>Strategies]
    C --> F
    D --> F
    E --> F
    
    style A fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style E fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style F fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
</div>

#### ðŸ”’ 1. Privacy and Data Security Issues

**Online data collection raises significant privacy and data security issues.**

**Key Concerns**:

**Personal Data Protection** ðŸ›¡ï¸
- Collection of sensitive information
- Risk of data breaches and hacks
- Unauthorized access to participant data
- Third-party data sharing
- Data retention and deletion

**Surveillance Concerns** ðŸ‘ï¸
- Tracking online behavior
- Location monitoring
- Continuous data collection
- Scope creep (collecting more than necessary)
- Government or corporate surveillance

**Best Practices**:

âœ… **Informed Consent**
- Clear explanation of data collection
- Specific purposes stated
- Right to withdraw explained
- Consent obtained before collection
- Language accessible to participants

âœ… **Data Anonymization**
- Remove personally identifiable information (PII)
- Use pseudonyms or participant IDs
- Aggregate data where possible
- De-identification techniques
- Prevent re-identification risks

âœ… **Secure Data Storage**
- Encryption at rest and in transit
- Secure servers and databases
- Access controls and authentication
- Regular security audits
- Backup and recovery procedures

âœ… **Compliance with Regulations**
- **GDPR** (General Data Protection Regulation - EU)
- **CCPA** (California Consumer Privacy Act - USA)
- **HIPAA** (Health Insurance Portability and Accountability Act - healthcare)
- **COPPA** (Children's Online Privacy Protection Act - under 13)
- Local data protection laws

**Example of Good Practice**:
A health research app collects symptom data with clear consent forms, encrypts all data, stores it on HIPAA-compliant servers, and automatically deletes participant data after study completion.

#### âš–ï¸ 2. Digital Divide and Sampling Bias

**Online data can be biased** because it frequently underrepresents some demographic groups and overrepresents others.

**Overrepresented Groups** âœ… (Online):
- Younger individuals (18-45 years)
- More educated individuals (college/university)
- Wealthier individuals (higher income)
- Urban and suburban residents
- Tech-savvy populations
- Developed countries

**Underrepresented Groups** âŒ (Online):
- Older adults (65+ years)
- Less-educated individuals
- Low-income individuals
- Rural residents
- People with disabilities
- Developing countries
- Non-native language speakers

**Consequences**:
- **Skewed Results**: Don't represent general population
- **Limited Generalizability**: Findings may not apply broadly
- **Exclusion**: Voices of marginalized groups not heard
- **Policy Implications**: Decisions based on incomplete data
- **Digital Inequality**: Reinforces existing disparities

**Mitigation Strategies**:

ðŸ”§ **Mixed-Mode Data Collection**
- Combine online with phone, mail, in-person
- Offer multiple participation options
- Reach those without internet access

ðŸ”§ **Targeted Recruitment**
- Actively recruit underrepresented groups
- Partner with community organizations
- Provide devices and internet access if needed
- Offer incentives for participation

ðŸ”§ **Accessibility Features**
- Screen reader compatibility
- Multiple language options
- Simple, clear interfaces
- Mobile-responsive design
- Low-bandwidth options

ðŸ”§ **Statistical Weighting**
- Weight responses to match population demographics
- Post-stratification adjustments
- Acknowledge limitations in reporting

**Example**:
A government survey combines online responses with telephone interviews and mailed paper surveys to ensure representation across all age groups and socioeconomic levels.

#### ðŸ“Š 3. Data Quality Concerns

**Online Data Quality Issues**:
- **Bot Responses**: Automated fake responses
- **Satisficing**: Rushed, low-effort responses
- **Multiple Submissions**: Same person responding multiple times
- **Attention Failures**: Not reading questions carefully
- **Technical Errors**: Survey glitches, incomplete submissions

**Quality Control Measures**:
- CAPTCHA verification
- Attention check questions
- Response time monitoring
- IP address tracking
- Data validation rules
- Duplicate detection

#### ðŸ’» 4. Technical Issues

**Common Technical Challenges**:
- Internet connectivity problems
- Device compatibility issues
- Browser incompatibilities
- Software bugs and glitches
- Data transmission errors
- Platform downtime

**Solutions**:
- Responsive design for all devices
- Cross-browser testing
- Offline data collection options
- Automatic save features
- Technical support availability
- Pilot testing before launch

<div class="mermaid">
flowchart LR
    A[Online Data<br/>Collection] --> B{Ethical<br/>Review}
    
    B --> C1[âœ… Privacy<br/>Protected]
    B --> C2[âœ… Informed<br/>Consent]
    B --> C3[âœ… Secure<br/>Storage]
    B --> C4[âœ… Representative<br/>Sample]
    
    C1 --> D[Proceed with<br/>Collection]
    C2 --> D
    C3 --> D
    C4 --> D
    
    D --> E[Quality<br/>Data<br/>ðŸ’Ž]
    
    B -->|âŒ Issues| F[Address<br/>Concerns]
    F --> B
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style C1 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C2 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C3 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C4 fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    style F fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
</div>

---

### ðŸ”‘ Key Benefits vs Challenges Summary

| Aspect | Benefits âœ… | Challenges âš ï¸ |
|--------|------------|---------------|
| **Reach** | Global audience, large samples | Digital divide, sampling bias |
| **Cost** | Lower than traditional methods | May exclude those without internet |
| **Speed** | Real-time data, quick deployment | Technical issues, connectivity problems |
| **Convenience** | Participants respond anytime, anywhere | Requires devices and digital literacy |
| **Analysis** | Automated, immediate results | Data quality concerns (bots, satisficing) |
| **Privacy** | Anonymous response options | Security risks, tracking concerns |
| **Flexibility** | Multimedia, interactive features | Platform compatibility issues |
| **Scale** | Crowdsourcing massive data collection | Quality control at scale difficult |

---

### ðŸ› ï¸ Primary Data Collection Platforms

Although the **pen-and-paper method is still an option**, collecting data online can be **much more efficient**. Surveys and questionnaires are some of the most popular methods for gathering data online. Many existing online tools have been specifically designed for this purpose, so it **may not be necessary for some organisations to create their own tools**.

<div class="mermaid">
graph TB
    A[Primary Data<br/>Collection Platforms] --> B[Survey Tools<br/>ðŸ“‹<br/>Online Questionnaires]
    A --> C[Video Conferencing<br/>ðŸ’»<br/>Interviews & Focus Groups]
    A --> D[Web Analytics<br/>ðŸ“Š<br/>Behavior Tracking]
    
    B --> B1[Google Forms<br/>SurveyMonkey<br/>Qualtrics]
    C --> C1[Zoom<br/>Teams<br/>Google Meet]
    D --> D1[Google Analytics<br/>Hotjar<br/>Mixpanel]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style B1 fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style C1 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style D1 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
</div>

---

#### ðŸ“‹ Online Survey Platforms

These platforms allow you to create, distribute, and analyze surveys efficiently. Each offers different features and pricing models to suit various needs.

##### 1. Google Forms ðŸŸ¢

**Description**: A free tool included in the Google online application suite.

**Key Features**:
- **Simple to use**: Intuitive drag-and-drop interface
- **Question Types**: Multiple choice, drop-down, linear scale, checkboxes, short answer, paragraph, file upload, date/time
- **Real-time Collaboration**: Multiple users can edit simultaneously
- **Integration**: Responses compiled in Google Sheets for analysis
- **Templates**: Pre-built forms for common use cases
- **Free**: No cost for unlimited surveys and responses

**Best For**: Quick surveys, simple questionnaires, event registrations, feedback forms

**Pricing**: Free

**Website**: https://www.google.com/forms/about/

##### 2. SurveyMonkey ðŸµ

**Description**: One of the most popular survey tools available online with extensive customization options.

**Key Features**:
- **Customization**: Customize survey's look and feel (colors, logos, themes)
- **Question Types**: Wide variety including matrix, ranking, slider, A/B testing
- **Logic and Branching**: Skip logic, piping, randomization
- **Analysis Tools**: Built-in analytics, cross-tabulations, filtering
- **Distribution**: Email, web link, social media, website embed
- **Templates**: 200+ expert-designed survey templates

**Pricing**:
- **Basic**: Free (limited to 10 questions, 40 responses per survey)
- **Paid Plans**: More sophisticated features available with subscription

**Best For**: Professional surveys, market research, customer satisfaction, employee engagement

**Website**: https://www.surveymonkey.com/

##### 3. Qualtrics ðŸ”¬

**Description**: Feature-rich and complex survey tool frequently used in academic, market research, and business settings.

**Key Features**:
- **Advanced Question Types**: Extensive library of question formats
- **Complex Branching**: Intricate logic and skip patterns
- **Powerful Analysis**: Statistical analysis, text analytics, predictive intelligence
- **Enterprise Features**: API access, single sign-on, advanced security
- **Multi-language**: Support for 100+ languages
- **Academic License**: Special pricing for universities

**Pricing**:
- **Paid Service**: Enterprise pricing (custom quotes)
- **Trial**: Free trial period available
- **Academic**: Discounted rates for educational institutions

**Best For**: Academic research, large-scale enterprise surveys, complex market research

**Website**: https://www.qualtrics.com/uk/lp/uk-ppc-experience-management/

##### 4. Typeform ðŸŽ¨

**Description**: Tool renowned for its distinctive, aesthetically pleasing interface that creates engaging user experiences.

**Key Features**:
- **Unique Interface**: One-question-at-a-time format
- **Conversational Feel**: Feels like a conversation, not a survey
- **Beautiful Design**: Modern, visually appealing templates
- **Logic Jumps**: Smart conditional logic based on answers
- **Integrations**: Connects with 100+ apps (Google Sheets, Mailchimp, Slack)
- **Analytics**: Track completion rates, average time, drop-off points

**Pricing**:
- **Basic**: Free (limited features, 10 questions, 100 responses/month)
- **Paid Plans**: More sophisticated features available

**Best For**: Engaging surveys, lead generation, customer feedback, creative questionnaires

**Website**: https://www.typeform.com/

##### 5. Microsoft Forms ðŸ“Š

**Description**: Component of Microsoft Office 365, comparable to Google Forms with seamless Microsoft integration.

**Key Features**:
- **Office 365 Integration**: Works with Excel, Teams, SharePoint
- **Question Types**: Multiple choice, text, rating, date, ranking, Likert
- **Branching**: Basic skip logic and sections
- **Quizzes**: Create tests with automatic grading
- **Collaboration**: Share with colleagues for co-editing
- **Responses**: Collected in Excel workbook

**Pricing**: Included with Microsoft Office 365 subscription (or free basic version)

**Best For**: Organizations using Microsoft ecosystem, internal surveys, quizzes, polls

**Website**: https://forms.office.com/

##### 6. Zoho Survey ðŸ”¶

**Description**: Part of the Zoho business app suite with comprehensive features and easy sharing.

**Key Features**:
- **Zoho Ecosystem**: Integrates with other Zoho apps (CRM, Analytics)
- **Question Types**: 25+ question types available
- **Multi-language**: Support for multiple languages
- **Offline Surveys**: Collect data without internet connection
- **Custom Themes**: Brand surveys with your colors and logo
- **Analytics Dashboard**: Real-time response tracking

**Pricing**:
- **Free Basic Version**: Limited features
- **Paid Plans**: Advanced features available

**Best For**: Businesses using Zoho suite, multi-platform sharing, offline data collection

**Website**: https://www.zoho.com/survey/

##### 7. QuestionPro ðŸ“

**Description**: Reliable tool offering wide range of features for creating, disseminating, and analysing surveys.

**Key Features**:
- **Comprehensive Features**: Survey creation, distribution, analysis
- **Question Library**: 100+ question types and templates
- **Complex Logic**: Advanced branching, piping, quotas
- **Multi-language**: Support for 100+ languages
- **Offline Data Collection**: Mobile app for offline surveys
- **Panel Management**: Build and manage respondent panels
- **Advanced Analytics**: Statistical analysis, sentiment analysis, text analytics

**Pricing**:
- **Free Basic Plan**: Limited features
- **Premium Plans**: Advanced features (offline collection, panel management, advanced analytics)

**Best For**: Complex research projects, panel management, offline data collection, enterprise surveys

**Website**: https://www.questionpro.com/

---

#### ðŸ“Š Survey Platform Comparison Table

| Platform | Pricing | Best For | Key Strength | Integration |
|----------|---------|----------|--------------|-------------|
| **Google Forms** | Free | Quick surveys, simple needs | Ease of use, Google ecosystem | Google Workspace |
| **SurveyMonkey** | Free + Paid | Professional surveys, market research | Wide adoption, templates | 100+ apps |
| **Qualtrics** | Paid | Academic, enterprise research | Advanced analytics, complexity | Enterprise systems |
| **Typeform** | Free + Paid | Engaging experiences | Beautiful design, UX | 100+ apps |
| **Microsoft Forms** | Free/O365 | Microsoft users, quizzes | Office 365 integration | Microsoft ecosystem |
| **Zoho Survey** | Free + Paid | Zoho users, offline collection | Zoho ecosystem | Zoho apps |
| **QuestionPro** | Free + Paid | Complex research, panels | Advanced features | Multiple platforms |

---

#### ðŸ’» Video Conferencing Platforms

Online focus groups and interviews can be conducted using various platforms that enable simultaneous communication with multiple participants.

##### 1. Zoom ðŸ”µ

**Description**: Well-liked video conferencing platform commonly used for focus groups, interviews, and remote meetings.

**Key Features**:
- **Screen Sharing**: Present materials, prototypes, stimuli
- **Recording**: Record sessions for later transcription and analysis
- **Breakout Rooms**: Create smaller discussion groups from larger focus groups
- **Chat**: Text messaging during sessions
- **Virtual Backgrounds**: Professional appearance
- **Whiteboard**: Collaborative visual brainstorming
- **Polling**: Quick surveys during sessions
- **Capacity**: Up to 1,000 participants (with large meeting add-on)

**Pricing**:
- **Free**: 40-minute limit for group meetings
- **Paid Plans**: Unlimited duration, more features

**Best For**: Focus groups, one-on-one interviews, large webinars, breakout discussions

**Website**: https://zoom.us/

##### 2. Microsoft Teams ðŸŸ¦

**Description**: Collaboration platform supporting video conferencing, part of Microsoft 365 product line.

**Key Features**:
- **Video Conferencing**: HD video and audio
- **Screen Sharing**: Share entire screen or specific windows
- **Breakout Rooms**: Divide into smaller groups
- **Recording**: Meeting recordings saved to OneDrive/SharePoint
- **Chat**: Persistent chat before/during/after meetings
- **Files**: Share and collaborate on documents during calls
- **Integration**: Deep integration with Office 365 apps
- **Live Captions**: Real-time transcription

**Pricing**: Included with Microsoft 365 subscription (or free basic version)

**Best For**: Organizations using Microsoft ecosystem, ongoing collaboration, file sharing

**Website**: https://www.microsoft.com/en-za/microsoft-teams/log-in

##### 3. Google Meet ðŸŸ¢

**Description**: Video conferencing platform part of Google Workspace with seamless Google integration.

**Key Features**:
- **Large Meeting Support**: Up to 250 participants (with enterprise plan)
- **Screen Sharing**: Present to all participants
- **Recording**: Save meetings to Google Drive
- **Live Captions**: Automatic speech-to-text (English)
- **Google Calendar**: Integrated scheduling
- **Breakout Rooms**: Create up to 100 breakout rooms
- **Polls and Q&A**: Engage participants
- **Mobile Apps**: Join from anywhere

**Pricing**:
- **Free**: 60-minute limit for group meetings
- **Paid**: Google Workspace subscription

**Best For**: Google Workspace users, projects using Google services, simple video calls

**Website**: https://workspace.google.com/intl/en/lp/meet/

##### 4. Cisco Webex ðŸ”·

**Description**: Video conferencing platform frequently used in business settings with enterprise-grade security.

**Key Features**:
- **Large Meetings**: Support for up to 100,000 participants (webinar mode)
- **Breakout Sessions**: Divide participants into groups
- **Screen Sharing**: Share screen, applications, or files
- **Recording**: Cloud or local recording options
- **Whiteboard**: Collaborative brainstorming
- **Polls and Q&A**: Interactive engagement
- **End-to-End Encryption**: Enterprise security
- **AI Features**: Real-time transcription, noise removal

**Pricing**:
- **Free**: Basic features
- **Paid Plans**: Advanced features and capacity

**Best For**: Enterprise environments, high-security requirements, large webinars

**Website**: https://www.webex.com/

##### 5. GoTo Meeting ðŸŸ§

**Description**: Video conferencing service with high-quality audio and video, ideal for professional meetings.

**Key Features**:
- **High-Quality Audio/Video**: Crystal-clear communication
- **Screen Sharing**: Share screen or specific applications
- **Recording**: Cloud recording with transcription
- **Mobile Access**: Full-featured mobile apps
- **Drawing Tools**: Annotate shared screens
- **Keyboard and Mouse Sharing**: Remote control capabilities
- **Meeting Lock**: Secure meetings from late joiners
- **Personal Meeting Room**: Consistent meeting URL

**Pricing**: Paid plans (14-day free trial available)

**Best For**: Professional interviews, sales presentations, client meetings

**Website**: https://www.goto.com/meeting

---

#### ðŸŽ¥ Video Platform Comparison Table

| Platform | Max Participants (Free) | Breakout Rooms | Recording | Best For |
|----------|------------------------|----------------|-----------|----------|
| **Zoom** | 100 (40 min limit) | âœ… Yes | âœ… Yes | Focus groups, versatility |
| **Microsoft Teams** | 100 (60 min limit) | âœ… Yes | âœ… Yes (to OneDrive) | Microsoft 365 users |
| **Google Meet** | 100 (60 min limit) | âœ… Yes | âœ… Yes (to Drive) | Google Workspace users |
| **Cisco Webex** | 100 (50 min limit) | âœ… Yes | âœ… Yes | Enterprise security |
| **GoTo Meeting** | N/A (paid only) | âŒ No | âœ… Yes | Professional meetings |

---

#### ðŸ“Š Web Analytics Platforms

Web analytics tools are a large group of platforms for gathering, analysing, and reporting on user interactions with websites. Understanding user behaviour and improving the user experience on a website depends greatly on these platforms.

##### 1. Google Analytics ðŸ“ˆ

**Description**: Arguably the most well-liked web analytics tool on the market, mainly because of its extensive feature set and free entry point.

**Key Features**:
- **Comprehensive Statistics**: Website traffic, traffic sources, user demographics
- **Conversion Tracking**: Measure sales, signups, downloads
- **E-commerce Tracking**: Revenue, transactions, product performance
- **Real-time Data**: Current visitors, active pages, traffic sources
- **Custom Reports**: Build reports tailored to your needs
- **Goals and Funnels**: Track conversion paths
- **Integration**: Works with Google Ads, Search Console, other Google tools
- **Mobile App Analytics**: Track app usage

**Pricing**: Free (with Google Analytics 360 for enterprise needs)

**Best For**: Most websites, comprehensive free analytics, Google ecosystem integration

**Website**: https://analytics.google.com/analytics/web/#/

##### 2. Adobe Analytics ðŸ’¼

**Description**: Powerful, enterprise-level analytics tool part of Adobe Experience Cloud.

**Key Features**:
- **Multi-channel Data Collection**: Web, mobile, IoT, offline
- **Real-time Data**: Instant insights and alerts
- **AI and Machine Learning**: Adobe Sensei for predictive analytics
- **Advanced Segmentation**: Deep customer segmentation
- **Attribution Analysis**: Multi-touch attribution models
- **Predictive Analytics**: Forecast trends and behaviors
- **Data Warehouse**: Store and analyze historical data
- **Custom Variables**: Unlimited custom dimensions

**Pricing**: Enterprise pricing (custom quotes)

**Best For**: Large enterprises, complex analytics needs, Adobe ecosystem users

**Website**: https://business.adobe.com/products/analytics/adobe-analytics.html

##### 3. Mixpanel ðŸ”¬

**Description**: User-centric data tool tracking user interactions with web and mobile applications.

**Key Features**:
- **Event-Based Tracking**: Track specific user actions
- **User Profiles**: Individual user journey tracking
- **Funnel Analysis**: Conversion funnel visualization
- **Retention Reports**: User engagement over time
- **Cohort Analysis**: Group users by shared characteristics
- **A/B Testing**: Experiment tracking
- **Targeted Communication**: Send messages to specific user segments
- **User-Friendly Design**: Intuitive interface

**Pricing**:
- **Free Plan**: Up to 100,000 monthly tracked users
- **Paid Plans**: More users and features

**Best For**: Product analytics, SaaS applications, mobile apps, user behavior tracking

**Website**: https://mixpanel.com/

##### 4. Heap ðŸ“¦

**Description**: Automatically records every web, mobile, and cloud interaction without needing to define events beforehand.

**Key Features**:
- **Auto-Capture**: Automatically captures all events (clicks, submits, transactions, etc.)
- **Retroactive Analysis**: Analyze past data without prior event definition
- **No Code Required**: No need for manual event tracking code
- **User Sessions**: Replay individual user sessions
- **Conversion Funnels**: Analyze user paths
- **Event Visualizer**: Point-and-click event definition
- **Data Science Tools**: SQL access, Python/R integration

**Pricing**:
- **Free Plan**: Limited features
- **Paid Plans**: Full feature access

**Best For**: Teams without developers, retroactive analysis, complete data capture

**Website**: https://www.heap.io/

##### 5. Matomo (formerly Piwik) ðŸ”“

**Description**: Open-source web analytics platform offering powerful analytics with full data ownership and privacy.

**Key Features**:
- **Open Source**: Self-hosted option for complete data control
- **Privacy-Focused**: GDPR, CCPA compliant by design
- **Full Data Ownership**: Your data stays on your servers
- **Comprehensive Analytics**: Similar features to Google Analytics
- **Heatmaps and Session Recording**: Visual behavior analysis
- **Custom Reports**: Unlimited custom dimensions and metrics
- **No Data Sampling**: Analyze 100% of your data
- **No Data Limits**: Track unlimited websites and users

**Pricing**:
- **Free**: Self-hosted open-source version
- **Cloud**: Paid cloud hosting option available

**Best For**: Privacy-conscious organizations, data sovereignty requirements, unlimited data

**Website**: https://matomo.org/

##### 6. Clicky âš¡

**Description**: All-inclusive web analytics tool that tracks and monitors website traffic in real time.

**Key Features**:
- **Real-time Analytics**: See visitors as they browse
- **Heatmaps**: Visual click analytics
- **Uptime Monitoring**: Site availability tracking
- **Individual Visitor Tracking**: Detailed visitor profiles
- **Twitter Search**: Monitor Twitter mentions
- **Campaign Tracking**: UTM and custom parameters
- **Goals and Conversions**: Track important actions
- **Simple Interface**: Easy to understand reports

**Pricing**:
- **Free Plan**: Up to 3,000 daily page views
- **Paid Plans**: Higher limits and more features

**Best For**: Real-time monitoring, small to medium websites, simple analytics

**Website**: https://clicky.com/

##### 7. Hotjar ðŸ”¥

**Description**: User behaviour analytics tool providing heatmaps, session recordings, and survey tools.

**Key Features**:
- **Heatmaps**: Click, move, scroll heatmaps
- **Session Recordings**: Watch user interactions
- **Conversion Funnels**: Identify drop-off points
- **Form Analysis**: See where users abandon forms
- **Feedback Polls**: On-site user surveys
- **Incoming Feedback**: Users can report issues
- **Recruit Users**: Find participants for user testing
- **Qualitative Insights**: Understand the "why" behind the data

**Pricing**:
- **Free Plan**: Basic features, limited data
- **Paid Plans**: More recordings, unlimited heatmaps

**Best For**: UX research, qualitative insights, used with another analytics tool

**Website**: https://www.hotjar.com/behavior-analytics-software2/

##### 8. Crazy Egg ðŸ¥š

**Description**: Visual analytics tool using heatmaps, scroll maps, and other visual reports to understand user interaction.

**Key Features**:
- **Heatmaps**: Click and attention heatmaps
- **Scroll Maps**: See how far users scroll
- **Confetti Reports**: Segment clicks by traffic source
- **Overlay Reports**: Click data directly on your page
- **Session Recordings**: Watch user sessions
- **A/B Testing**: Test page variations
- **User Surveys**: On-site feedback collection
- **Error Tracking**: Identify technical issues

**Pricing**: Paid plans (30-day free trial)

**Best For**: Visual analytics, page optimization, conversion improvement

**Website**: https://www.crazyegg.com/

---

#### ðŸ”¬ Web Analytics Platform Comparison Table

| Platform | Pricing | Focus | Key Strength | Best For |
|----------|---------|-------|--------------|----------|
| **Google Analytics** | Free + Enterprise | Comprehensive | Free, powerful, widely used | Most websites |
| **Adobe Analytics** | Enterprise | Enterprise-level | AI/ML, predictive analytics | Large enterprises |
| **Mixpanel** | Free + Paid | Product/User | User-centric, event tracking | SaaS, mobile apps |
| **Heap** | Free + Paid | Auto-capture | Retroactive analysis | No-code analytics |
| **Matomo** | Free (self-hosted) + Cloud | Privacy | Data ownership, open-source | Privacy-conscious |
| **Clicky** | Free + Paid | Real-time | Live monitoring | Real-time insights |
| **Hotjar** | Free + Paid | UX/Qualitative | Heatmaps, recordings | User experience |
| **Crazy Egg** | Paid | Visual | Visual reports | Page optimization |

---

### ðŸ’¡ Choosing the Right Platform

<div class="mermaid">
flowchart TD
    A[Choose Platform] --> B{Purpose?}
    
    B -->|Survey| C{Budget &<br/>Features?}
    B -->|Video Meeting| D{Ecosystem?}
    B -->|Web Analytics| E{Privacy &<br/>Control?}
    
    C -->|Free, Simple| C1[Google Forms<br/>Typeform Free]
    C -->|Professional| C2[SurveyMonkey<br/>Qualtrics]
    
    D -->|Google| D1[Google Meet]
    D -->|Microsoft| D2[Teams]
    D -->|Neutral| D3[Zoom]
    
    E -->|Full Control| E1[Matomo<br/>Self-hosted]
    E -->|Managed| E2[Google Analytics<br/>Mixpanel]
    E -->|Visual UX| E3[Hotjar<br/>Crazy Egg]
    
    C1 --> F[Start Collecting!]
    C2 --> F
    D1 --> F
    D2 --> F
    D3 --> F
    E1 --> F
    E2 --> F
    E3 --> F
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style F fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

**Selection Criteria**:

âœ… **Budget**: Free vs paid features needed  
âœ… **Existing Ecosystem**: Integration with current tools  
âœ… **Technical Expertise**: Ease of use vs advanced features  
âœ… **Data Privacy**: Control and ownership requirements  
âœ… **Scale**: Number of responses/participants needed  
âœ… **Features**: Specific capabilities required  
âœ… **Support**: Level of customer support needed

---

#### âœ… Advantages of Primary Data

One of its main advantages is that primary data is **specifically targeted to the research question at hand** and can offer **in-depth, accurate information** that is directly applicable to the project.

**Key Advantages**:

**1. Specificity** ðŸŽ¯
- Designed exactly for your research question
- Addresses specific information needs
- No irrelevant data collection
- Perfect fit for research objectives

**2. Accuracy** âœ“
- Collected using controlled methods
- First-hand verification possible
- Known data quality and reliability
- Researcher controls quality assurance

**3. Currency** ðŸ“…
- Most up-to-date information
- Reflects current conditions
- No time lag between collection and use
- Relevant to present context

**4. Ownership** ðŸ”‘
- Exclusive access to unique data
- Competitive advantage
- Control over data usage
- Intellectual property potential

**5. Control** âš™ï¸
- Choose collection methods
- Design questions/instruments
- Select sample and timing
- Ensure consistency

<div class="mermaid">
graph TB
    A[Primary Data<br/>Advantages] --> B[Specificity<br/>ðŸŽ¯<br/>Exact Fit]
    A --> C[Accuracy<br/>âœ“<br/>High Quality]
    A --> D[Currency<br/>ðŸ“…<br/>Up-to-date]
    A --> E[Ownership<br/>ðŸ”‘<br/>Exclusive]
    A --> F[Control<br/>âš™ï¸<br/>Full Control]
    
    B --> G[Better Research<br/>Outcomes<br/>ðŸ“ˆ]
    C --> G
    D --> G
    E --> G
    F --> G
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### âŒ Disadvantages of Primary Data

However, gathering primary data can **take a while and be costly**, especially if it calls for extensive surveys or challenging experiments.

**Key Disadvantages**:

**1. Time-Consuming** â±ï¸
- Designing research instruments
- Recruiting participants
- Conducting data collection
- Processing and cleaning data
- Analyzing results

**2. Expensive** ðŸ’°
- Staff and researcher costs
- Participant incentives/compensation
- Equipment and materials
- Travel and logistics
- Technology platforms

**3. Limited Scope** ðŸ“
- Budget constraints limit sample size
- Time constraints limit depth
- May not be comprehensive
- Trade-offs between breadth and depth

**4. Expertise Required** ðŸŽ“
- Proper methodology design
- Valid sampling techniques
- Bias mitigation strategies
- Statistical analysis skills
- Ethical considerations

**5. Potential Bias** âš ï¸
- Researcher bias in design
- Selection bias in sampling
- Response bias from participants
- Observer effects
- Interpretation bias

<div class="mermaid">
graph TB
    A[Primary Data<br/>Disadvantages] --> B[Time-Consuming<br/>â±ï¸<br/>Long Process]
    A --> C[Expensive<br/>ðŸ’°<br/>High Cost]
    A --> D[Limited Scope<br/>ðŸ“<br/>Constraints]
    A --> E[Expertise Required<br/>ðŸŽ“<br/>Skills Needed]
    A --> F[Potential Bias<br/>âš ï¸<br/>Various Biases]
    
    B --> G[Consider<br/>Trade-offs<br/>âš–ï¸]
    C --> G
    D --> G
    E --> G
    F --> G
    
    G --> H[Primary Data<br/>Worth It?]
    
    style A fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style H fill:#FFD700,stroke:#B8860B,stroke-width:2px
</div>

---

### ðŸ“š Secondary Data (Second-Order Data)

On the other hand, **secondary data**, also called **second-order data**, is information that another party gathers for a different objective. This can include information obtained from company records, information obtained from government databases, information obtained from other researchers (which may have been published in books, articles, or reports), etc.

#### ðŸŽ¨ Secondary Data Sources

<div class="mermaid">
graph TB
    A[Secondary Data<br/>Sources] --> B[Internal<br/>ðŸ’¼<br/>Company Records]
    A --> C[Government<br/>ðŸ›ï¸<br/>Public Databases]
    A --> D[Commercial<br/>ðŸ’µ<br/>Market Research]
    A --> E[Academic<br/>ðŸŽ“<br/>Research Publications]
    A --> F[Media<br/>ðŸ“°<br/>News & Reports]
    
    B --> B1[Sales records<br/>Financial data<br/>HR records]
    C --> C1[Census data<br/>Economic indicators<br/>Health statistics]
    D --> D1[Industry reports<br/>Market analysis<br/>Consumer data]
    E --> E1[Journal articles<br/>Studies<br/>Dissertations]
    F --> F1[News articles<br/>Industry publications<br/>Online content]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
</div>

#### ðŸ§­ Secondary Data Collection Methods

Using data already collected by others is known as **secondary data collection**. It is often **time- and cost-efficient**, but it requires careful evaluation of quality and relevance.

**Common Methods**:

**Public records**
- Birth and death certificates, marriage licenses, and other official records.
- Useful for demographic research and longitudinal trends.

**Statistical data**
- Data published by government agencies, international organizations, and NGOs.
- Includes social trends, economic indicators, health statistics, and more.

**Literature reviews**
- Academic books, theses, dissertations, and journal articles.
- Summarize prior research findings and methodologies on a topic.

**Online databases**
- Open or subscription databases for market research, legal cases, legislation, and scientific research.
- Efficient for cross-domain evidence gathering.

**Archives**
- Historical records, corporate records, and document collections.
- Can be physical (libraries, museums) or digital (newspaper archives).

**Internal business data**
- Sales records, CRM data, support logs, and website analytics.
- Enables secondary analysis for market or competitive insights.

**Social media data**
- Platform data reflecting consumer behavior, trends, and sentiment.
- Useful for brand monitoring and public opinion analysis.

**Data aggregators**
- Organizations that compile datasets from multiple sources for purchase or subscription.
- Provides broad coverage but may limit transparency in methodology.

**Quality and Ethics Check**:
- Evaluate relevance, accuracy, and timeliness before use.
- Consider copyright, licensing, and privacy laws.
- Document limitations and potential biases in your analysis.

#### ðŸ“Š Secondary Methods Comparison Table

| Method | Typical Use | Strength | Risk to Watch |
|--------|-------------|----------|---------------|
| Public records | Demographic and legal context | Official, standardized | Coverage gaps, access limits |
| Statistical data | Trends and benchmarking | Broad scope | Outdated or aggregated too coarsely |
| Literature reviews | Research synthesis | Depth and rigor | Publication bias |
| Online databases | Market or scientific evidence | Fast access | Paywalls, unclear methodology |
| Archives | Historical context | Long-term view | Incomplete or fragmented records |
| Internal business data | Performance and operations | High relevance | Data silos, quality issues |
| Social media data | Sentiment and trends | Real-time signals | Sampling bias, privacy concerns |
| Data aggregators | Multi-source coverage | Convenience | Opaque sourcing/licensing |

**Types of Secondary Data Sources**:

##### 1. Company Records ðŸ’¼

**Description**: Internal data already collected by the organization.

**Examples**:
- Sales and transaction histories
- Customer relationship management (CRM) data
- Financial records and reports
- Employee records
- Production and inventory data
- Previous research reports

**Use Cases**:
- Trend analysis over time
- Performance benchmarking
- Historical comparisons
- Internal decision-making

##### 2. Government Databases ðŸ›ï¸

**Description**: Public data collected by government agencies.

**Examples**:
- Census data (population, demographics)
- Economic indicators (GDP, unemployment, inflation)
- Health statistics (disease rates, mortality)
- Environmental data (weather, pollution)
- Education statistics
- Crime statistics

**Common Sources**:
- data.gov (United States)
- data.gov.uk (United Kingdom)
- Eurostat (European Union)
- Statistics Norway (SSB)
- National statistical agencies

**Use Cases**:
- Market research
- Policy analysis
- Academic research
- Business planning

##### 3. Commercial Data Providers ðŸ’µ

**Description**: Data sold by market research firms and data brokers.

**Examples**:
- Market research reports (Nielsen, Gartner, Forrester)
- Industry analysis
- Consumer behavior data
- Credit reports
- Business intelligence databases
- Subscription databases

**Characteristics**:
- High quality and reliability
- Professional analysis included
- Can be expensive
- Regularly updated

##### 4. Academic Research ðŸŽ“

**Description**: Published studies and research papers.

**Examples**:
- Peer-reviewed journal articles
- Academic books and textbooks
- Conference papers
- Dissertations and theses
- Research databases (Google Scholar, JSTOR, PubMed)

**Use Cases**:
- Literature reviews
- Theoretical foundation
- Methodology reference
- Building on previous research

##### 5. Media and Publications ðŸ“°

**Description**: Information from news outlets and industry publications.

**Examples**:
- News articles and reports
- Industry magazines and trade journals
- Online blogs and websites
- Press releases
- Annual reports

**Use Cases**:
- Current events and trends
- Industry insights
- Competitor analysis
- Market monitoring

<div class="mermaid">
flowchart LR
    A[Choose Secondary<br/>Data Source] --> B{Data Needs?}
    
    B -->|Internal trends| C[Company<br/>Records<br/>ðŸ’¼]
    B -->|Population data| D[Government<br/>Databases<br/>ðŸ›ï¸]
    B -->|Market insights| E[Commercial<br/>Providers<br/>ðŸ’µ]
    B -->|Research basis| F[Academic<br/>Publications<br/>ðŸŽ“]
    B -->|Current trends| G[Media &<br/>Publications<br/>ðŸ“°]
    
    C --> H[Data Collection]
    D --> H
    E --> H
    F --> H
    G --> H
    
    H --> I[Analysis &<br/>Application<br/>ðŸ’¡]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style H fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style I fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### âœ… Advantages of Secondary Data

The main benefit of secondary data is that it may be **easier and quicker to obtain** than primary data because the data collection has already been completed. Secondary data can offer a **more comprehensive or long-term perspective** than what may be possible to gather with primary data.

**Key Advantages**:

**1. Cost-Effective** ðŸ’°
- No data collection expenses
- Often free or low-cost (especially government data)
- Saves resources for analysis
- Multiple researchers can use same data

**2. Time-Saving** â±ï¸
- Data already collected
- Immediate availability
- No waiting for responses
- Quick start to analysis

**3. Large Sample Sizes** ðŸ“Š
- Census data covers entire populations
- Commercial databases have extensive samples
- Longitudinal data spans many years
- Geographic breadth

**4. Historical Perspective** ðŸ“…
- Access to past data
- Trend analysis over time
- Long-term comparisons
- Historical context

**5. Credibility** âœ“
- Government and academic sources
- Professional data collection methods
- Peer-reviewed research
- Established reputation

**6. Comprehensive Coverage** ðŸŒ
- Broad geographic scope
- Multiple variables
- Large-scale studies
- Diverse populations

<div class="mermaid">
graph TB
    A[Secondary Data<br/>Advantages] --> B[Cost-Effective<br/>ðŸ’°<br/>Low Expense]
    A --> C[Time-Saving<br/>â±ï¸<br/>Quick Access]
    A --> D[Large Samples<br/>ðŸ“Š<br/>Extensive Data]
    A --> E[Historical<br/>ðŸ“…<br/>Long-term View]
    A --> F[Credibility<br/>âœ“<br/>Trusted Sources]
    A --> G[Comprehensive<br/>ðŸŒ<br/>Broad Coverage]
    
    B --> H[Efficient<br/>Research<br/>âš¡]
    C --> H
    D --> H
    E --> H
    F --> H
    G --> H
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style H fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### âŒ Disadvantages of Secondary Data

Secondary data does not always **perfectly fit the research question**, and the researcher has **less control over the accuracy or relevance** of the data.

**Key Disadvantages**:

**1. Relevance Issues** ðŸŽ¯
- Collected for different purpose
- May not address your specific questions
- Variables may not match your needs
- Context may differ from your study

**2. Quality Concerns** âš ï¸
- Unknown data collection methods
- Uncertain accuracy and reliability
- Potential biases in original collection
- Difficult to verify quality

**3. Outdated Information** ðŸ“…
- May not reflect current conditions
- Time lag between collection and use
- Changes in definitions or methods over time
- Historical data may not predict future

**4. Limited Control** ðŸ”’
- Cannot modify collection methods
- Cannot add variables
- Cannot control sample selection
- No influence on quality

**5. Accessibility** ðŸš«
- Some data requires payment
- Privacy restrictions
- Limited documentation
- Format compatibility issues

**6. Fit Problems** ðŸ“
- Geographic mismatch
- Different time periods
- Incompatible definitions
- Aggregation levels don't match needs

<div class="mermaid">
graph TB
    A[Secondary Data<br/>Disadvantages] --> B[Relevance<br/>ðŸŽ¯<br/>May Not Fit]
    A --> C[Quality<br/>âš ï¸<br/>Unknown Methods]
    A --> D[Outdated<br/>ðŸ“…<br/>Not Current]
    A --> E[Limited Control<br/>ðŸ”’<br/>Cannot Modify]
    A --> F[Accessibility<br/>ðŸš«<br/>Restrictions]
    A --> G[Fit Problems<br/>ðŸ“<br/>Mismatches]
    
    B --> H[Careful<br/>Evaluation<br/>Needed<br/>ðŸ”]
    C --> H
    D --> H
    E --> H
    F --> H
    G --> H
    
    style A fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
    style B fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style F fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style G fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style H fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
</div>

---

### ðŸ”„ Combining Primary and Secondary Data

In reality, **primary and secondary data are frequently combined** in the work of researchers and analysts. For instance, a market researcher may use **secondary data** to gain a general understanding of industry trends before conducting a **survey (primary data)** to gain more detailed knowledge about consumer attitudes.

#### ðŸ“Š Integration Strategy

<div class="mermaid">
flowchart TD
    A[Research Project] --> B{Start with<br/>Secondary Data}
    
    B --> C[Literature Review<br/>ðŸ“š<br/>What's Known?]
    C --> D[Industry Reports<br/>ðŸ“Š<br/>Market Context]
    D --> E[Government Data<br/>ðŸ›ï¸<br/>Demographics]
    
    E --> F{Gaps<br/>Identified?}
    
    F -->|Yes| G[Design Primary<br/>Data Collection<br/>ðŸ“]
    F -->|No| H[Use Only<br/>Secondary Data]
    
    G --> I[Surveys /<br/>Interviews /<br/>Experiments]
    
    I --> J[Combine<br/>Both Sources<br/>ðŸ”„]
    H --> J
    
    J --> K[Comprehensive<br/>Analysis<br/>ðŸ’¡]
    
    K --> L[Actionable<br/>Insights<br/>âœ…]
    
    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style D fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style E fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style F fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style G fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style H fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style I fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style J fill:#FFD700,stroke:#B8860B,stroke-width:3px
    style K fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    style L fill:#50C878,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

#### ðŸ’¼ Real-World Example: Market Research Project

**Scenario**: A retail company wants to understand consumer preferences for a new product line.

**Phase 1: Secondary Data** ðŸ“š
- **Industry reports**: Overall market size and trends
- **Government data**: Demographics of target market
- **Competitor analysis**: Published financial reports and news articles
- **Academic research**: Consumer behavior studies

**Insights from Secondary Data**:
- Market is growing at 15% annually
- Target demographic: 25-40 years old, urban
- Major competitors and their market share
- Key factors influencing purchase decisions

**Phase 2: Primary Data** ðŸ“
- **Surveys**: 1,000 potential customers about specific product features
- **Focus groups**: In-depth discussions with 50 participants
- **A/B testing**: Website designs with 5,000 visitors
- **Interviews**: 20 one-on-one interviews with key customers

**Insights from Primary Data**:
- Specific features customers value most
- Price points customers are willing to pay
- Preferred marketing messages
- Unmet needs in current market

**Phase 3: Combined Analysis** ðŸ”„
- **Secondary data** provides context and benchmarks
- **Primary data** addresses specific product questions
- **Integration** creates comprehensive understanding
- **Validation** of secondary insights through primary research

**Outcome**: 
- Developed product features based on customer preferences
- Priced competitively based on market research
- Targeted marketing to identified segments
- Projected market share using combined insights

<div class="mermaid">
graph LR
    subgraph "Secondary Data Foundation"
        A1[Industry<br/>Trends] --> C
        A2[Demographics] --> C
        A3[Competitor<br/>Analysis] --> C
    end
    
    subgraph "Primary Data Specifics"
        B1[Customer<br/>Surveys] --> C
        B2[Focus<br/>Groups] --> C
        B3[A/B<br/>Testing] --> C
    end
    
    C[Integrated<br/>Analysis<br/>ðŸ”„] --> D[Product<br/>Development]
    C --> E[Pricing<br/>Strategy]
    C --> F[Marketing<br/>Plan]
    
    D --> G[Successful<br/>Product Launch<br/>ðŸš€]
    E --> G
    F --> G
    
    style C fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

---

### ðŸ“‹ Primary vs Secondary Data Comparison

#### Comprehensive Comparison Table

| Aspect | Primary Data | Secondary Data |
|--------|-------------|----------------|
| **Definition** | First-hand information collected by researcher for specific purpose | Pre-existing information collected by others for different purpose |
| **Collection** | Surveys, interviews, experiments, observations, focus groups | Company records, government databases, published research, media |
| **Cost** | High (collection expenses, staff time, materials) | Low to moderate (often free or subscription-based) |
| **Time** | Long (design, collect, process, analyze) | Short (readily available, immediate access) |
| **Specificity** | Perfectly tailored to research question | May not perfectly match research needs |
| **Accuracy** | High (controlled collection methods) | Variable (depends on original source) |
| **Control** | Full control over methods and quality | Limited or no control |
| **Currency** | Most up-to-date | May be outdated |
| **Sample Size** | Usually smaller (budget/time constraints) | Often larger (government/commercial data) |
| **Ownership** | Exclusive to collector | Shared, publicly available |
| **Flexibility** | Can modify approach during collection | Fixed, cannot change |
| **Reliability** | Known (researcher controls quality) | Unknown (must evaluate source) |
| **Geographic Scope** | Limited by resources | Often comprehensive |
| **Historical Data** | Only current collection | May span decades |
| **Best For** | Specific research questions, unique needs | Context, trends, benchmarks, exploratory research |
| **Examples** | Customer satisfaction survey, user interviews | Census data, industry reports, academic studies |

#### ðŸŽ¯ Decision Framework: When to Use Which?

<div class="mermaid">
flowchart TD
    Start{Research<br/>Question} --> Q1{Data<br/>Exists?}
    
    Q1 -->|No| Primary[Use Primary<br/>Data<br/>ðŸ“]
    Q1 -->|Yes| Q2{Fits Your<br/>Needs?}
    
    Q2 -->|Yes| Secondary[Use Secondary<br/>Data<br/>ðŸ“š]
    Q2 -->|Partially| Q3{Budget &<br/>Time?}
    
    Q3 -->|Limited| Secondary2[Secondary<br/>+ Small Primary]
    Q3 -->|Adequate| Both[Use Both<br/>ðŸ”„]
    
    Primary --> End[Data Collection<br/>& Analysis]
    Secondary --> End
    Secondary2 --> End
    Both --> End
    
    style Start fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style Q1 fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style Q2 fill:#FF9800,stroke:#E65100,stroke-width:2px
    style Q3 fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style Primary fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style Secondary fill:#4ECDC4,stroke:#2C7873,stroke-width:2px,color:#fff
    style Secondary2 fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px
    style Both fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style End fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

**Use Primary Data When**:
- No existing data addresses your question
- Need very specific information
- Require current, up-to-date data
- Need to establish causality (experiments)
- Competitive advantage from unique insights
- Have sufficient budget and time

**Use Secondary Data When**:
- Existing data meets your needs
- Need quick, cost-effective information
- Require historical perspective
- Need large sample sizes or broad coverage
- Building foundation before primary research
- Limited resources

**Use Both When**:
- Secondary provides context, primary provides specifics
- Need to validate secondary findings
- Comprehensive research project
- Triangulation for reliability
- Filling gaps in existing data

---

### ðŸŽ¯ Best Practices for Data Collection

#### Primary Data Collection Best Practices

**1. Clear Objectives** ðŸŽ¯
- Define specific research questions
- Identify key variables needed
- Determine required sample size
- Establish success criteria

**2. Proper Design** ðŸ“
- Choose appropriate method (survey/interview/etc.)
- Design unbiased questions
- Pilot test instruments
- Plan sampling strategy

**3. Ethical Considerations** âš–ï¸
- Obtain informed consent
- Protect participant privacy
- Ensure data security
- Follow GDPR/regulations

**4. Quality Control** âœ“
- Train data collectors
- Monitor collection process
- Validate responses
- Document procedures

**5. Data Management** ðŸ’¾
- Plan storage and backup
- Ensure data security
- Document metadata
- Prepare for analysis

### âš–ï¸ Ethical and Privacy Concerns

Whether primary or secondary, data collection raises ethical and privacy issues. These issues are more prominent than ever in todayâ€™s digital age because gathering, storing, and analyzing vast amounts of data is possible.

**Informed consent**
- Participants must understand how data will be used, stored, who can access it, and how long it will be retained.
- Participation should be voluntary, and withdrawal must always be an option.

**Anonymity and confidentiality**
- Protect participant identities through anonymization or secure access controls.
- Do not disclose identities in reports without explicit permission.

**Data security**
- Use encryption, secure servers, and access controls to prevent unauthorized access or loss.
- Especially important for sensitive data such as health or financial records.

**Data accuracy**
- Ensure accurate collection to avoid flawed conclusions and harmful decisions.
- Validate instruments and clarify misunderstandings during collection.

**Use of secondary data**
- Evaluate whether the original consent covers new uses of the data.
- Consider context, licensing, and the ethical fit of reuse.

**Transparency**
- Clearly explain how data will be used, stored, and protected.
- Provide contact points for questions or concerns.

**Legal compliance**
- Follow applicable laws and regulations (for example, GDPR in the EU).
- Non-compliance can result in significant penalties and reputational risk.

#### âœ… Ethics Checklist (Quick Use)

- Informed consent captured and documented
- Participation is voluntary with clear withdrawal options
- Anonymity/confidentiality protections in place
- Data stored securely with access controls
- Secondary data reuse fits original consent and licensing
- Legal requirements reviewed (GDPR/sector rules)

### ðŸ›ï¸ Data Protection Acts and Laws

A data analyst may need to consider several privacy laws and regulations when working with personal data worldwide. The list below highlights a few major frameworks, but laws and regulations are not limited to these:

**General Data Protection Regulation (GDPR)**
- European Union regulation that reshaped global privacy practices.
- Applies to any organization processing EU residents' personal data, regardless of location.

**California Consumer Privacy Act (CCPA)**
- California state law that strengthens consumer privacy rights.
- Applies to companies handling personal data of California residents, even if based elsewhere.

**Personal Data Protection Act (PDPA)**
- Singapore law governing collection, use, and disclosure of personal data by private organizations.
- Emphasizes consent, purpose limitation, and reasonable security safeguards.

**Data Protection Act 2018 (DPA 2018)**
- UK law that supplements GDPR and replaced the 1998 act.
- Sets rules for how organizations process personal data in the UK.

**Personal Information Protection and Electronic Documents Act (PIPEDA)**
- Canadian federal law covering commercial use of personal information.
- Requires transparency, consent, and accountable handling of personal data.

**Brazil General Personal Data Protection Law (LGPD)**
- Brazil's privacy law similar to GDPR with rights for data subjects.
- Applies broadly to processing of Brazilian personal data.

**Privacy Act 1988**
- Australian law protecting personal information used by government agencies and many organizations.
- Covers principles around collection, use, disclosure, and data quality.

**Protection of Personal Information Act (POPIA)**
- South African law governing personal data in both public and private sectors.
- Establishes processing conditions and data subject rights.

**Key Reminder**: These laws differ in scope, requirements, and penalties. Analysts should confirm which rules apply, document compliance steps, and stay updated as regulations evolve.

#### ðŸŒ Regional Comparison (At a Glance)

| Region | Key Laws | Primary Focus |
|--------|----------|----------------|
| EU | GDPR | Strong data subject rights, lawful basis, cross-border safeguards |
| US (California) | CCPA | Consumer rights to access, delete, and opt-out of sale/sharing |
| UK | DPA 2018 + UK GDPR | GDPR-aligned rules with local governance |
| Canada | PIPEDA | Consent-based commercial data handling |
| Brazil | LGPD | GDPR-like framework with national oversight |
| Singapore | PDPA | Consent and purpose limitation for private organizations |
| Australia | Privacy Act 1988 | Principles-based protection across sectors |
| South Africa | POPIA | Processing conditions and data subject rights |

<div class="mermaid">
flowchart TD
    A[Start: Personal Data?] --> B{Data subject location?}
    B -->|EU| C[Apply GDPR]
    B -->|UK| D[Apply UK GDPR + DPA 2018]
    B -->|California| E[Apply CCPA]
    B -->|Canada| F[Apply PIPEDA]
    B -->|Brazil| G[Apply LGPD]
    B -->|Singapore| H[Apply PDPA]
    B -->|Australia| I[Apply Privacy Act 1988]
    B -->|South Africa| J[Apply POPIA]
    B -->|Other| K[Check local privacy laws]

    C --> L[Confirm lawful basis,
consent, retention, rights]
    D --> L
    E --> L
    F --> L
    G --> L
    H --> L
    I --> L
    J --> L
    K --> L

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style B fill:#FFD700,stroke:#B8860B,stroke-width:2px
    style C fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style D fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style E fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style F fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style G fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style H fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style I fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style J fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style K fill:#9E9E9E,stroke:#424242,stroke-width:2px,color:#fff
    style L fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
</div>

**Cross-border note**: If data moves across borders, check transfer rules (e.g., GDPR adequacy decisions, SCCs) and document safeguards.

### ðŸ‡ªðŸ‡º GDPR (General Data Protection Regulation)

The GDPR is a cornerstone of modern data protection. Adopted by the European Union, it took effect on 25 May 2018 and reshaped how personal data about individuals is handled within the EU and beyond.

**Key Features**:

**Territorial scope**
- Applies to any organization processing the personal data of EU residents, regardless of where the organization is located.
- Covers organizations that offer goods/services to, or monitor, people in the EU.

**Consent**
- Must be freely given, specific, informed, and unambiguous.
- It must be as easy to withdraw consent as it is to give it.

**Rights of individuals**
- Access to personal data and information about processing.
- Rectification of inaccurate data.
- Erasure ("right to be forgotten").
- Restriction of processing.
- Data portability.
- Objection to processing.

**Data Protection Impact Assessments (DPIAs)**
- Required for high-risk processing to identify and reduce privacy risks.

**Data Protection Officers (DPOs)**
- Mandatory in certain cases, such as public authorities or large-scale processing of special category data.

**Breach notification**
- Certain breaches must be reported to the supervisory authority within 72 hours.
- Individuals must be notified if there is a high risk to their rights and freedoms.

**Penalties**
- Severe fines: up to 4% of global annual turnover or 20 million EUR (whichever is higher).

**Why it matters for analysts**: GDPR gives people more control over their data and requires transparent, careful handling. Analysts regularly work with personal data and must ensure collection, storage, and analysis remain compliant.

#### ðŸ”Ž GDPR vs CCPA (Quick Comparison)

| Area | GDPR (EU) | CCPA (California) |
|------|----------|-------------------|
| Scope | Applies to processing EU residents' personal data | Applies to personal data of California residents |
| Legal basis | Requires a lawful basis (e.g., consent, contract, legitimate interest) | Focuses on consumer rights; no explicit lawful basis model |
| Consent | Often required; must be explicit and easy to withdraw | Opt-out model for sale/sharing of personal data |
| Individual rights | Access, rectification, erasure, restriction, portability, objection | Access, delete, opt-out of sale/sharing, non-discrimination |
| Breach notice | Notify authority within 72 hours for qualifying breaches | Notification required under state breach laws |
| Penalties | Up to 4% of global turnover or 20M EUR | Civil penalties enforced by the state; private right of action for certain breaches |

#### ðŸ”Ž GDPR vs LGPD (Quick Comparison)

| Area | GDPR (EU) | LGPD (Brazil) |
|------|----------|--------------|
| Scope | Applies to processing EU residents' personal data | Applies to processing Brazilian personal data |
| Legal basis | Requires a lawful basis (e.g., consent, contract, legitimate interest) | Similar lawful bases including consent and legitimate interest |
| Individual rights | Access, rectification, erasure, restriction, portability, objection | Access, correction, anonymization/deletion, portability, objection |
| Regulator | Independent supervisory authorities | National Data Protection Authority (ANPD) |
| Penalties | Up to 4% of global turnover or 20M EUR | Up to 2% of revenue in Brazil, capped by law |

#### Secondary Data Evaluation Best Practices

**1. Source Credibility** ðŸ”
- Who collected the data?
- What is their reputation?
- What were their objectives?
- Any potential biases?

**2. Methodology Assessment** ðŸ“Š
- How was data collected?
- What was the sample size?
- What time period covered?
- Any limitations noted?

**3. Relevance Check** ðŸŽ¯
- Does it address your question?
- Are definitions compatible?
- Is geographic scope appropriate?
- Is time period relevant?

**4. Currency Evaluation** ðŸ“…
- When was data collected?
- Still relevant today?
- Have conditions changed?
- Need supplementary current data?

**5. Quality Indicators** âœ“
- Peer-reviewed?
- Government/academic source?
- Transparent methodology?
- Replicable results?

<div class="mermaid">
graph TB
    subgraph "Primary Data Best Practices"
        P1[Clear<br/>Objectives] --> P6[Quality<br/>Data]
        P2[Proper<br/>Design] --> P6
        P3[Ethical<br/>Considerations] --> P6
        P4[Quality<br/>Control] --> P6
        P5[Data<br/>Management] --> P6
    end
    
    subgraph "Secondary Data Best Practices"
        S1[Source<br/>Credibility] --> S6[Reliable<br/>Data]
        S2[Methodology<br/>Assessment] --> S6
        S3[Relevance<br/>Check] --> S6
        S4[Currency<br/>Evaluation] --> S6
        S5[Quality<br/>Indicators] --> S6
    end
    
    P6 --> Final[Successful<br/>Research<br/>Outcomes<br/>ðŸŽ¯]
    S6 --> Final
    
    style P6 fill:#50C878,stroke:#2E7D32,stroke-width:3px,color:#fff
    style S6 fill:#FF9800,stroke:#E65100,stroke-width:3px,color:#fff
    style Final fill:#4CAF50,stroke:#2E7D32,stroke-width:4px,color:#fff
</div>

---

<svg width="680" height="82" viewBox="0 0 680 82" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Lesson task banner">
    <rect x="1" y="1" width="678" height="80" rx="10" fill="#E8F3FF" stroke="#4A90D9" stroke-width="2"/>
    <text x="20" y="34" fill="#1F4E79" font-size="18" font-weight="700">Lesson Task</text>
    <text x="20" y="56" fill="#1F4E79" font-size="13">Design a short Google Forms survey with clear structure and valid question types.</text>
</svg>

### The Task

In this lesson, you learned how to collect primary data using online methods. A simple and free approach is to create a survey in **Google Forms**. If you have never used Google Forms, follow this short guide and build the survey described below.

#### Quick Guide: How to Create a Google Form

1. Go to https://forms.google.com and sign in.
2. Click **Blank** to create a new form.
3. Add a **title** and **description** (e.g., "Customer Experience Survey").
4. Use **Add question** to insert items and choose the **question type**.
5. Group questions with **Section** to keep the survey short and clear.
6. Turn on **Required** for essential questions only.
7. Click **Send** to share by link, email, or QR code.

#### Survey Scenario and Requirements

**Scenario**: Alex manages a well-known clothing store and wants to understand customers better to improve offerings and satisfaction.

Design a survey that captures:
- **Demographics** (age, gender, location, income)
- **Shopping habits** (frequency, spend, channel)
- **Store experience** (layout, cleanliness, fitting rooms, checkout)
- **Customer service** (friendliness, product knowledge, responsiveness)
- **Product selection** (variety, brands, quality, style, price)
- **Loyalty programs and promotions** (awareness, participation, appeal)
- **Overall satisfaction and improvement suggestions**

Keep the survey short and focused to respect customers' time. Start with the most essential questions and end with demographics.

#### Example Survey Structure (Recommended Question Types)

**Section 1: Shopping Habits**
- How often do you shop with us? (Multiple choice)
- What is your typical spend per visit? (Multiple choice)
- Which channels do you use most? In-store, online, both (Multiple choice)

**Section 2: Store Experience**
- Rate the store layout and ease of finding items. (Likert scale)
- Rate cleanliness and fitting room experience. (Likert scale)
- How would you rate checkout speed? (Rating scale)

**Section 3: Customer Service**
- Staff friendliness (Likert scale)
- Product knowledge (Likert scale)
- Responsiveness to questions (Likert scale)

**Section 4: Product Selection**
- Is our product selection adequate? (Yes/No)
- Which categories should we expand? (Checkboxes)
- Rate quality, style, and price. (Matrix)

**Section 5: Loyalty and Promotions**
- Are you aware of our loyalty program? (Yes/No)
- Do you participate in it? (Yes/No)
- What would make it more appealing? (Open-ended)

**Section 6: Overall Satisfaction**
- Overall satisfaction with the store (Likert scale)
- What is one improvement you want most? (Open-ended)

**Section 7: Demographics**
- Age group, gender, location, income range (Multiple choice)

**Tip**: Aim for 12-18 questions total. Use mostly closed questions for analysis and 2-3 open-ended questions for insight.

#### Submission Checklist (Rubric)

- Covers all required categories (demographics, habits, experience, service, products, loyalty, satisfaction)
- Uses appropriate question types (Likert, multiple choice, matrix, open-ended)
- Survey length is reasonable (12-18 questions)
- Questions are clear, unbiased, and easy to answer
- Sensitive items (demographics, income) placed near the end

#### Sample Google Form Title and Description

**Title**: Customer Experience Survey - Alex's Clothing Store

**Description**: Thank you for visiting our store. This short survey (3-5 minutes) helps us understand your experience and improve our products and service. Your answers are anonymous and will be used only for research and service improvement. By continuing, you consent to participate and may stop at any time.

#### Sample Survey Questions (18 Items)

1. How often do you shop with us? (Multiple choice)
2. What is your typical spend per visit? (Multiple choice)
3. Which shopping channel do you use most? In-store, online, both (Multiple choice)
4. Rate the store experience (layout, cleanliness, fitting rooms, checkout). (Matrix)
5. Rate staff service (friendliness, product knowledge, responsiveness). (Matrix)
6. Is our product selection adequate? (Yes/No)
7. Which categories should we expand? (Checkboxes)
8. Rate product quality, style, and price. (Matrix)
9. Are you aware of our loyalty program? (Yes/No)
10. Do you participate in the loyalty program? (Yes/No)
11. What would make the loyalty program more appealing? (Open-ended)
12. Overall satisfaction with the store. (Likert scale)
13. What is one improvement you want most? (Open-ended)
14. Age group. (Multiple choice)
15. Gender. (Multiple choice)
16. Location. (Multiple choice)
17. Household income range. (Multiple choice)
18. How did you first hear about our store? (Multiple choice)

### ðŸ“š What Did I Learn in This Lesson?

This lesson provided the following insights:

- Primary and secondary data are two broad categories describing how data was collected.
- There are a variety of existing platforms for collecting primary data or retrieving secondary data.
- Applying the right question type improves the quality of data collected.
- All data must be collected and stored according to ethical principles and in adherence to the privacy laws of the region where data is collected or retained.

### ðŸ’¡ Key Takeaways

1. **Primary data** = first-hand information collected by researcher for specific research purpose
2. **Secondary data** = pre-existing information collected by others for different purposes
3. **Primary data methods** include: surveys/questionnaires, interviews, observations, experiments, focus groups, case studies, field trials/pilots, ethnography
4. **Surveys** can be delivered in-person, by mail, by phone, or online; use structured questions to collect information from samples
5. **Interviews** have three types: structured (predetermined questions), semi-structured (mix of set and exploratory), unstructured (guided conversation)
6. **Survey question types** include multiple choice, Likert scale, rank order, open-ended, dichotomous, demographic, matrix, pictorial, and semantic differential
7. **Interview/focus group questions** commonly include open-ended, probing, experience/example, hypothetical, opinion, knowledge, sensory, and demographic prompts
8. **Question design** should balance data needs, analysis simplicity, and respondent experience to improve response quality
9. **Observations** can be overt (participants aware) or covert (participants unaware); also participant vs non-participant
10. **Experiments** are controlled studies changing one variable to track effects on another; powerful for demonstrating cause-and-effect
11. **Focus groups** use moderator-guided discussions (6-12 people) to explore experiences, beliefs, and attitudes in depth
12. **Case studies** thoroughly investigate single subjects using various data sources; provide deep understanding but limited generalizability
13. **Field trials/pilots** test concepts under real-world conditions; frequently used in product development, engineering, agriculture
14. **Ethnography** involves researcher immersion in community for extended period to study behaviors, culture, and practices
15. **Mixed-methods approach** combines several methods (e.g., surveys + interviews) for more thorough understanding
16. **Modern online environments** have significantly impacted primary data collection, offering fresh approaches to gathering, archiving, and analyzing data
17. **Online surveys** revolutionized by digital platforms; quicker, less expensive, larger and more varied audience reach via email/social media
18. **Digital survey platforms** (Google Forms, SurveyMonkey, Typeform, Qualtrics) enable automated data entry and analysis, reducing errors
19. **Virtual interviews/focus groups** made possible by video conferencing tools; include participants worldwide, enable thorough transcription
20. **Web analytics and digital ethnography** track online behavior (websites visited, links clicked, time on page) with user permission
21. **Online experiments (A/B testing)** platforms (Google Optimize, Optimizely) make it simple to test different versions and see which performs better
22. **Crowdsourcing and citizen science** allow researchers to collect data from general public on previously unimaginable scale
23. **Mobile data collection** uses smartphones for GPS data, images, videos, real-time responses; quicker and more effective across fields
24. **Privacy and data security** are significant concerns with online data collection; requires informed consent, anonymization, secure storage
25. **Digital divide creates bias**: Online data often overrepresents younger, educated, wealthier individuals and underrepresents older, less-educated, low-income groups
26. **GDPR, CCPA, HIPAA compliance** essential for protecting participants' privacy and securing data properly
27. **Primary data advantages**: Specificity (exact fit), accuracy (controlled), currency (up-to-date), ownership (exclusive), control (full)
28. **Primary data disadvantages**: Time-consuming, expensive, limited scope, requires expertise, potential bias
29. **Secondary data sources**: company records, government databases, commercial providers, academic research, media publications
30. **Secondary data collection methods** include public records, statistical data, literature reviews, online databases, and archives
31. **Internal business data, social media data, and data aggregators** provide rich secondary datasets for market, trend, and sentiment insights
32. **Ethical data collection** requires informed consent, voluntary participation, and clear withdrawal options
33. **Anonymity and confidentiality** protect identities; avoid disclosure without explicit permission
34. **Data security and accuracy** are critical to prevent misuse and incorrect conclusions
35. **Secondary data reuse** must respect original context, licensing, and consent scope
36. **Legal compliance** (e.g., GDPR) is mandatory and violations can be costly
37. **GDPR scope** applies to organizations processing EU residents' data, regardless of location
38. **GDPR consent** must be freely given, specific, informed, and easy to withdraw
39. **GDPR rights** include access, rectification, erasure, restriction, portability, and objection
40. **GDPR obligations** include DPIAs, DPOs in certain cases, and 72-hour breach notification
41. **Data protection frameworks** include GDPR, CCPA, PDPA, DPA 2018, PIPEDA, LGPD, Privacy Act 1988, and POPIA
42. **Quality and ethics checks** are essential: evaluate relevance, accuracy, timeliness, licensing, privacy, and copyright constraints
43. **Secondary data advantages**: Cost-effective, time-saving, large samples, historical perspective, credibility, comprehensive coverage
44. **Secondary data disadvantages**: Relevance issues, quality concerns, outdated information, limited control, accessibility restrictions
45. **Best practice**: Combine primary and secondary data for comprehensive research; start with secondary for context, use primary to fill gaps
46. **Triangulation**: Using multiple data sources and methods (mixed-methods) increases reliability and validity of research findings
            """,
            "key_points": [
                "Primary data is first-hand information collected by the researcher for a specific research purpose",
                "Secondary data is pre-existing information collected by others for different objectives",
                "Choice of primary data collection method depends on: purpose of study, resources at hand, kind of data being sought",
                "Surveys/Questionnaires: Can be delivered in-person, by mail, by phone, or online; use structured questions to gather quantitative data from samples",
                "Interviews: Three types - Structured (predetermined questions), Semi-structured (some predetermined + freedom to explore), Unstructured (guided conversation)",
                "Survey question types include multiple choice, Likert scale, rank order, open-ended, dichotomous, demographic, matrix, pictorial, and semantic differential; choice impacts analysis",
                "Interview and focus group questions commonly use open-ended, probing, experience/example, hypothetical, opinion, knowledge, sensory, and demographic prompts",
                "Good question design balances data needs, analysis simplicity, and respondent experience to improve response quality",
                "Interviews can be conducted in-person, by phone, or online via various platforms",
                "Observations: Can be overt (participants aware) or covert (participants unaware); methodically observe and document behavior as it arises naturally",
                "Experiments: Controlled studies changing one variable to track effects on another; powerful for demonstrating cause-and-effect but resource-intensive",
                "Focus Groups: Moderator-guided group discussion (6-12 participants) on specific topics; helpful for thoroughly examining experiences, beliefs, attitudes",
                "Case Studies: Thorough investigation of single subject using various data sources; provides deep understanding but results might not apply to others",
                "Field Trials/Pilots: Test concepts under actual/real-world conditions while recording results; frequently used in product development, engineering, agriculture",
                "Ethnography: Researcher immerses in community for extended period to study behaviors, culture, and practices of members",
                "Mixed-methods approach: Typical to combine several methods within single study to obtain more thorough understanding of research topic",
                "Modern online environments have significantly impacted primary data collection techniques, offering fresh approaches to information gathering, archiving, and analysis",
                "Online surveys revolutionized by digital platforms (Google Forms, SurveyMonkey, Typeform, Qualtrics); quicker, less expensive to reach larger, more varied audience",
                "Digital surveys enable automated data entry and analysis, lowering possibility of errors and accelerating the research process",
                "Virtual interviews and focus groups made possible by video conferencing tools (Zoom, Teams, Google Meet); participants from all over world can be included",
                "Video conferencing enables recordings for thorough transcription and analysis of interviews and focus groups",
                "Web analytics and digital ethnography: Track online behavior (websites visited, links clicked, time spent) with user's permission using cookies, server logs, specialized software",
                "Online behavior data valuable for researching social media use, online consumer behavior, and information-seeking behavior",
                "Online experiments (A/B testing): Platforms like Google Optimize and Optimizely make it simpler to test different web page versions to see which performs better",
                "Crowdsourcing and citizen science: Internet allows collecting data from general public on previously unimaginable scale (e.g., wildlife sightings, document transcriptions)",
                "Mobile data collection: Widespread smartphones enable quicker, more effective data collection using GPS data, images, videos, and real-time responses",
                "Mobile data used across social science, health research, environmental studies, market research, and more fields",
                "Online data collection brings new difficulties and moral dilemmas that must be carefully addressed",
                "Privacy and data security: Online data collection raises significant issues; crucial to get informed consent, anonymize data, and ensure secure storage",
                "GDPR, CCPA, HIPAA compliance essential to protect participants' privacy throughout online data collection process",
                "Digital divide creates sampling bias: Online data frequently overrepresents younger, more educated, wealthier individuals",
                "Digital divide underrepresents older adults, less-educated individuals, low-income individuals in online research",
                "Mitigation strategies for bias: Mixed-mode data collection, targeted recruitment, accessibility features, statistical weighting to match population demographics",
                "Each method has advantages and disadvantages; best choice depends on research question nature, context, resources, and researcher skills/expertise",
                "Primary data advantages: specificity (exact fit), accuracy (controlled methods), currency (up-to-date), ownership (exclusive), control (full)",
                "Primary data disadvantages: time-consuming, expensive, limited scope, requires expertise, potential for various biases",
                "Secondary data sources: company records, government databases, commercial providers, academic research, media publications",
                "Secondary data collection methods include public records, statistical data, literature reviews, online databases, and archives",
                "Internal business data, social media data, and data aggregators provide additional secondary datasets for market and sentiment insights",
                "Ethical data collection requires informed consent, voluntary participation, and clear withdrawal options",
                "Anonymity and confidentiality protect participant identities; do not disclose without explicit permission",
                "Data security and accuracy are critical to prevent misuse and incorrect conclusions",
                "Secondary data reuse must respect original context, licensing, and consent scope",
                "Legal compliance (e.g., GDPR) is mandatory and violations can be costly",
                "GDPR scope applies to organizations processing EU residents' data, regardless of location",
                "GDPR consent must be freely given, specific, informed, and easy to withdraw",
                "GDPR rights include access, rectification, erasure, restriction, portability, and objection",
                "GDPR obligations include DPIAs, DPOs in certain cases, and 72-hour breach notification",
                "Data protection frameworks include GDPR, CCPA, PDPA, DPA 2018, PIPEDA, LGPD, Privacy Act 1988, and POPIA",
                "Quality and ethics checks: evaluate relevance, accuracy, timeliness, licensing, privacy, and copyright constraints before use",
                "Secondary data advantages: cost-effective, time-saving, large sample sizes, historical perspective, credibility, comprehensive coverage",
                "Secondary data disadvantages: relevance issues, quality concerns, outdated information, limited control, accessibility restrictions, fit problems",
                "Best practice: Combine primary and secondary data for comprehensive research outcomes (e.g., secondary for context, primary for specifics)",
                "Integration strategy: Start with secondary data for general understanding, then conduct primary data collection for more detailed knowledge",
                "Decision framework: Use primary when no existing data fits; use secondary when existing data meets needs; use both for comprehensive projects",
                "Quality evaluation essential: Assess source credibility, methodology, relevance, currency, and quality indicators for secondary data",
                "Ethical considerations critical: Informed consent, privacy protection, data security, especially for online methods, covert observation, and ethnography",
                "Triangulation: Using multiple data sources and methods (mixed-methods) increases reliability and validity of research findings"
            ],
            "visual_elements": {
                "diagrams": True,
                "tables": True,
                "highlighted_sections": True
            }
        },
        {
            "lesson_number": "1.4",
            "title": "Calculating Distributions",
            "content": """
### Introduction

The features in a data set are the fundamental building blocks in a data analyst's arsenal in providing insights regarding the state of a situation or domain. Features may be used to describe a specific outcome or represent an outcome that requires description.

Regardless of the approach, gathering descriptive statistics regarding a data set is helpful. These descriptive statistics may be generated differently for the various features and feature types and yield different insights to aid analysis.
 
---

### Explained vs Explanatory Variables

When conducting data analysis, some fields support an outcome, whereas others represent the outcome. These two types of variables are represented by explanatory and explained variables.

**Explained variables** are the variables we are trying to predict or forecast in a statistical model. For instance, the **house price** would be the explained variable in a linear regression model that forecasts house prices. It is also frequently known as the target, response, dependent, or outcome variable. It is "explained" in that the model's independent or explanatory variables can explain its values.

**Explanatory variables** explain or influence the changes in the explained variable. The house size, number of bedrooms, age, and other factors could all be explanatory variables in the house price example. These variables explain the variability in the dependent (explained) variable. They can also be referred to as features, input variables, predictor variables, independent variables, or regressors. One of the first steps in conducting an analysis is learning more about the specifics of the various variables used, which is why descriptive statistics are generated early in the process.

It is important to remember that unless a study specifically intends to establish a causal relationship, the relationship between explained and explanatory variables in a statistical model is typically **association rather than causation** (such as in a randomized controlled trial). It is not always the case that explanatory variables cause changes in the explained variable, even when a model shows they are associated with those changes.

#### ðŸ“Œ Example: Variable Roles

| Scenario | Explained Variable (Target) | Explanatory Variables (Predictors) |
|----------|-----------------------------|------------------------------------|
| House price model | House price | Size, bedrooms, age, location |
| Sales forecasting | Monthly sales | Ad spend, season, promotions, price |
| Customer churn | Churn (yes/no) | Tenure, usage, support tickets |

#### ðŸ§  Mini Exercise: Identify Target vs Predictors

1. A retailer wants to predict weekly sales using store size, local income, and promotion spend.
2. A hospital wants to predict patient length of stay using age, diagnosis, and procedure type.
3. A streaming service wants to predict churn using watch time, plan type, and recent support tickets.

**Answer key**:
1. Target: weekly sales; Predictors: store size, local income, promotion spend.
2. Target: length of stay; Predictors: age, diagnosis, procedure type.
3. Target: churn; Predictors: watch time, plan type, support tickets.

---

### 1. Feature Types and Distributions

Different feature types require different summaries. Numeric variables are summarized with measures like mean or standard deviation, while categorical variables are summarized with counts and proportions.

| Feature Type | Examples | Typical Distribution Summary |
|--------------|----------|------------------------------|
| Numeric (continuous) | income, height, time | mean, median, variance, standard deviation |
| Numeric (discrete) | orders, defects, clicks | frequency table, mean, variance |
| Categorical | brand, region, payment method | counts, percentages, mode |
| Ordinal | satisfaction rating, size (S/M/L) | counts, median, percentile |

<div class="mermaid">
flowchart TB
    A[Feature Type] --> B[Numeric]
    A --> C[Categorical]
    B --> D[Central Tendency]
    B --> E[Spread]
    B --> F[Shape]
    C --> G[Counts]
    C --> H[Percentages]

    style A fill:#4A90D9,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#50C878,stroke:#2E7D32,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    style D fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style E fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style F fill:#9B59B6,stroke:#6C3483,stroke-width:2px,color:#fff
    style G fill:#FFD700,stroke:#B8860B,stroke-width:2px,color:#000
    style H fill:#FFD700,stroke:#B8860B,stroke-width:2px,color:#000
</div>

---

### Descriptive Statistics

Descriptive statistics summarize and organize the characteristics of a data set, whether numerical or categorical.

#### Numerical Data

Common descriptive measures include:
- **Central tendency**: mean, median, mode
- **Dispersion**: variance, standard deviation, range, interquartile range (IQR)
- **Distribution shape**: skewness and kurtosis

These measures help condense the overall distribution. Knowing whether a data set is approximately normal (Gaussian) can influence which statistical tests or predictive models are appropriate.

#### Categorical Data

Descriptive statistics for categorical variables typically include:
- **Frequency counts**: how often each category appears
- **Proportions**: percentage of the total for each category
- **Cross-tabulations**: relationship between two categorical variables

<div class="important-info">
In essence, any technique that summarizes, organizes, and describes the properties of a data set can be regarded as descriptive statistics.
</div>

#### Example: Cross-Tabulation

| Gender \ Satisfaction | Low | Medium | High | Total |
|-----------------------|-----|--------|------|-------|
| Female | 8 | 22 | 30 | 60 |
| Male | 10 | 18 | 12 | 40 |
| Total | 18 | 40 | 42 | 100 |

**Interpretation**: This table suggests higher satisfaction among female respondents, while male responses skew more toward low and medium satisfaction.

---

### Distribution of Values

A variable's distribution describes how its values are spread across the range of possible values. Understanding distributions helps reveal patterns, trends, and outliers, and it guides which statistical tools and models are appropriate.

**Why distribution matters**:
- Some methods assume specific distributions (e.g., normal, binomial).
- The distribution affects how you interpret mean, median, mode, variance, and standard deviation.
- Unexpected distributions can signal data quality issues or collection errors.
- Skewed distributions can indicate meaningful trends (values clustering at one end).

**Model implications**:
- If assumptions are violated, you may need transformations or alternative models.
- Distribution checks are often one of the first steps in analysis.

#### Categorical Variable Distributions

For categorical data, the focus is on counts and proportions across categories:

**Frequency counts**
- Count how many times each category appears.

**Relative frequencies (proportions)**
- Convert counts to percentages of the total.

**Cumulative frequencies**
- Total counts up to a given category (useful for ordered categories).

These numeric summaries complement common visuals such as bar charts and pie charts.

#### Understanding Frequency Counts with Real Examples

**What is a frequency count?**

A frequency count is a straightforward approach to summarizing categorical data. The goal is to determine **how often each category appears** in your dataset. This reveals which categories are most and least prevalent, giving you a basic understanding of your data's distribution.

---

##### Example 1: Norwegian Cities Dataset

**Scenario**: You have data on 20 individuals and where they live in Norway.

**Raw Data**:

| ID | Home_City | ID | Home_City |
|----|-----------|----|-----------|
| 1 | Oslo | 11 | Oslo |
| 2 | Oslo | 12 | Oslo |
| 3 | Kristiansand | 13 | Stavanger |
| 4 | Bergen | 14 | Bergen |
| 5 | Oslo | 15 | Kristiansand |
| 6 | Stavanger | 16 | Kristiansand |
| 7 | Stavanger | 17 | Stavanger |
| 8 | Bergen | 18 | Oslo |
| 9 | Bergen | 19 | Bergen |
| 10 | Kristiansand | 20 | Stavanger |

**Analysis Steps**:
1. Identify distinct cities in the data
2. Count how many people live in each city
3. Create a frequency table

**Frequency Count Result**:

| City | Frequency Count |
|------|-----------------|
| Bergen | 5 |
| Kristiansand | 4 |
| Oslo | 6 |
| Stavanger | 5 |

**Interpretation**: Oslo is the most common city (6 people), followed by Bergen and Stavanger (5 each), and Kristiansand (4).

---

**How to Calculate: Excel Method**

**Step 1: Enter your data** in Excel:

| A | B |
|---|---|
| ID | Home_City |
| 1 | Oslo |
| 2 | Oslo |
| 3 | Kristiansand |
| 4 | Bergen |
| 5 | Oslo |
| ... | ... |
| 20 | Stavanger |

**Step 2: Create frequency table** using COUNTIF:

In columns D and E, create:

| D | E | Formula |
|---|---|---|
| **City** | **Frequency Count** | |
| Bergen | 5 | =COUNTIF($B$2:$B$21,"Bergen") |
| Kristiansand | 4 | =COUNTIF($B$2:$B$21,"Kristiansand") |
| Oslo | 6 | =COUNTIF($B$2:$B$21,"Oslo") |
| Stavanger | 5 | =COUNTIF($B$2:$B$21,"Stavanger") |
| **Total** | 20 | =SUM(E2:E5) |

**Alternative: Use a Pivot Table**
1. Select your data (A1:B21)
2. Insert â†’ PivotTable
3. Drag "Home_City" to **Rows**
4. Drag "Home_City" to **Values** (it will count automatically)

---

**How to Calculate: Python Method**

```python
import pandas as pd

# Create the dataset
data = {
    'ID': range(1, 21),
    'Home_City': ['Oslo', 'Oslo', 'Kristiansand', 'Bergen', 'Oslo', 'Stavanger', 
                  'Stavanger', 'Bergen', 'Bergen', 'Kristiansand', 'Oslo', 'Oslo', 
                  'Stavanger', 'Bergen', 'Kristiansand', 'Kristiansand', 'Stavanger', 
                  'Oslo', 'Bergen', 'Stavanger']
}
df = pd.DataFrame(data)

# Method 1: Using value_counts()
frequency = df['Home_City'].value_counts().sort_index()
print("Frequency Count by City:")
print(frequency)

# Method 2: Create a more detailed table
frequency_table = pd.DataFrame({
    'City': frequency.index,
    'Frequency Count': frequency.values
})
print("\nFormatted Table:")
print(frequency_table)

# Verify total
print(f"\nTotal observations: {frequency.sum()}")
```

**Expected Output**:
```
Frequency Count by City:
Bergen          5
Kristiansand    4
Oslo            6
Stavanger       5
Name: Home_City, dtype: int64

Formatted Table:
           City  Frequency Count
0      Bergen              5
1  Kristiansand              4
2         Oslo              6
3     Stavanger              5

Total observations: 20
```

---

##### Example 2: Student Satisfaction Ratings

**Scenario**: You surveyed 51 university students asking them to rate the quality of learning materials on a scale of 1-5:
- 1 = Very Poor
- 2 = Poor  
- 3 = Neutral
- 4 = Good
- 5 = Excellent

**Raw Ratings Data**:
```
3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 4, 2, 1, 3, 5, 2, 2, 3, 4, 4, 4, 5, 1, 3, 3, 4, 5, 
2, 1, 4, 3, 2, 5, 5, 3, 2, 2, 3, 4, 4, 4, 5, 2, 3, 3, 4, 5, 2, 1, 4
```

**Analysis Steps**:
1. Find all distinct rating values (1, 2, 3, 4, 5)
2. Count how many times each rating appears
3. Create a frequency table

---

**Step-by-Step Manual Counting (Understanding the Process)**

Before using Python, let's understand the manual process so you grasp what's happening under the hood.

**Step 1: Identify the distinct categories**

The rating scale is 1 to 5, so the possible categories are:
- 1 (Very Poor)
- 2 (Poor)
- 3 (Neutral)
- 4 (Good)
- 5 (Excellent)

**Step 2: Create an empty counting table**

Start with a table with columns for Category and Count:

| Rating | Count |
|--------|-------|
| 1 | ? |
| 2 | ? |
| 3 | ? |
| 4 | ? |
| 5 | ? |

**Step 3: Go through the data and count each category**

Raw data: 3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 4, 2, 1, 3, 5, 2, 2, 3, 4, 4, 4, 5, 1, 3, 3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 2, 2, 3, 4, 4, 4, 5, 2, 3, 3, 4, 5, 2, 1, 4

**Count all the 1's**: Go through and mark each time you see a 1:
- Position 5: 1
- Position 14: 1
- Position 24: 1
- Position 30: 1
- Position 50: 1
- **Total 1's: 5**

**Count all the 2's**: Go through and mark each time you see a 2:
- Positions: 4, 8, 13, 17, 18, 29, 33, 37, 38, 44, 49, 51
- **Total 2's: 7**

**Count all the 3's**: Go through and mark each time you see a 3:
- Positions: 1, 7, 11, 15, 20, 25, 26, 32, 36, 40, 45, 46
- **Total 3's: 10**

**Count all the 4's**: Continue the same process...
- **Total 4's: 14**

**Count all the 5's**: Continue the same process...
- **Total 5's: 15**

**Step 4: Fill in the final table**

After counting all categories, you get:

| Rating | Count | Label |
|--------|-------|-------|
| 1 | 5 | Very Poor |
| 2 | 7 | Poor |
| 3 | 10 | Neutral |
| 4 | 14 | Good |
| 5 | 15 | Excellent |
| **Total** | **51** | |

âœ… **Quick Verification**: 5 + 7 + 10 + 14 + 15 = 51 âœ“

**Key Insight**: As you can see, manual counting:
- âš ï¸ **Time-consuming** with larger datasets
- âš ï¸ **Error-prone** (easy to miscount)
- âš ï¸ **Tedious** to verify

This is why we use Python - it does this automatically, accurately, and instantly!

---

**Automated Counting with Python Code**:


```python
import pandas as pd

# Raw survey responses
ratings = [3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 4, 2, 1, 3, 5, 2, 2, 3, 4, 4, 4, 5, 1, 3, 
           3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 2, 2, 3, 4, 4, 4, 5, 2, 3, 3, 4, 5, 2, 1, 4]

df = pd.DataFrame({'Rating': ratings})

# Step 1: Count frequency for each rating
frequency = df['Rating'].value_counts().sort_index()
print("Frequency by Rating:")
print(frequency)

# Step 2: Create detailed frequency table with interpretation
rating_labels = {
    1: 'Very Poor',
    2: 'Poor',
    3: 'Neutral',
    4: 'Good',
    5: 'Excellent'
}

frequency_table = pd.DataFrame({
    'Rating': frequency.index,
    'Label': [rating_labels[r] for r in frequency.index],
    'Frequency Count': frequency.values,
    'Percentage': (frequency.values / len(df) * 100).round(2)
})

print("\nDetailed Frequency Table:")
print(frequency_table)

# Step 3: Quick insights
print(f"\nTotal responses: {len(df)}")
print(f"Most common rating: {frequency.idxmax()} ({frequency.max()} responses)")
print(f"Least common rating: {frequency.idxmin()} ({frequency.min()} responses)")
```

**Expected Output**:
```
Frequency by Rating:
1    5
2    7
3    10
4    14
5    15
Name: Rating, dtype: int64

Detailed Frequency Table:
   Rating      Label  Frequency Count  Percentage
0       1  Very Poor                5        9.80
1       2      Poor                7       13.73
2       3    Neutral               10       19.61
3       4      Good               14       27.45
4       5   Excellent              15       29.41

Total responses: 51
Most common rating: 5 (15 responses)
Least common rating: 1 (5 responses)
```

**Interpretation**:
- **5 (Excellent)** is the most common rating with 15 responses (29.41%)
- **4 (Good)** is the second most common with 14 responses (27.45%)
- Together, ratings 4 and 5 represent 56.86% of responses (very positive feedback)
- Only 9.80% gave a "Very Poor" rating (1)

---

##### Example 3: Exam Grade Distribution

**Scenario**: A statistics professor wants to analyze the grade distribution for 40 students who completed the final exam. Grades are on a scale of A through F.

**Raw Grade Data**:
```
B, A, C, B, B, A, F, C, B, A, C, D, B, A, C, B, A, B, C, A, 
D, B, A, C, B, B, A, C, D, A, B, C, A, F, B, A, C, B, A, C
```

**Grade Distribution Breakdown**:
- **A**: Excellent (90-100%)
- **B**: Good (80-89%)
- **C**: Average (70-79%)
- **D**: Below Average (60-69%)
- **F**: Fail (<60%)

---

**Analysis Steps**:
1. Count frequency of each grade (A, B, C, D, F)
2. Calculate relative frequencies (proportions)
3. Calculate percentages
4. Interpret the results

---

**How to Calculate: Excel Method**

**Step 1: Enter grades in Excel**:

| A | B |
|---|---|
| Student_ID | Grade |
| 1 | B |
| 2 | A |
| 3 | C |
| ... | ... |
| 40 | C |

**Step 2: Create frequency table** (columns D-E):

| D | E | Formula |
|---|---|---|
| **Grade** | **Frequency** | |
| A | 13 | =COUNTIF($B$2:$B$41,"A") |
| B | 14 | =COUNTIF($B$2:$B$41,"B") |
| C | 9 | =COUNTIF($B$2:$B$41,"C") |
| D | 2 | =COUNTIF($B$2:$B$41,"D") |
| F | 2 | =COUNTIF($B$2:$B$41,"F") |
| **Total** | 40 | =SUM(E2:E6) |

**Step 3: Calculate relative frequency** (column F):

| F | Formula |
|---|---|
| **Relative Freq** | |
| 0.325 | =E2/$E$7 |
| 0.350 | =E3/$E$7 |
| 0.225 | =E4/$E$7 |
| 0.050 | =E5/$E$7 |
| 0.050 | =E6/$E$7 |
| 1.000 | =SUM(F2:F6) |

**Step 4: Calculate percentage** (column G):

| G | Formula |
|---|---|
| **Percentage (%)** | |
| 32.5 | =F2*100 |
| 35.0 | =F3*100 |
| 22.5 | =F4*100 |
| 5.0 | =F5*100 |
| 5.0 | =F6*100 |
| 100.0 | =SUM(G2:G6) |

**Final Excel Table**:

| Grade | Frequency | Relative Frequency | Percentage (%) |
|-------|-----------|-------------------|----------------|
| A | 13 | 0.325 | 32.5 |
| B | 14 | 0.350 | 35.0 |
| C | 9 | 0.225 | 22.5 |
| D | 2 | 0.050 | 5.0 |
| F | 2 | 0.050 | 5.0 |
| **Total** | 40 | 1.000 | 100.0 |

---

**How to Calculate: Python Method**

```python
import pandas as pd

# Raw exam grades
grades = ['B', 'A', 'C', 'B', 'B', 'A', 'F', 'C', 'B', 'A', 'C', 'D', 'B', 'A', 
          'C', 'B', 'A', 'B', 'C', 'A', 'D', 'B', 'A', 'C', 'B', 'B', 'A', 'C', 
          'D', 'A', 'B', 'C', 'A', 'F', 'B', 'A', 'C', 'B', 'A', 'C']

df = pd.DataFrame({'Grade': grades})

# Define grade order (A to F)
grade_order = ['A', 'B', 'C', 'D', 'F']

# Step 1: Calculate frequency for each grade
frequency = df['Grade'].value_counts().reindex(grade_order)
print("Frequency by Grade:")
print(frequency)

# Step 2: Calculate relative frequencies
rel_freq = frequency / len(df)
print("\nRelative Frequencies:")
print(rel_freq)

# Step 3: Calculate percentages
percentages = (frequency / len(df) * 100).round(1)
print("\nPercentages:")
print(percentages)

# Step 4: Create comprehensive summary table
grade_labels = {
    'A': 'Excellent',
    'B': 'Good',
    'C': 'Average',
    'D': 'Below Average',
    'F': 'Fail'
}

summary_table = pd.DataFrame({
    'Grade': frequency.index,
    'Label': [grade_labels[g] for g in frequency.index],
    'Frequency': frequency.values,
    'Relative Freq': rel_freq.round(3).values,
    'Percentage (%)': percentages.values
})

print("\nComplete Grade Distribution:")
print(summary_table)

# Step 5: Calculate pass/fail statistics
pass_count = summary_table[summary_table['Grade'] != 'F']['Frequency'].sum()
fail_count = summary_table[summary_table['Grade'] == 'F']['Frequency'].sum()
pass_rate = (pass_count / len(df) * 100).round(1)

print(f"\nPass/Fail Analysis:")
print(f"Students who passed (A-D): {pass_count} ({pass_rate}%)")
print(f"Students who failed (F): {fail_count} ({100-pass_rate}%)")

# Step 6: Calculate students with A or B (excellent/good performance)
high_performers = summary_table[summary_table['Grade'].isin(['A', 'B'])]['Frequency'].sum()
high_performer_rate = (high_performers / len(df) * 100).round(1)

print(f"\nHigh Performers (A or B): {high_performers} students ({high_performer_rate}%)")
```

**Expected Output**:
```
Frequency by Grade:
A    13
B    14
C     9
D     2
F     2
Name: Grade, dtype: int64

Relative Frequencies:
A    0.325
B    0.350
C    0.225
D    0.050
F    0.050
Name: Grade, dtype: float64

Percentages:
A    32.5
B    35.0
C    22.5
D     5.0
F     5.0
Name: Grade, dtype: float64

Complete Grade Distribution:
  Grade          Label  Frequency  Relative Freq  Percentage (%)
0     A      Excellent         13          0.325            32.5
1     B           Good         14          0.350            35.0
2     C        Average          9          0.225            22.5
3     D  Below Average          2          0.050             5.0
4     F           Fail          2          0.050             5.0

Pass/Fail Analysis:
Students who passed (A-D): 38 (95.0%)
Students who failed (F): 2 (5.0%)

High Performers (A or B): 27 students (67.5%)
```

**Interpretation**:
- **Most common grade**: B (35.0%), followed by A (32.5%)
- **Pass rate**: 95% of students passed the exam (grades A-D)
- **High performers**: 67.5% of students achieved A or B grades (excellent/good performance)
- **Concern area**: 5% failed (2 students) - may need remedial support
- **Distribution shape**: Positively skewed - most students performed well
- **Grade C and below**: Only 32.5% of students - indicates exam was fair but not too easy

**Teaching Insights from this Data**:
- âœ… High pass rate suggests effective teaching and appropriate difficulty level
- âœ… Strong concentration in A-B range indicates student comprehension
- âš ï¸ Consider reviewing content for the 2 students who failed
- ðŸ“Š Healthy distribution with most students in "good" to "excellent" range

---

##### Your Turn: Practice Problem

**Dataset**: 30 survey responses on preferred learning mode:
```
'Online', 'In-Person', 'Hybrid', 'Online', 'Hybrid', 'In-Person', 'Online', 'Online', 'In-Person', 'Hybrid',
'Online', 'Online', 'Hybrid', 'In-Person', 'Online', 'In-Person', 'Hybrid', 'Online', 'In-Person', 'Online',
'Online', 'Hybrid', 'In-Person', 'Online', 'Online', 'In-Person', 'Hybrid', 'Online', 'Hybrid', 'In-Person'
```

**Your Task**:
1. Create a pandas dataframe with this data
2. Calculate the frequency count for each learning mode
3. Create a table showing: Mode, Frequency Count, Percentage
4. Answer: Which mode is most preferred? What percentage?

**Solution Code Template**:
```python
import pandas as pd

learning_modes = [
    'Online', 'In-Person', 'Hybrid', 'Online', 'Hybrid', 'In-Person', 'Online', 'Online', 'In-Person', 'Hybrid',
    'Online', 'Online', 'Hybrid', 'In-Person', 'Online', 'In-Person', 'Hybrid', 'Online', 'In-Person', 'Online',
    'Online', 'Hybrid', 'In-Person', 'Online', 'Online', 'In-Person', 'Hybrid', 'Online', 'Hybrid', 'In-Person'
]

df = pd.DataFrame({'Learning_Mode': learning_modes})

# YOUR CODE HERE: Calculate frequency and create table
# frequency = ...
# table = ...
# print(table)
```

<details>
<summary><strong>Click to see solution</strong></summary>

```python
import pandas as pd

learning_modes = [
    'Online', 'In-Person', 'Hybrid', 'Online', 'Hybrid', 'In-Person', 'Online', 'Online', 'In-Person', 'Hybrid',
    'Online', 'Online', 'Hybrid', 'In-Person', 'Online', 'In-Person', 'Hybrid', 'Online', 'In-Person', 'Online',
    'Online', 'Hybrid', 'In-Person', 'Online', 'Online', 'In-Person', 'Hybrid', 'Online', 'Hybrid', 'In-Person'
]

df = pd.DataFrame({'Learning_Mode': learning_modes})

# Calculate frequency
frequency = df['Learning_Mode'].value_counts().sort_index()

# Create summary table
summary_table = pd.DataFrame({
    'Learning Mode': frequency.index,
    'Frequency Count': frequency.values,
    'Percentage': (frequency.values / len(df) * 100).round(2)
})

print(summary_table)
print(f"\nMost preferred: {frequency.idxmax()} ({frequency.max()} responses, {frequency.max()/len(df)*100:.2f}%)")
```

**Result**:
```
  Learning Mode  Frequency Count  Percentage
0        Hybrid                7       23.33
1    In-Person                9       30.00
2       Online               14       46.67

Most preferred: Online (14 responses, 46.67%)
```

</details>

---

### Relative Frequencies

The **relative frequency** of a category is the **percentage** (or proportion) that category represents out of the total. While frequency counts show raw numbers, relative frequencies normalize the data, making it easier to compare categories and understand data distribution.

**Formula**:
$$\text{Relative Frequency} = \frac{\text{Frequency Count}}{\text{Total Observations}}$$

$$\text{Relative Frequency (\%)} = \frac{\text{Frequency Count}}{\text{Total Observations}} \times 100$$

---

#### Example: Norwegian Cities Revisited

Using our 20 people across Norwegian cities, let's calculate relative frequencies:

| City | Frequency Count | Calculation | Relative Frequency | Relative Frequency (%) |
|------|-----------------|-------------|-------------------|----------------------|
| Bergen | 5 | 5 / 20 | 0.25 | 25% |
| Kristiansand | 4 | 4 / 20 | 0.20 | 20% |
| Oslo | 6 | 6 / 20 | 0.30 | 30% |
| Stavanger | 5 | 5 / 20 | 0.25 | 25% |
| **Totals** | **20** | N/A | **1.00** | **100%** |

**Interpretation**: 
- 30% of people live in **Oslo** (most common)
- 25% live in **Bergen** and **Stavanger** (tied)
- 20% live in **Kristiansand** (least common)

**Why use relative frequencies?**
- **Easier to communicate**: "30% of people live in Oslo" is clearer than "6 out of 20 people"
- **Better for comparison**: When comparing datasets of different sizes, percentages allow fair comparison
- **Normalized view**: Shows proportions rather than raw counts, which is especially helpful with large datasets

---

#### How to Calculate Relative Frequencies: Excel vs Python

You can calculate relative frequencies using either **Excel** or **Python**. Both methods produce the same results, but each has different strengths.

---

##### Method 1: Excel (Manual Approach)

**Step-by-Step Instructions:**

1. **Enter your data** in Excel:

| A | B |
|---|---|
| ID | Home_City |
| 1 | Oslo |
| 2 | Oslo |
| 3 | Kristiansand |
| 4 | Bergen |
| ... | ... |
| 20 | Stavanger |

2. **Create a frequency count table** (in columns D-E):

| D | E |
|---|---|
| **City** | **Frequency Count** |
| Bergen | =COUNTIF($B$2:$B$21,"Bergen") |
| Kristiansand | =COUNTIF($B$2:$B$21,"Kristiansand") |
| Oslo | =COUNTIF($B$2:$B$21,"Oslo") |
| Stavanger | =COUNTIF($B$2:$B$21,"Stavanger") |

**Result**: E2:E5 will show 5, 4, 6, 5

3. **Calculate total observations** (cell E6):
```
=SUM(E2:E5)
```
**Result**: 20

4. **Calculate relative frequency** (column F):

| F | Formula |
|---|---|
| **Relative Frequency** | |
| =E2/$E$6 | (for Bergen) |
| =E3/$E$6 | (for Kristiansand) |
| =E4/$E$6 | (for Oslo) |
| =E5/$E$6 | (for Stavanger) |

**Note**: The `$` signs lock the total cell reference when copying the formula down.

**Result**: F2:F5 will show 0.25, 0.20, 0.30, 0.25

5. **Calculate percentage** (column G):

| G | Formula |
|---|---|
| **Percentage (%)** | |
| =F2*100 | (for Bergen) |
| =F3*100 | (for Kristiansand) |
| =F4*100 | (for Oslo) |
| =F5*100 | (for Stavanger) |

**Or combine steps 4 & 5**:
```
=E2/$E$6*100
```

**Result**: G2:G5 will show 25.00, 20.00, 30.00, 25.00

6. **Verify**: Add a SUM formula in F6 and G6 to check:
- `=SUM(F2:F5)` should equal **1.00**
- `=SUM(G2:G5)` should equal **100.00**

**Final Excel Table**:

| City | Frequency Count | Relative Frequency | Percentage (%) |
|------|-----------------|-------------------|----------------|
| Bergen | 5 | 0.25 | 25.00 |
| Kristiansand | 4 | 0.20 | 20.00 |
| Oslo | 6 | 0.30 | 30.00 |
| Stavanger | 5 | 0.25 | 25.00 |
| **Total** | 20 | 1.00 | 100.00 |

---

**Excel Tips:**
- Use **Pivot Tables** for larger datasets: Insert â†’ PivotTable â†’ Drag "Home_City" to Rows and Values
- Format cells: Right-click column â†’ Format Cells â†’ Percentage (for column F) or Number with 2 decimals (for column G)
- Use **COUNTIF** for categorical data, **FREQUENCY** for numerical ranges

---

##### Method 2: Python with Pandas (Automated Approach)


```python
import pandas as pd

# Norwegian cities data
data = {
    'Home_City': ['Oslo', 'Oslo', 'Kristiansand', 'Bergen', 'Oslo', 'Stavanger', 
                  'Stavanger', 'Bergen', 'Bergen', 'Kristiansand', 'Oslo', 'Oslo', 
                  'Stavanger', 'Bergen', 'Kristiansand', 'Kristiansand', 'Stavanger', 
                  'Oslo', 'Bergen', 'Stavanger']
}
df = pd.DataFrame(data)

# Step 1: Calculate frequency counts
frequency = df['Home_City'].value_counts().sort_index()
print("Frequency counts:")
print(frequency)

# Step 2: Calculate relative frequencies (as decimals)
relative_freq = frequency / len(df)
print("\nRelative frequencies (as decimals):")
print(relative_freq)

# Step 3: Calculate relative frequencies (as percentages)
relative_freq_percent = (frequency / len(df) * 100).round(2)
print("\nRelative frequencies (as percentages):")
print(relative_freq_percent)

# Step 4: Create comprehensive table
summary_table = pd.DataFrame({
    'City': frequency.index,
    'Frequency Count': frequency.values,
    'Relative Frequency': relative_freq.values.round(2),
    'Percentage (%)': relative_freq_percent.values
})

print("\nComplete Summary Table:")
print(summary_table)

# Verification
print(f"\nTotal observations: {len(df)}")
print(f"Sum of relative frequencies: {relative_freq.sum():.2f}")  # Should be 1.00
print(f"Sum of percentages: {relative_freq_percent.sum():.2f}")  # Should be 100.00
```

**Expected Output**:
```
Frequency counts:
Bergen          5
Kristiansand    4
Oslo            6
Stavanger       5
Name: Home_City, dtype: int64

Relative frequencies (as decimals):
Bergen          0.25
Kristiansand    0.20
Oslo            0.30
Stavanger       0.25
Name: Home_City, dtype: float64

Relative frequencies (as percentages):
Bergen          25.00
Kristiansand    20.00
Oslo            30.00
Stavanger       25.00
Name: Home_City, dtype: float64

Complete Summary Table:
           City  Frequency Count  Relative Frequency  Percentage (%)
0      Bergen              5                  0.25               25.0
1  Kristiansand              4                  0.20               20.0
2         Oslo              6                  0.30               30.0
3     Stavanger              5                  0.25               25.0

Total observations: 20
Sum of relative frequencies: 1.00
Sum of percentages: 100.00
```

---

#### Student Task: Relative Frequencies for Satisfaction Ratings

**Using the student satisfaction data from earlier** (51 students rating learning materials 1-5):

```
3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 4, 2, 1, 3, 5, 2, 2, 3, 4, 4, 4, 5, 1, 3, 
3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 2, 2, 3, 4, 4, 4, 5, 2, 3, 3, 4, 5, 2, 1, 4
```

**Your Task**:
1. Calculate the **relative frequency** (as a decimal) for each rating (1-5)
2. Calculate the **relative frequency as a percentage** for each rating
3. Create a table showing: Rating, Frequency Count, Relative Frequency, Percentage
4. Verify your total: The percentages should sum to 100%
5. **Interpretation**: Which rating represents more than 25% of responses?

**Choose Your Method**: You can use either **Excel** or **Python** to complete this task.

---

##### Solution Option A: Excel Method

**Step 1: Enter the raw data**

Put all 51 ratings in column A (cells A2:A52):

| A |
|---|
| Rating |
| 3 |
| 4 |
| 5 |
| 2 |
| ... |
| 4 |

**Step 2: Create frequency count table**

In columns C-D:

| C | D | Formula for D |
|---|---|---|
| **Rating** | **Frequency Count** | |
| 1 | =COUNTIF($A$2:$A$52,1) | â†’ Result: 5 |
| 2 | =COUNTIF($A$2:$A$52,2) | â†’ Result: 7 |
| 3 | =COUNTIF($A$2:$A$52,3) | â†’ Result: 10 |
| 4 | =COUNTIF($A$2:$A$52,4) | â†’ Result: 14 |
| 5 | =COUNTIF($A$2:$A$52,5) | â†’ Result: 15 |

**Step 3: Calculate total** (cell D7):
```
=SUM(D2:D6)
```
â†’ Result: 51

**Step 4: Calculate relative frequency** (column E):

| E | Formula |
|---|---|
| **Relative Frequency** | |
| =D2/$D$7 | â†’ 0.0980 |
| =D3/$D$7 | â†’ 0.1373 |
| =D4/$D$7 | â†’ 0.1961 |
| =D5/$D$7 | â†’ 0.2745 |
| =D6/$D$7 | â†’ 0.2941 |

**Step 5: Calculate percentage** (column F):

| F | Formula |
|---|---|
| **Percentage (%)** | |
| =E2*100 | â†’ 9.80 |
| =E3*100 | â†’ 13.73 |
| =E4*100 | â†’ 19.61 |
| =E5*100 | â†’ 27.45 |
| =E6*100 | â†’ 29.41 |

**Or use combined formula**:
```
=D2/$D$7*100
```

**Step 6: Verify totals**:
- `=SUM(E2:E6)` should equal **1.00** (or close to it due to rounding)
- `=SUM(F2:F6)` should equal **100.00**

**Step 7: Find ratings > 25%**:
- Look at column F: Ratings **4** (27.45%) and **5** (29.41%) both exceed 25%

**Final Excel Result**:

| Rating | Frequency Count | Relative Frequency | Percentage (%) |
|--------|-----------------|-------------------|----------------|
| 1 | 5 | 0.0980 | 9.80 |
| 2 | 7 | 0.1373 | 13.73 |
| 3 | 10 | 0.1961 | 19.61 |
| 4 | 14 | 0.2745 | 27.45 |
| 5 | 15 | 0.2941 | 29.41 |
| **Total** | 51 | 1.00 | 100.00 |

---

##### Solution Option B: Python Method

**Solution Code Template**:
```python
import pandas as pd

ratings = [3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 4, 2, 1, 3, 5, 2, 2, 3, 4, 4, 4, 5, 1, 3, 
           3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 2, 2, 3, 4, 4, 4, 5, 2, 3, 3, 4, 5, 2, 1, 4]

df = pd.DataFrame({'Rating': ratings})

# YOUR CODE HERE:
# 1. Calculate frequency counts
# frequency = ...

# 2. Calculate relative frequencies (decimal)
# rel_freq = ...

# 3. Calculate percentages
# percentages = ...

# 4. Create summary table
# summary = pd.DataFrame({...})

# 5. Print and verify
# print(summary)
```

<details>
<summary><strong>Click to see solution</strong></summary>

```python
import pandas as pd

ratings = [3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 4, 2, 1, 3, 5, 2, 2, 3, 4, 4, 4, 5, 1, 3, 
           3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 2, 2, 3, 4, 4, 4, 5, 2, 3, 3, 4, 5, 2, 1, 4]

df = pd.DataFrame({'Rating': ratings})

# Step 1: Calculate frequency counts
frequency = df['Rating'].value_counts().sort_index()

# Step 2: Calculate relative frequencies
rel_freq = frequency / len(df)

# Step 3: Calculate percentages
percentages = (frequency / len(df) * 100).round(2)

# Step 4: Create summary table
summary = pd.DataFrame({
    'Rating': frequency.index,
    'Frequency Count': frequency.values,
    'Relative Frequency': rel_freq.round(4).values,
    'Percentage (%)': percentages.values
})

print(summary)
print(f"\nTotal: {len(df)} observations")
print(f"Percentages sum to: {percentages.sum():.2f}%")

# Answer interpretation question
over_25_percent = summary[summary['Percentage (%)'] > 25]
print(f"\nRatings representing > 25% of responses:")
print(over_25_percent)
```

**Result**:
```
   Rating  Frequency Count  Relative Frequency  Percentage (%)
0       1                5              0.0980            9.80
1       2                7              0.1373           13.73
2       3               10              0.1961           19.61
3       4               14              0.2745           27.45
4       5               15              0.2941           29.41

Total: 51 observations
Percentages sum to: 100.00%

Ratings representing > 25% of responses:
   Rating  Frequency Count  Relative Frequency  Percentage (%)
3       4               14              0.2745           27.45
4       5               15              0.2941           29.41
```

**Interpretation**: 
- Ratings **4 (Good)** and **5 (Excellent)** both represent more than 25% of responses
- Combined, they represent **56.86%** of all responses (very positive feedback!)
- Only **9.80%** gave the lowest rating (1 - Very Poor)

</details>

---

#### Excel vs Python: When to Use Which?

Here's a quick guide to help you choose the right tool:

| Aspect | Excel | Python |
|--------|-------|--------|
| **Best for** | Small to medium datasets (<10,000 rows) | Large datasets (>10,000 rows) or complex analysis |
| **Ease of use** | Visual, click-based, beginner-friendly | Code-based, steeper learning curve |
| **Speed** | Fast for small data; slow for large datasets | Very fast, even with millions of rows |
| **Reproducibility** | Manual steps; easy to make errors when repeating | Code is fully reproducible and shareable |
| **Automation** | Limited automation (macros/VBA needed) | Highly automated; can process many files at once |
| **Visualization** | Built-in charts and formatting | Requires libraries (matplotlib, seaborn) but more flexible |
| **Collaboration** | Easy to share .xlsx files | Requires Python installation but code is portable |
| **Cost** | Microsoft Office license required | Free and open-source |

**Recommendation**:
- âœ… **Use Excel** for: Quick one-time analyses, small datasets, sharing with non-technical stakeholders
- âœ… **Use Python** for: Repeated analyses, large datasets, automation, advanced statistical methods, reproducible research

**Pro Tip**: You can combine both! Do exploratory analysis in Excel, then automate your workflow in Python once you know what you need.

---

### Cumulative Frequencies

The **cumulative frequency** for a category is the **running total** of frequencies from the first category up to and including that category. It shows how many observations fall at or below a certain point in your ordered data.

**Formula**:
$$\text{Cumulative Frequency}_i = \sum_{j=1}^{i} \text{Frequency}_j$$

**When to use cumulative frequencies**:
- Categories have a natural order (alphabetical, geographic, temporal, ordinal scales)
- You want to know "how many observations up to this point?"
- Analyzing percentiles or quartiles
- Creating cumulative distribution functions (CDFs)

**Key Point**: Cumulative frequencies change when you reorder categories, so the ordering must be meaningful!

---

#### Example 1: Norwegian Cities (Alphabetical Order)

Using our 20 people across Norwegian cities, let's calculate cumulative frequencies **in alphabetical order**:

| City | Frequency Count | Calculation | Cumulative Frequency |
|------|-----------------|-------------|---------------------|
| Bergen | 5 | Bergen | 5 |
| Kristiansand | 4 | Bergen + Kristiansand | 9 |
| Oslo | 6 | Bergen + Kristiansand + Oslo | 15 |
| Stavanger | 5 | Bergen + Kristiansand + Oslo + Stavanger | 20 |

**Interpretation**:
- 5 people live in cities alphabetically up to "Bergen"
- 9 people live in cities alphabetically up to "Kristiansand" (B-K)
- 15 people live in cities alphabetically up to "Oslo" (B-O)
- 20 people total (cumulative total matches dataset size âœ“)

---

#### Example 2: Norwegian Cities (Geographic Order - North to South)

Now let's reorder the same data **by latitude** (North â†’ South):

| City | Frequency Count | Calculation | Cumulative Frequency |
|------|-----------------|-------------|---------------------|
| Bergen | 5 | Bergen | 5 |
| Oslo | 6 | Bergen + Oslo | 11 |
| Stavanger | 5 | Bergen + Oslo + Stavanger | 16 |
| Kristiansand | 4 | Bergen + Oslo + Stavanger + Kristiansand | 20 |

**Interpretation**:
- 5 people live in Bergen or further north
- 11 people live north of or at the same latitude as Oslo
- 16 people live north of Kristiansand
- **Notice**: The cumulative frequencies changed dramatically just by reordering!

**Key Insight**: 11 people live further north than Stavanger, while 16 live further north than Kristiansand. This geographic perspective provides different insights than alphabetical ordering.

---

#### How to Calculate Cumulative Frequencies: Excel vs Python

---

##### Method 1: Excel (Step-by-Step)

**Step 1: Set up your frequency table** (from previous examples):

| D | E |
|---|---|
| **City** | **Frequency** |
| Bergen | 5 |
| Kristiansand | 4 |
| Oslo | 6 |
| Stavanger | 5 |

**Step 2: Add cumulative frequency column** (column F):

| F | Formula | Result |
|---|---|---|
| **Cumulative Freq** | | |
| 5 | =E2 | 5 |
| 9 | =F2+E3 | 9 |
| 15 | =F3+E4 | 15 |
| 20 | =F4+E5 | 20 |

**Alternative: Using SUM with absolute reference**

| F | Formula | Result |
|---|---|---|
| **Cumulative Freq** | | |
| 5 | =SUM($E$2:E2) | 5 |
| 9 | =SUM($E$2:E3) | 9 |
| 15 | =SUM($E$2:E4) | 15 |
| 20 | =SUM($E$2:E5) | 20 |

**Why this works**: The `$E$2` locks the starting row, while `E2`, `E3`, `E4`, `E5` expand the range as you copy the formula down.

**Step 3: Verify**: The last cumulative frequency should equal your total count (20).

**Complete Excel Table**:

| City | Frequency | Cumulative Freq | Cumulative % |
|------|-----------|----------------|--------------|
| Bergen | 5 | 5 | 25.0 |
| Kristiansand | 4 | 9 | 45.0 |
| Oslo | 6 | 15 | 75.0 |
| Stavanger | 5 | 20 | 100.0 |

**Cumulative % formula** (column G):
```
=F2/$F$5*100
```

---

##### Method 2: Python with Pandas

```python
import pandas as pd

# Norwegian cities data (alphabetical order)
cities_data = {
    'City': ['Bergen', 'Kristiansand', 'Oslo', 'Stavanger'],
    'Frequency': [5, 4, 6, 5]
}
df = pd.DataFrame(cities_data)

# Calculate cumulative frequency
df['Cumulative Freq'] = df['Frequency'].cumsum()

# Calculate cumulative percentage
df['Cumulative %'] = (df['Cumulative Freq'] / df['Frequency'].sum() * 100).round(1)

print("Alphabetical Order:")
print(df)

# Now reorder by latitude (North to South)
cities_geographic = {
    'City': ['Bergen', 'Oslo', 'Stavanger', 'Kristiansand'],
    'Frequency': [5, 6, 5, 4]
}
df_geo = pd.DataFrame(cities_geographic)

# Recalculate cumulative frequency with new order
df_geo['Cumulative Freq'] = df_geo['Frequency'].cumsum()
df_geo['Cumulative %'] = (df_geo['Cumulative Freq'] / df_geo['Frequency'].sum() * 100).round(1)

print("\nGeographic Order (North to South):")
print(df_geo)

# Compare the difference
print("\nKey Insight:")
print(f"People north of Stavanger (geographic): {df_geo[df_geo['City']=='Oslo']['Cumulative Freq'].values[0]}")
print(f"People alphabetically before Stavanger: {df[df['City']=='Oslo']['Cumulative Freq'].values[0]}")
```

**Expected Output**:
```
Alphabetical Order:
           City  Frequency  Cumulative Freq  Cumulative %
0     Bergen          5                5          25.0
1  Kristiansand          4                9          45.0
2         Oslo          6               15          75.0
3     Stavanger          5               20         100.0

Geographic Order (North to South):
           City  Frequency  Cumulative Freq  Cumulative %
0      Bergen          5                5          25.0
1         Oslo          6               11          55.0
2    Stavanger          5               16          80.0
3  Kristiansand          4               20         100.0

Key Insight:
People north of Stavanger (geographic): 11
People alphabetically before Stavanger: 15
```

---

#### Example 3: Exam Grades with Cumulative Frequencies

**Scenario**: Analyze the cumulative distribution of 40 exam grades (A, B, C, D, F) to understand performance thresholds.

**Raw Grade Data** (same as before):
```
B, A, C, B, B, A, F, C, B, A, C, D, B, A, C, B, A, B, C, A, 
D, B, A, C, B, B, A, C, D, A, B, C, A, F, B, A, C, B, A, C
```

**Grade counts**: A=13, B=14, C=9, D=2, F=2

---

**How to Calculate: Excel Method**

**Step 1: Create frequency table** (ordered F â†’ A, worst to best):

| D | E | F | Formula for F |
|---|---|---|---|
| **Grade** | **Frequency** | **Cumulative Freq** | |
| F | 2 | 2 | =E2 |
| D | 2 | 4 | =F2+E3 |
| C | 9 | 13 | =F3+E4 |
| B | 14 | 27 | =F4+E5 |
| A | 13 | 40 | =F5+E6 |

**Or use SUM with expanding range**:
```
=SUM($E$2:E2)  (for F2)
=SUM($E$2:E3)  (for F3)
... and so on
```

**Step 2: Add cumulative percentage** (column G):

| G | Formula |
|---|---|
| **Cumulative %** | |
| 5.0 | =F2/$F$6*100 |
| 10.0 | =F3/$F$6*100 |
| 32.5 | =F4/$F$6*100 |
| 67.5 | =F5/$F$6*100 |
| 100.0 | =F6/$F$6*100 |

**Complete Excel Table**:

| Grade | Frequency | Cumulative Freq | Cumulative % |
|-------|-----------|----------------|--------------|
| F (Fail) | 2 | 2 | 5.0 |
| D (Below Avg) | 2 | 4 | 10.0 |
| C (Average) | 9 | 13 | 32.5 |
| B (Good) | 14 | 27 | 67.5 |
| A (Excellent) | 13 | 40 | 100.0 |

---

**How to Calculate: Python Method**

```python
import pandas as pd

# Raw exam grades
grades = ['B', 'A', 'C', 'B', 'B', 'A', 'F', 'C', 'B', 'A', 'C', 'D', 'B', 'A', 
          'C', 'B', 'A', 'B', 'C', 'A', 'D', 'B', 'A', 'C', 'B', 'B', 'A', 'C', 
          'D', 'A', 'B', 'C', 'A', 'F', 'B', 'A', 'C', 'B', 'A', 'C']

df = pd.DataFrame({'Grade': grades})

# Define grade order (F to A, worst to best)
grade_order = ['F', 'D', 'C', 'B', 'A']
grade_labels = {
    'F': 'Fail',
    'D': 'Below Average',
    'C': 'Average',
    'B': 'Good',
    'A': 'Excellent'
}

# Calculate frequency for each grade
frequency = df['Grade'].value_counts().reindex(grade_order)

# Create summary table
summary = pd.DataFrame({
    'Grade': frequency.index,
    'Label': [grade_labels[g] for g in frequency.index],
    'Frequency': frequency.values,
    'Cumulative Freq': frequency.cumsum().values,
    'Cumulative %': (frequency.cumsum() / len(df) * 100).round(1).values
})

print("Cumulative Grade Distribution:")
print(summary)

# Key insights using cumulative frequencies
print("\nKey Performance Thresholds:")
print(f"Students at D or below: {summary[summary['Grade']=='D']['Cumulative Freq'].values[0]} ({summary[summary['Grade']=='D']['Cumulative %'].values[0]}%)")
print(f"Students at C or below: {summary[summary['Grade']=='C']['Cumulative Freq'].values[0]} ({summary[summary['Grade']=='C']['Cumulative %'].values[0]}%)")
print(f"Students at B or below: {summary[summary['Grade']=='B']['Cumulative Freq'].values[0]} ({summary[summary['Grade']=='B']['Cumulative %'].values[0]}%)")

# Percentile interpretation
print("\nPercentile Interpretation:")
print(f"67.5% of students scored B or lower")
print(f"32.5% of students scored A (top third of class)")
```

**Expected Output**:
```
Cumulative Grade Distribution:
  Grade          Label  Frequency  Cumulative Freq  Cumulative %
0     F           Fail          2                2           5.0
1     D  Below Average          2                4          10.0
2     C        Average          9               13          32.5
3     B           Good         14               27          67.5
4     A      Excellent         13               40         100.0

Key Performance Thresholds:
Students at D or below: 4 (10.0%)
Students at C or below: 13 (32.5%)
Students at B or below: 27 (67.5%)

Percentile Interpretation:
67.5% of students scored B or lower
32.5% of students scored A (top third of class)
```

**Interpretation**:
- **5% failed** (F grade) - 2 students need remedial support
- **10% at D or below** - 4 students are struggling
- **32.5% at C or below** - bottom third of class
- **67.5% at B or below** - two-thirds scored "good" or lower
- **Top 32.5% earned an A** - excellent performers (13 students)

**Educational Insights**:
- ðŸ“Š Grade distribution shows most students in the B-A range (good performance)
- âœ… Only 10% at risk (D or F) - manageable intervention group
- ðŸŽ¯ Median performance is around B (50th percentile falls in B range)
- ðŸ“ˆ Top third performed excellently - consider honors/advanced track

---

#### Student Task: Calculate Cumulative Frequencies for Satisfaction Ratings

**Using the student satisfaction data** (51 students rating learning materials 1-5):

Frequency counts from earlier:
- Rating 1 (Very Poor): 5 students
- Rating 2 (Poor): 7 students  
- Rating 3 (Neutral): 10 students
- Rating 4 (Good): 14 students
- Rating 5 (Excellent): 15 students

**Your Task**:
1. Calculate cumulative frequencies for ratings 1 through 5 (in order)
2. Calculate cumulative percentages
3. Create a complete table showing: Rating, Frequency, Cumulative Freq, Cumulative %
4. **Interpretation**: What percentage of students gave a rating of 3 (Neutral) or lower?

---

**Choose Your Method**:

##### Solution Option A: Excel Method

**Step 1: Set up your table** (columns A-B):

| A | B |
|---|---|
| **Rating** | **Frequency** |
| 1 | 5 |
| 2 | 7 |
| 3 | 10 |
| 4 | 14 |
| 5 | 15 |

**Step 2: Add cumulative frequency** (column C):

| C | Formula |
|---|---|
| **Cumulative Freq** | |
| 5 | =SUM($B$2:B2) |
| 12 | =SUM($B$2:B3) |
| 22 | =SUM($B$2:B4) |
| 36 | =SUM($B$2:B5) |
| 51 | =SUM($B$2:B6) |

**Step 3: Add cumulative percentage** (column D):

| D | Formula |
|---|---|
| **Cumulative %** | |
| 9.8 | =C2/$C$6*100 |
| 23.5 | =C3/$C$6*100 |
| 43.1 | =C4/$C$6*100 |
| 70.6 | =C5/$C$6*100 |
| 100.0 | =C6/$C$6*100 |

**Final Excel Table**:

| Rating | Label | Frequency | Cumulative Freq | Cumulative % |
|--------|-------|-----------|----------------|--------------|
| 1 | Very Poor | 5 | 5 | 9.8 |
| 2 | Poor | 7 | 12 | 23.5 |
| 3 | Neutral | 10 | 22 | 43.1 |
| 4 | Good | 14 | 36 | 70.6 |
| 5 | Excellent | 15 | 51 | 100.0 |

**Answer**: 43.1% of students gave a rating of 3 (Neutral) or lower.

---

##### Solution Option B: Python Method

```python
import pandas as pd

ratings = [3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 4, 2, 1, 3, 5, 2, 2, 3, 4, 4, 4, 5, 1, 3, 
           3, 4, 5, 2, 1, 4, 3, 2, 5, 5, 3, 2, 2, 3, 4, 4, 4, 5, 2, 3, 3, 4, 5, 2, 1, 4]

df = pd.DataFrame({'Rating': ratings})

# Define rating order and labels
rating_order = [1, 2, 3, 4, 5]
rating_labels = {
    1: 'Very Poor',
    2: 'Poor',
    3: 'Neutral',
    4: 'Good',
    5: 'Excellent'
}

# Calculate frequency counts
frequency = df['Rating'].value_counts().sort_index()

# Create comprehensive summary table
summary = pd.DataFrame({
    'Rating': frequency.index,
    'Label': [rating_labels[r] for r in frequency.index],
    'Frequency': frequency.values,
    'Cumulative Freq': frequency.cumsum().values,
    'Cumulative %': (frequency.cumsum() / len(df) * 100).round(1).values
})

print("Cumulative Satisfaction Distribution:")
print(summary)
print(f"\nTotal responses: {len(df)}")

# Answer the question
neutral_or_lower = summary[summary['Rating'] <= 3]['Cumulative Freq'].max()
neutral_or_lower_pct = summary[summary['Rating'] <= 3]['Cumulative %'].max()

print(f"\nAnswer: {neutral_or_lower_pct}% of students gave a rating of 3 (Neutral) or lower")
print(f"That's {neutral_or_lower} out of {len(df)} students")

# Additional insights
print("\nAdditional Insights:")
print(f"Students rating 4 or higher (satisfied): {51-22} ({100-43.1:.1f}%)")
print(f"Students rating 2 or lower (dissatisfied): {summary[summary['Rating']==2]['Cumulative Freq'].values[0]} ({summary[summary['Rating']==2]['Cumulative %'].values[0]}%)")
```

**Expected Output**:
```
Cumulative Satisfaction Distribution:
   Rating      Label  Frequency  Cumulative Freq  Cumulative %
0       1  Very Poor          5                5           9.8
1       2       Poor          7               12          23.5
2       3    Neutral         10               22          43.1
3       4       Good         14               36          70.6
4       5  Excellent         15               51         100.0

Total responses: 51

Answer: 43.1% of students gave a rating of 3 (Neutral) or lower
That's 22 out of 51 students

Additional Insights:
Students rating 4 or higher (satisfied): 29 (56.9%)
Students rating 2 or lower (dissatisfied): 12 (23.5%)
```

**Interpretation**:
- **43.1% rated Neutral or lower** (22 students) - nearly half aren't satisfied
- **56.9% rated Good or Excellent** (29 students) - majority are satisfied
- **23.5% dissatisfied** (ratings 1-2) - this group needs attention
- **70th percentile** falls at rating 4 (Good)
- **Top 29.4%** gave Excellent rating

**Actionable Insights**:
- âš ï¸ 43% neutral-or-lower is concerning - investigate root causes
- âœ… Majority (57%) are satisfied - learning materials are working for most
- ðŸŽ¯ Focus on the 23.5% who are dissatisfied (12 students)
- ðŸ“Š Consider what separates satisfied from unsatisfied students

---

### Binning (Grouping Continuous Data)

**Binning** is the process of dividing continuous numerical values into discrete **bins** or **intervals**. This technique transforms continuous data into categorical data, making it easier to analyze, visualize, and interpret, especially with large datasets.

**Why use binning?**
- Simplifies analysis of continuous variables with many unique values
- Reduces noise and outliers in data
- Makes data easier to visualize (histograms, bar charts)
- Groups similar values together for pattern recognition
- Converts continuous data for methods that require categorical input

**Formula for bin width (equal width binning)**:
$$\text{Bin Width} = \frac{\text{Max Value} - \text{Min Value}}{\text{Number of Bins}}$$

---

#### Example: Norwegian Cities with Age Data

Let's extend our Norwegian cities example by adding an **Age** feature:

| ID | Home_City | Age |
|----|-----------|-----|
| 1 | Oslo | 25 |
| 2 | Oslo | 60 |
| 3 | Kristiansand | 33 |
| 4 | Bergen | 42 |
| 5 | Oslo | 19 |
| 6 | Stavanger | 56 |
| 7 | Stavanger | 34 |
| 8 | Bergen | 57 |
| 9 | Bergen | 43 |
| 10 | Kristiansand | 61 |
| 11 | Oslo | 27 |
| 12 | Oslo | 31 |
| 13 | Stavanger | 52 |
| 14 | Bergen | 29 |
| 15 | Kristiansand | 63 |
| 16 | Kristiansand | 39 |
| 17 | Stavanger | 34 |
| 18 | Oslo | 28 |
| 19 | Bergen | 41 |
| 20 | Stavanger | 47 |

**Data Range**: Ages 18-65 (online survey restrictions)

---

#### Common Binning Methods

There are several approaches to creating bins, each with different use cases:

##### 1. Equal Width Binning

**Concept**: Divide the entire range into intervals of equal size.

**Example**: Ages 0-100 â†’ bins of 10 years each (0-9, 10-19, 20-29, etc.)

**Calculation**:
1. Find range: Max - Min
2. Divide by number of bins: Range / Number of Bins
3. Create bins of that width

**Example with our data**:
- Min age: 18, Max age: 65
- Range: 65 - 18 = 47
- If we want 5 bins: 47 / 5 = 9.4 â‰ˆ 9-10 per bin
- **Note**: When using whole numbers, one bin may be larger (typically the last one)

**Pros**: Simple, easy to understand
**Cons**: Can create bins with very different numbers of observations if data is skewed

---

##### 2. Equal Frequency (Quantile) Binning

**Concept**: Divide data so each bin has roughly the same number of observations.

**Example**: 100 observations â†’ 4 bins of 25 observations each

**Pros**: Each bin equally representative of the sample
**Cons**: Bin widths can vary significantly

---

##### 3. Custom Binning (Domain-Specific)

**Concept**: Use expert knowledge to define meaningful bins.

**Example - Age Groups (Life Stages)**:

| Description | Bin Range | Reasoning |
|-------------|-----------|-----------|
| Young adults | 18-25 | College/early career |
| Adults | 26-35 | Career establishment |
| Mature adults | 36-45 | Career peak |
| Middle-aged | 46-55 | Pre-retirement |
| Senior adults | 56-65 | Near retirement |

**Pros**: Bins have real-world meaning and interpretability
**Cons**: Requires domain expertise, may not be optimal for statistical analysis

---

##### 4. Automated Binning

**Concept**: Use algorithms (Sturges' rule, Scott's rule, Freedman-Diaconis) to determine optimal bin count.

**Sturges' Rule**: Number of bins = $\lceil \log_2(n) + 1 \rceil$ where $n$ = number of observations

**Example**: For 20 observations: $\lceil \log_2(20) + 1 \rceil = \lceil 5.32 \rceil = 6$ bins

---

#### How to Bin Data: Excel vs Python

We'll use **custom binning** with the 5 age groups defined above.

---

##### Method 1: Excel Binning

**Step 1: Set up your data** (columns A-C):

| A | B | C |
|---|---|---|
| ID | Home_City | Age |
| 1 | Oslo | 25 |
| 2 | Oslo | 60 |
| ... | ... | ... |
| 20 | Stavanger | 47 |

**Step 2: Create bin definitions** (columns E-F):

| E | F |
|---|---|
| **Bin Label** | **Range** |
| Young adults | 18-25 |
| Adults | 26-35 |
| Mature adults | 36-45 |
| Middle-aged | 46-55 |
| Senior adults | 56-65 |

**Step 3: Add Age_Group column** (column D) using nested IF:

```excel
=IF(C2<=25, "Young adults",
  IF(C2<=35, "Adults",
    IF(C2<=45, "Mature adults",
      IF(C2<=55, "Middle-aged", "Senior adults"))))
```

**Or use IFS function** (Excel 2016+):
```excel
=IFS(C2<=25, "Young adults",
     C2<=35, "Adults",
     C2<=45, "Mature adults",
     C2<=55, "Middle-aged",
     C2<=65, "Senior adults")
```

**Step 4: Create frequency table** (columns H-I):

| H | I | Formula |
|---|---|---|
| **Age Group** | **Count** | |
| Young adults | 3 | =COUNTIF($D$2:$D$21,"Young adults") |
| Adults | 6 | =COUNTIF($D$2:$D$21,"Adults") |
| Mature adults | 5 | =COUNTIF($D$2:$D$21,"Mature adults") |
| Middle-aged | 3 | =COUNTIF($D$2:$D$21,"Middle-aged") |
| Senior adults | 3 | =COUNTIF($D$2:$D$21,"Senior adults") |

**Result Table**:

| ID | Home_City | Age | Age_Group |
|----|-----------|-----|-----------|
| 1 | Oslo | 25 | Young adults |
| 2 | Oslo | 60 | Senior adults |
| 3 | Kristiansand | 33 | Adults |
| 4 | Bergen | 42 | Mature adults |
| 5 | Oslo | 19 | Young adults |
| ... | ... | ... | ... |

**Frequency Summary**:

| Age Group | Count | Percentage |
|-----------|-------|------------|
| Young adults (18-25) | 3 | 15% |
| Adults (26-35) | 6 | 30% |
| Mature adults (36-45) | 5 | 25% |
| Middle-aged (46-55) | 3 | 15% |
| Senior adults (56-65) | 3 | 15% |

---

##### Method 2: Python Binning

```python
import pandas as pd

# Create the Norwegian cities + age dataset
data = {
    'ID': range(1, 21),
    'Home_City': ['Oslo', 'Oslo', 'Kristiansand', 'Bergen', 'Oslo', 'Stavanger',
                  'Stavanger', 'Bergen', 'Bergen', 'Kristiansand', 'Oslo', 'Oslo',
                  'Stavanger', 'Bergen', 'Kristiansand', 'Kristiansand', 'Stavanger',
                  'Oslo', 'Bergen', 'Stavanger'],
    'Age': [25, 60, 33, 42, 19, 56, 34, 57, 43, 61, 27, 31, 52, 29, 63, 39, 34, 28, 41, 47]
}
df = pd.DataFrame(data)

# Method 1: Using pd.cut() with custom bins
bins = [18, 25, 35, 45, 55, 65]
labels = ['Young adults', 'Adults', 'Mature adults', 'Middle-aged', 'Senior adults']

df['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels, right=True, include_lowest=True)

print("Dataset with Age Groups:")
print(df)

# Calculate frequency distribution of age groups
age_group_freq = df['Age_Group'].value_counts().sort_index()
print("\nAge Group Frequency:")
print(age_group_freq)

# Create comprehensive summary
summary = pd.DataFrame({
    'Age Group': labels,
    'Range': ['18-25', '26-35', '36-45', '46-55', '56-65'],
    'Count': [age_group_freq.get(label, 0) for label in labels],
    'Percentage': [(age_group_freq.get(label, 0) / len(df) * 100).round(1) for label in labels]
})

print("\nAge Group Summary:")
print(summary)

# Find min and max ages
print(f"\nData Range:")
print(f"Minimum age: {df['Age'].min()}")
print(f"Maximum age: {df['Age'].max()}")
print(f"Age span: {df['Age'].max() - df['Age'].min()} years")
```

**Expected Output**:
```
Dataset with Age Groups:
    ID     Home_City  Age      Age_Group
0    1          Oslo   25  Young adults
1    2          Oslo   60 Senior adults
2    3  Kristiansand   33        Adults
3    4        Bergen   42 Mature adults
4    5          Oslo   19  Young adults
...

Age Group Frequency:
Young adults     2
Adults           7
Mature adults    4
Middle-aged      2
Senior adults    5
Name: Age_Group, dtype: int64

Age Group Summary:
       Age Group  Range  Count  Percentage
0  Young adults  18-25      2        10.0
1        Adults  26-35      7        35.0
2 Mature adults  36-45      4        20.0
3   Middle-aged  46-55      2        10.0
4 Senior adults  56-65      5        25.0

Data Range:
Minimum age: 19
Maximum age: 63
Age span: 44 years
```

**Interpretation**:
- **Most common age group**: Adults (26-35) with 7 people (35%)
- **Least common groups**: Young adults and Middle-aged (10% each)
- **Distribution pattern**: Relatively balanced across life stages
- **Data concentration**: 55% of respondents are in Adults and Senior adults groups

**Using Binning for Longitudinal Analysis**:

Binning makes it easier to:
- **Compare results over time**: If this survey is repeated next year, you can easily see if the age distribution remains stable
- **Track demographic shifts**: Monitor whether the Mature adults bin frequency increases while the Adults bin decreases (aging population)
- **Identify trends**: Spot patterns like whether younger age groups are growing or shrinking
- **Standardize comparisons**: Same bins allow year-over-year comparison even with different sample sizes

**Example Year-over-Year Comparison**:

| Age Group | 2025 Count | 2025 % | 2026 Count | 2026 % | Change |
|-----------|------------|--------|------------|--------|--------|
| Young adults | 2 | 10% | 3 | 12% | +1 (+2%) |
| Adults | 7 | 35% | 6 | 24% | -1 (-11%) |
| Mature adults | 4 | 20% | 5 | 20% | +1 (0%) |
| Middle-aged | 2 | 10% | 3 | 12% | +1 (+2%) |
| Senior adults | 5 | 25% | 8 | 32% | +3 (+7%) |

**Insight**: Population is aging - Adults group declining while Senior adults increasing.

---

**Important Note: Bin Choice Affects Insights**

Different binning strategies produce different results and insights. Consider these alternatives:

**Alternative 1: Broader bins** (3 categories)
- Young (18-35): 9 people (45%)
- Middle (36-55): 6 people (30%)
- Senior (56-65): 5 people (25%)

**Alternative 2: Narrower bins** (10-year intervals)
- 18-27: 4 people (20%)
- 28-37: 5 people (25%)
- 38-47: 4 people (20%)
- 48-57: 4 people (20%)
- 58-65: 3 people (15%)

**Takeaway**: Always experiment with different bin ranges to see which provides the most meaningful insights for your analysis. Binning is especially helpful when creating **histograms** - visual representations of frequency distributions.

---

#### Example: Movie Runtime Binning

**Scenario**: You're analyzing a movie database and have 20 movie runtimes (in minutes). You want to categorize them by length.

**Raw Movie Runtimes (minutes)**:
```
125, 98, 105, 134, 90, 102, 145, 87, 115, 129, 96, 122, 135, 110, 140, 85, 100, 132, 95, 130
```

**Analysis Goal**: Group movies into Short, Medium, and Long categories to understand the distribution of movie lengths.

**Custom Bins**:

| Category | Runtime Range | Description |
|----------|---------------|-------------|
| Short | < 100 minutes | Quick viewing |
| Medium | 100-120 minutes | Standard length |
| Long | > 120 minutes | Extended viewing |

---

**How to Calculate: Excel Method**

**Step 1: Enter runtimes** in column A (A2:A21):

| A |
|---|
| **Runtime** |
| 125 |
| 98 |
| 105 |
| ... |
| 130 |

**Step 2: Create Length_Category column** (column B) using nested IF:

```excel
=IF(A2<100, "Short",
  IF(A2<=120, "Medium", "Long"))
```

**Step 3: Create frequency table** (columns D-E):

| D | E | Formula |
|---|---|---|
| **Category** | **Count** | |
| Short | 5 | =COUNTIF($B$2:$B$21,"Short") |
| Medium | 6 | =COUNTIF($B$2:$B$21,"Medium") |
| Long | 9 | =COUNTIF($B$2:$B$21,"Long") |
| **Total** | 20 | =SUM(E2:E4) |

**Step 4: Add percentages** (column F):

| F | Formula |
|---|---|
| **Percentage** | |
| 25% | =E2/$E$5*100 |
| 30% | =E3/$E$5*100 |
| 45% | =E4/$E$5*100 |

**Complete Excel Table**:

| Runtime | Category | | Summary |
|---------|----------|---|---------|
| 125 | Long | | **Category** \| **Count** \| **%** |
| 98 | Short | | Short \| 5 \| 25% |
| 105 | Medium | | Medium \| 6 \| 30% |
| 134 | Long | | Long \| 9 \| 45% |
| 90 | Short | | **Total** \| **20** \| **100%** |
| ... | ... | | |

---

**How to Calculate: Python Method**

```python
import pandas as pd

# Movie runtimes in minutes
runtimes = [125, 98, 105, 134, 90, 102, 145, 87, 115, 129, 96, 122, 135, 110, 140, 85, 100, 132, 95, 130]

df = pd.DataFrame({'Runtime': runtimes})

# Method 1: Using pd.cut() with custom bins
bins = [0, 100, 120, float('inf')]
labels = ['Short', 'Medium', 'Long']

df['Category'] = pd.cut(df['Runtime'], bins=bins, labels=labels, right=False)

print("Movies with Categories:")
print(df)

# Calculate frequency distribution
category_freq = df['Category'].value_counts().reindex(labels)

# Create summary table
summary = pd.DataFrame({
    'Category': labels,
    'Runtime Range': ['< 100 min', '100-120 min', '> 120 min'],
    'Count': category_freq.values,
    'Percentage': (category_freq.values / len(df) * 100).round(1)
})

print("\nRuntime Distribution:")
print(summary)

# Statistical insights
print(f"\nRuntime Statistics:")
print(f"Mean runtime: {df['Runtime'].mean():.1f} minutes")
print(f"Median runtime: {df['Runtime'].median():.1f} minutes")
print(f"Shortest movie: {df['Runtime'].min()} minutes")
print(f"Longest movie: {df['Runtime'].max()} minutes")
print(f"Most common category: {category_freq.idxmax()} ({category_freq.max()} movies)")
```

**Expected Output**:
```
Movies with Categories:
    Runtime Category
0       125     Long
1        98    Short
2       105   Medium
3       134     Long
4        90    Short
5       102   Medium
6       145     Long
7        87    Short
8       115   Medium
9       129     Long
10       96    Short
11      122     Long
12      135     Long
13      110   Medium
14      140     Long
15       85    Short
16      100   Medium
17      132     Long
18       95    Short
19      130     Long

Runtime Distribution:
  Category Runtime Range  Count  Percentage
0    Short     < 100 min      5        25.0
1   Medium   100-120 min      6        30.0
2     Long     > 120 min      9        45.0

Runtime Statistics:
Mean runtime: 113.8 minutes
Median runtime: 113.5 minutes
Shortest movie: 85 minutes
Longest movie: 145 minutes
Most common category: Long (9 movies)
```

**Interpretation**:
- **45% of movies are Long** (> 120 min) - most common category
- **30% are Medium length** (100-120 min) - standard theatrical releases
- **25% are Short** (< 100 min) - quick viewing options
- **Mean â‰ˆ Median**: Symmetric distribution, no significant skew
- **Range**: 60 minutes (85 to 145) - moderate variability

**Practical Insights**:
- ðŸŽ¬ Most movies trend toward longer runtimes (45% > 2 hours)
- ðŸ“Š If planning a movie night, 75% of options are 100+ minutes
- ðŸ• Short movies (< 100 min) are less common - only 25% of collection
- ðŸ“ˆ Distribution is right-skewed toward longer films

---

**Student Task: Bin the Movie Runtimes**

**Your Assignment**: Practice binning by categorizing the 20 movie runtimes yourself.

**Data**: 125, 98, 105, 134, 90, 102, 145, 87, 115, 129, 96, 122, 135, 110, 140, 85, 100, 132, 95, 130

**Steps**:
1. **Choose bins**: Use Short (< 100), Medium (100-120), Long (> 120)
2. **Categorize each movie**: Assign each runtime to the appropriate bin
3. **Count frequencies**: How many movies in each category?
4. **Create a table**: Show Category, Count, and Percentage
5. **Interpret**: Which category is most common? What does this tell you about the movie collection?

**Choose Your Method**:

**Option A: Manual Counting**
- Go through the list and mark each runtime as S, M, or L
- Count how many in each category
- Calculate percentages

**Option B: Excel**
- Enter runtimes in column A
- Use IF formula to categorize
- Use COUNTIF to count frequencies

**Option C: Python**
- Use pd.cut() to bin the data
- Use value_counts() for frequencies
- Create summary table

**Expected Results**:
- Short: 5 movies (25%)
- Medium: 6 movies (30%)
- Long: 9 movies (45%)

**Reflection Questions**:
1. What if you used different bins like (< 90, 90-130, > 130)? Would the insights change?
2. Which binning strategy is more useful for a movie streaming service?
3. How would you bin if you wanted to separate "Epic" films (> 150 min)?

---

#### Example: Binning Exam Scores

**Scenario**: A professor has 40 exam scores (0-100) and wants to bin them into letter grade categories.

**Raw Scores**:
```
78, 92, 65, 88, 71, 45, 95, 73, 82, 91, 68, 55, 84, 93, 76, 81, 90, 69, 87, 94,
58, 85, 92, 77, 80, 83, 91, 72, 61, 89, 86, 74, 93, 52, 79, 90, 70, 84, 91, 75
```

**Custom Bins (Grading Scale)**:

| Grade | Range | Description |
|-------|-------|-------------|
| A | 90-100 | Excellent |
| B | 80-89 | Good |
| C | 70-79 | Average |
| D | 60-69 | Below Average |
| F | 0-59 | Fail |

---

**How to Calculate: Excel Method**

**Step 1: Enter scores** in column A (A2:A41)

**Step 2: Create Grade column** (column B) using nested IF:

```excel
=IF(A2>=90, "A",
  IF(A2>=80, "B",
    IF(A2>=70, "C",
      IF(A2>=60, "D", "F"))))
```

**Step 3: Create frequency table**:

| D | E | Formula |
|---|---|---|
| **Grade** | **Count** | |
| A (90-100) | 13 | =COUNTIF($B$2:$B$41,"A") |
| B (80-89) | 14 | =COUNTIF($B$2:$B$41,"B") |
| C (70-79) | 9 | =COUNTIF($B$2:$B$41,"C") |
| D (60-69) | 2 | =COUNTIF($B$2:$B$41,"D") |
| F (0-59) | 2 | =COUNTIF($B$2:$B$41,"F") |

---

**How to Calculate: Python Method**

```python
import pandas as pd

# Exam scores
scores = [78, 92, 65, 88, 71, 45, 95, 73, 82, 91, 68, 55, 84, 93, 76, 81, 90, 69, 87, 94,
          58, 85, 92, 77, 80, 83, 91, 72, 61, 89, 86, 74, 93, 52, 79, 90, 70, 84, 91, 75]

df = pd.DataFrame({'Score': scores})

# Method 1: Using pd.cut() with custom bins
bins = [0, 60, 70, 80, 90, 100]
labels = ['F', 'D', 'C', 'B', 'A']

df['Grade'] = pd.cut(df['Score'], bins=bins, labels=labels, right=False, include_lowest=True)

print("Scores with Grades:")
print(df.head(10))

# Calculate grade distribution
grade_freq = df['Grade'].value_counts().reindex(['A', 'B', 'C', 'D', 'F'])

# Create summary with grade ranges
grade_summary = pd.DataFrame({
    'Grade': ['A', 'B', 'C', 'D', 'F'],
    'Range': ['90-100', '80-89', '70-79', '60-69', '0-59'],
    'Count': grade_freq.values,
    'Percentage': (grade_freq.values / len(df) * 100).round(1),
    'Cumulative Count': grade_freq.cumsum().values,
    'Cumulative %': (grade_freq.cumsum() / len(df) * 100).round(1).values
})

print("\nGrade Distribution:")
print(grade_summary)

# Statistical insights
print(f"\nStatistical Summary:")
print(f"Mean score: {df['Score'].mean():.1f}")
print(f"Median score: {df['Score'].median():.1f}")
print(f"Minimum score: {df['Score'].min()}")
print(f"Maximum score: {df['Score'].max()}")
print(f"Pass rate (â‰¥60): {(df['Score'] >= 60).sum() / len(df) * 100:.1f}%")
```

**Expected Output**:
```
Scores with Grades:
   Score Grade
0     78     C
1     92     A
2     65     D
3     88     B
4     71     C
5     45     F
6     95     A
7     73     C
8     82     B
9     91     A

Grade Distribution:
  Grade   Range  Count  Percentage  Cumulative Count  Cumulative %
0     A  90-100     13        32.5                13          32.5
1     B   80-89     14        35.0                27          67.5
2     C   70-79      9        22.5                36          90.0
3     D   60-69      2         5.0                38          95.0
4     F    0-59      2         5.0                40         100.0

Statistical Summary:
Mean score: 78.5
Median score: 80.5
Minimum score: 45
Maximum score: 95
Pass rate (â‰¥60): 95.0%
```

**Interpretation**:
- **Binning reveals patterns**: 67.5% scored A or B (excellent/good performance)
- **Mean (78.5) vs Median (80.5)**: Slight left skew due to a few low scores
- **Pass rate**: 95% (38/40 students) passed with 60 or higher
- **Grade distribution**: Relatively normal with most students in B-C range

---

#### Choosing the Right Binning Method

| Method | Use When | Example |
|--------|----------|---------|
| **Equal Width** | Range is known and uniform distribution expected | Temperature ranges, test scores across full 0-100 scale |
| **Equal Frequency** | Need balanced bins for comparison | Quartiles for income data, percentile rankings |
| **Custom** | Categories have real-world meaning | Age groups (child, teen, adult), performance levels |
| **Automated** | Exploratory analysis, no domain knowledge | Initial data exploration, automatic histogram creation |

**Best Practice Tips**:
- âœ… **Know your min/max**: Always check data range before binning
- âœ… **Consider context**: What makes sense for interpretation?
- âœ… **Test multiple approaches**: Try different bin counts and methods
- âœ… **Avoid too many bins**: Defeats the purpose of simplification
- âœ… **Avoid too few bins**: Loses important patterns
- âš ï¸ **Watch bin boundaries**: Decide if boundaries are inclusive (â‰¤) or exclusive (<)

---

#### Python Implementation: Computing Distributions with Pandas

Here's how to calculate frequency counts, relative frequencies, and cumulative frequencies using **Python and the pandas library**. All examples use real, runnable code with expected outputs.

##### Example 1: Frequency Counts

**Scenario**: You have customer satisfaction survey responses (Satisfied, Neutral, Dissatisfied).

**Step 1: Create or load your data**

```python
import pandas as pd

# Create a dataset with survey responses
data = {
    'satisfaction': ['Satisfied', 'Dissatisfied', 'Satisfied', 'Neutral', 
                     'Satisfied', 'Satisfied', 'Dissatisfied', 'Neutral', 
                     'Satisfied', 'Satisfied']
}
df = pd.DataFrame(data)

# View first few rows
print(df.head())
```

**Output:**
```
  satisfaction
0     Satisfied
1  Dissatisfied
2     Satisfied
3       Neutral
4     Satisfied
```

**Step 2: Calculate frequency counts**

```python
# Method 1: Using value_counts()
frequency = df['satisfaction'].value_counts()
print(frequency)
```

**Output:**
```
Satisfied      6
Dissatisfied   2
Neutral        2
Name: satisfaction, dtype: int64
```

**What this means**: "Satisfied" appears 6 times, "Dissatisfied" 2 times, "Neutral" 2 times.

---

##### Example 2: Relative Frequencies (Percentages)

**Using the same data, calculate proportions:**

```python
# Method 1: Divide by total and multiply by 100
relative_freq = (df['satisfaction'].value_counts() / len(df)) * 100
print(relative_freq)

# Method 2: Use normalize=True parameter
relative_freq_alt = df['satisfaction'].value_counts(normalize=True) * 100
print(relative_freq_alt)
```

**Output:**
```
Satisfied      60.0
Dissatisfied   20.0
Neutral        20.0
Name: satisfaction, dtype: float64
```

**What this means**: 60% of responses are "Satisfied", 20% are "Dissatisfied", and 20% are "Neutral".

---

##### Example 3: Cumulative Frequencies

**For ordered categories, calculate cumulative counts:**

```python
# Step 1: Create an ordered dataframe
# First, define the category order (Dissatisfied â†’ Neutral â†’ Satisfied)
category_order = ['Dissatisfied', 'Neutral', 'Satisfied']

# Get frequency counts and sort by category order
frequency = df['satisfaction'].value_counts().reindex(category_order)
print("Frequency counts (ordered):")
print(frequency)

# Step 2: Calculate cumulative frequencies
cumulative = frequency.cumsum()
print("\nCumulative frequencies:")
print(cumulative)
```

**Output:**
```
Frequency counts (ordered):
Dissatisfied    2
Neutral         2
Satisfied       6
Name: satisfaction, dtype: int64

Cumulative frequencies:
Dissatisfied     2
Neutral          4
Satisfied       10
Name: satisfaction, dtype: int64
```

**What this means**: 
- Up to "Dissatisfied": 2 responses
- Up to "Neutral": 4 responses (2 + 2)
- Up to "Satisfied": 10 responses (2 + 2 + 6, which is the total)

---

##### Example 4: Creating a Summary Table

**Combine all three into one table:**

```python
# Create a comprehensive summary table
summary_df = pd.DataFrame({
    'Category': category_order,
    'Frequency': frequency.values,
    'Relative Freq (%)': (frequency.values / len(df)) * 100,
    'Cumulative Freq': cumulative.values,
    'Cumulative % ': (cumulative.values / len(df)) * 100
})

print(summary_df)
```

**Output:**
```
        Category  Frequency  Relative Freq (%)  Cumulative Freq  Cumulative %
0  Dissatisfied          2               20.0                2            20.0
1       Neutral          2               20.0                4            40.0
2      Satisfied          6               60.0               10           100.0
```

**Using this table:**
- **Frequency**: Raw count of each category
- **Relative Freq (%)**: Percentage of total responses
- **Cumulative Freq**: Running total of observations up to that point
- **Cumulative %**: Cumulative percentage (tops out at 100%)

---

##### Example 5: Continuous Data with Binning

**For numerical data, bin values and calculate distributions:**

```python
import pandas as pd
import numpy as np

# Create order value data
orders = {
    'order_value': [15, 32, 48, 51, 75, 82, 95, 120, 145, 180, 
                    25, 40, 65, 110, 160, 55, 90, 135, 175, 22]
}
df_orders = pd.DataFrame(orders)

# Step 1: Create bins
bins = [0, 50, 100, 150, 200]
labels = ['0-49', '50-99', '100-149', '150-199']

# Step 2: Bin the data
df_orders['bin'] = pd.cut(df_orders['order_value'], bins=bins, labels=labels, right=False)

# Step 3: Calculate frequency counts per bin
bin_counts = df_orders['bin'].value_counts().sort_index()
print("Frequency by bin:")
print(bin_counts)

# Step 4: Create binned summary table
binned_summary = pd.DataFrame({
    'Range ($)': bin_counts.index,
    'Count': bin_counts.values,
    'Relative Freq (%)': (bin_counts.values / len(df_orders)) * 100,
    'Cumulative Count': bin_counts.cumsum().values
})

print("\nBinned Summary:")
print(binned_summary)
```

**Output:**
```
Frequency by bin:
0-49        5
50-99       6
100-149     5
150-199     4
Name: bin, dtype: int64

Binned Summary:
  Range ($)  Count  Relative Freq (%)  Cumulative Count
0     0-49      5               25.0                 5
1    50-99      6               30.0               11
2  100-149      5               25.0               16
3  150-199      4               20.0               20
```

---

#### Quick Reference: Common Pandas Functions for Distributions

Here's a handy summary of the key functions you'll use:

| Function | Purpose | Example |
|----------|---------|---------|
| `.value_counts()` | Count occurrences of each category | `df['satisfaction'].value_counts()` |
| `.value_counts(normalize=True)` | Get proportions (not percentages) | `df['satisfaction'].value_counts(normalize=True)` |
| `.cumsum()` | Calculate cumulative sum | `frequency.cumsum()` |
| `pd.cut()` | Bin continuous values into ranges | `pd.cut(df['age'], bins=[0, 18, 65, 100])` |
| `.describe()` | Get summary statistics (mean, std, min, max, etc.) | `df['order_value'].describe()` |
| `.crosstab()` | Create cross-tabulation (frequency table) | `pd.crosstab(df['gender'], df['satisfaction'])` |

**Tip**: To save your results to a CSV file, use:
```python
summary_df.to_csv('distribution_summary.csv', index=False)
```

---

#### Hands-On Exercise: Calculate Distributions from Your Data

**Objective**: Practice calculating frequency, relative frequency, and cumulative frequency distributions.

**Step-by-step:**

1. **Prepare your environment**:
   ```python
   import pandas as pd
   ```

2. **Load your data** (choose one):
   - Create sample data (as shown above)
   - Read from a CSV: `df = pd.read_csv('your_file.csv')`
   - Read from Excel: `df = pd.read_excel('your_file.xlsx')`

3. **Choose a column** and calculate distributions:
   ```python
   # Replace 'column_name' with your actual column
   column = 'column_name'
   
   # Frequency count
   freq = df[column].value_counts().sort_index()
   
   # Relative frequency (%)
   rel_freq = (df[column].value_counts(normalize=True) * 100).sort_index()
   
   # Cumulative frequency
   cum_freq = freq.cumsum()
   
   # Create summary table
   result = pd.DataFrame({
       'Category': freq.index,
       'Frequency': freq.values,
       'Relative Freq (%)': rel_freq.values,
       'Cumulative Freq': cum_freq.values
   })
   
   print(result)
   ```

4. **Interpret the results**:
   - Which category is most common?
   - What percentage of the total does it represent?
   - At what cumulative count do you reach 50% of the data?

**Troubleshooting**:
- **Error: "KeyError"** â†’ Check that `column_name` matches exactly (case-sensitive)
- **No output** â†’ Add `print()` statements to see intermediate results
- **Wrong calculations** â†’ Verify your data has no missing values with `df[column].isnull().sum()`

---

#### Continuous Variable Distributions

For continuous variables, you may **bin** values into ranges to make distributions easier to interpret:

- **Binning** groups continuous values into fewer intervals.
- Helps reveal skewness and shape when the raw data is highly granular.

**Example: Binned Frequency Table (Order Value)**

| Range ($) | Count |
|-----------|-------|
| 0-49 | 18 |
| 50-99 | 26 |
| 100-149 | 14 |
| 150-199 | 7 |
| 200+ | 5 |

**Quick check for normality**: Use a histogram or Q-Q plot to see if values roughly follow a bell curve.

---

#### Common Mistakes When Computing Distributions

**Mistake 1: Forgetting to handle missing values**
```python
# âŒ Wrong: Calculates distribution including NaN values
freq = df['column'].value_counts()

# âœ… Right: Removes missing values first
freq = df['column'].dropna().value_counts()
```

**Mistake 2: Not sorting by a meaningful order**
```python
# âŒ Wrong: Categories sorted by frequency, not logically
freq = df['satisfaction'].value_counts()  # Returns: Satisfied, Dissatisfied, Neutral

# âœ… Right: Define a logical order
order = ['Very Dissatisfied', 'Dissatisfied', 'Neutral', 'Satisfied', 'Very Satisfied']
freq = df['satisfaction'].value_counts().reindex(order)
```

**Mistake 3: Using percentages instead of proportions in normalize**
```python
# âŒ Wrong: Returns 0.25 when you want 25%
rel_freq = df['column'].value_counts(normalize=True)

# âœ… Right: Multiply by 100 if you want percentages
rel_freq = df['column'].value_counts(normalize=True) * 100
```

**Mistake 4: Incorrect bin boundaries**
```python
# âŒ Wrong: Data < 0 or >= 200 gets lost
bins = [0, 50, 100, 150, 200]
df['binned'] = pd.cut(df['value'], bins=bins)

# âœ… Right: Set bin limits beyond your data range
bins = [-float('inf'), 50, 100, 150, float('inf')]
df['binned'] = pd.cut(df['value'], bins=bins)
```

**Mistake 5: Not checking the cumulative total**
```python
# Always verify your cumulative frequency ends at the total count
print(f"Total observations: {len(df)}")
print(f"Max cumulative freq: {cum_freq.iloc[-1]}")  # Should match total
```

---

### 2. Central Tendency

Central tendency describes the typical or central value in a distribution.

**Mean**: Average value, sensitive to outliers.

$$\bar{x} = \frac{\sum x_i}{n}$$

**Median**: Middle value when sorted; robust to outliers.

**Mode**: Most frequent value; useful for categorical data.

---

### 3. Spread (Dispersion)

Spread tells you how much values vary.

**Range**: Max minus min.

**Variance**: Average squared deviation from the mean.

$$s^2 = \frac{\sum (x_i - \bar{x})^2}{n-1}$$

**Standard deviation**: Square root of variance; in the same units as the data.

$$s = \sqrt{s^2}$$

**Interquartile range (IQR)**: $Q3 - Q1$; robust to outliers.

---

### 4. Distribution Shape

Shape indicates how values are distributed.

- **Symmetric**: Mean and median are similar.
- **Right-skewed**: Long tail to the right; mean > median.
- **Left-skewed**: Long tail to the left; mean < median.

---

### 5. Example: Quick Summary

**Sample data (daily orders)**: 5, 7, 7, 9, 12

- Mean = (5 + 7 + 7 + 9 + 12) / 5 = 8
- Median = 7
- Mode = 7
- Range = 12 - 5 = 7

---

### 6. Choosing the Right Summary

- Use **mean + standard deviation** for roughly symmetric numeric data.
- Use **median + IQR** for skewed data or when outliers exist.
- Use **counts and percentages** for categorical data.

---

### ðŸ“š What Did I Learn in This Lesson?

In this lesson, you learned how to calculate and interpret distributions, which are fundamental to understanding your data before building models or making predictions.

**1. Individual Data Attributes as Explanatory/Explained Variables**

You now understand that data attributes (features) play different roles in analysis:
- **Explained variables** (target, outcome, dependent) are what you're trying to predict or understand
- **Explanatory variables** (predictors, independent, features) help explain the variation in the target
- Example: Predicting house prices (explained) using size, bedrooms, and location (explanatory)

This distinction helps you structure your analysis and choose appropriate statistical methods.

**2. Descriptive Statistics Provide Valuable Insights**

You learned that summarizing data with descriptive statistics is essential for:
- Understanding central tendency (mean, median, mode)
- Measuring spread (range, variance, standard deviation, IQR)
- Identifying distribution shape (symmetric, skewed)
- Detecting outliers and unusual patterns
- Choosing appropriate analysis methods

These statistics give you a quick snapshot of your data before diving into complex modeling.

**3. Calculating Distributions is an Initial Step for Gathering Insights**

You now know how to calculate and interpret different types of distributions:

**Frequency Counts**: How often each value or category appears
- Identifies most/least common values
- Detects data quality issues
- Foundation for all other distribution calculations

**Relative Frequencies**: Proportions or percentages
- Enables comparison across datasets of different sizes
- Helps communicate findings clearly (e.g., "60% of customers are satisfied")
- Useful for probability calculations

**Cumulative Frequencies**: Running totals
- Answers "how many up to this point?" questions
- Calculates percentiles and quartiles
- Identifies thresholds (e.g., "75% of students scored below 85")

**Binning**: Grouping continuous values into intervals
- Simplifies complex data for patterns
- Creates meaningful categories (age groups, income brackets)
- Enables longitudinal analysis and trend tracking

**Key Takeaway**: Distribution calculations are the foundation of exploratory data analysis. They help you understand what you're working with, identify problems early, and guide your choice of statistical methods.

---

### âœ… Practice Questions

1. A dataset of salaries is heavily right-skewed. Which summary statistics are most appropriate?
2. For the values 3, 3, 4, 8, 12, compute the mean and median.
3. Which feature type is best summarized with counts and percentages: payment method, height, or response time?
4. Explain why the mean can be misleading when outliers are present.
5. A distribution has mean < median. Is it left-skewed or right-skewed?

#### âœ… Answer Key

1. Median and IQR (robust to skew and outliers).
2. Mean = 6; median = 4.
3. Payment method (categorical).
4. Outliers can pull the mean away from the typical value.
5. Left-skewed.

---

## ðŸš— Comprehensive Task: Car Dealership Distribution Analysis

### The Scenario

You are working as a data analyst at a car dealership that sold 30 cars with the following attributes:
- **Colour**: Red, Blue, Black, White, Silver
- **Type**: Sedan, SUV, Hatchback
- **Price**: Between 82,071 NOK and 890,996 NOK

Your task is to analyze this data by calculating frequency counts, relative frequencies, cumulative frequencies, and binning prices into meaningful categories.

---

### Part 1: Frequency Counts

**Step 1A: Count color frequencies**

| Colour | Frequency |
|--------|-----------|
| Red | 11 |
| Black | 6 |
| Blue | 5 |
| White | 5 |
| Silver | 3 |
| **Total** | **30** |

**Step 1B: Count type frequencies**

| Type | Frequency |
|------|-----------|
| Sedan | 17 |
| Hatchback | 7 |
| SUV | 6 |
| **Total** | **30** |

**Excel Method (COUNTIF)**:
```excel
=COUNTIF($B$2:$B$31, "Red")      â†’ 11
=COUNTIF($B$2:$B$31, "Sedan")    â†’ 17
```

**Python Method**:
```python
import pandas as pd

# Create dataset
df = pd.DataFrame({
    'Colour': ['Black', 'Silver', 'Silver', 'White', 'Black', 'Blue', ..., 'Red'],
    'Type': ['Sedan', 'Hatchback', 'Hatchback', 'Sedan', 'Sedan', 'SUV', ..., 'Sedan'],
    'Price': [547580.14, 82071.31, 795637.68, ..., 594293.78]
})

# Frequency counts
colour_freq = df['Colour'].value_counts().sort_values(ascending=False)
print("Colour Frequency:")
print(colour_freq)

type_freq = df['Type'].value_counts().sort_values(ascending=False)
print("\nType Frequency:")
print(type_freq)
```

**Output:**
```
Colour Frequency:
Red      11
Black     6
Blue      5
White     5
Silver    3

Type Frequency:
Sedan       17
Hatchback    7
SUV          6
```

---

### Part 2: Relative Frequencies

**Step 2A: Calculate color relative frequencies**

| Colour | Frequency | Relative Frequency (%) |
|--------|-----------|------------------------|
| Red | 11 | 36.7% |
| Black | 6 | 20.0% |
| Blue | 5 | 16.7% |
| White | 5 | 16.7% |
| Silver | 3 | 10.0% |
| **Total** | **30** | **100%** |

**Step 2B: Calculate type relative frequencies**

| Type | Frequency | Relative Frequency (%) |
|------|-----------|------------------------|
| Sedan | 17 | 56.7% |
| Hatchback | 7 | 23.3% |
| SUV | 6 | 20.0% |
| **Total** | **30** | **100%** |

**Excel Method**:
```excel
=E2/$E$6*100  where E2 is frequency and E6 is total (30)
```
Formula for Red: `=E2/$E$6*100` â†’ 11/30 Ã— 100 = 36.7%

**Python Method**:
```python
# Relative frequencies (percentages)
colour_rel_freq = (df['Colour'].value_counts() / len(df) * 100).sort_values(ascending=False)
print("Colour Relative Frequency (%):")
print(colour_rel_freq.round(1))

type_rel_freq = (df['Type'].value_counts() / len(df) * 100).sort_values(ascending=False)
print("\nType Relative Frequency (%):")
print(type_rel_freq.round(1))
```

**Output:**
```
Colour Relative Frequency (%):
Red      36.7
Black    20.0
Blue     16.7
White    16.7
Silver   10.0

Type Relative Frequency (%):
Sedan       56.7
Hatchback   23.3
SUV         20.0
```

**ðŸ“Š Interpretation**: Over one-third (36.7%) of sales are red cars. Sedans dominate the inventory at 56.7%, more than half of all sales.

---

### Part 3: Cumulative Frequencies

**Step 3A: Cumulative color frequencies (alphabetical order)**

| Colour | Frequency | Cumulative Frequency | Cumulative % |
|--------|-----------|----------------------|--------------|
| Black | 6 | 6 | 20.0% |
| Blue | 5 | 11 | 36.7% |
| Red | 11 | 22 | 73.3% |
| Silver | 3 | 25 | 83.3% |
| White | 5 | 30 | 100.0% |

**Step 3B: Cumulative type frequencies (alphabetical order)**

| Type | Frequency | Cumulative Frequency | Cumulative % |
|------|-----------|----------------------|--------------|
| Hatchback | 7 | 7 | 23.3% |
| Sedan | 17 | 24 | 80.0% |
| SUV | 6 | 30 | 100.0% |

**Excel Method (Expanding SUM)**:
```excel
Colour Cumulative (Column F):
F2: =E2              â†’ 6
F3: =F2+E3           â†’ 6+5 = 11
F4: =F3+E4           â†’ 11+11 = 22
...and so on

Or use expanding absolute reference:
=SUM($E$2:E2)  (copy down, E2 changes to E3, E4, etc.)
```

**Python Method**:
```python
# Sort colours alphabetically
colour_freq_sorted = df['Colour'].value_counts().reindex(['Black', 'Blue', 'Red', 'Silver', 'White'])

# Cumulative frequency
cumulative_colour = colour_freq_sorted.cumsum()
cumulative_pct = (cumulative_colour / len(df) * 100).round(1)

print("Cumulative Colour Distribution:")
print(f"{'Colour':<10} {'Frequency':<12} {'Cumulative':<12} {'Cumulative %':<12}")
for colour in ['Black', 'Blue', 'Red', 'Silver', 'White']:
    freq = colour_freq_sorted[colour]
    cum = cumulative_colour[colour]
    pct = cum / len(df) * 100
    print(f"{colour:<10} {freq:<12} {cum:<12} {pct:.1f}%")
```

**Output:**
```
Cumulative Colour Distribution:
Colour     Frequency    Cumulative   Cumulative %
Black             6            6         20.0%
Blue              5           11         36.7%
Red              11           22         73.3%
Silver            3           25         83.3%
White             5           30        100.0%
```

**ðŸ“Š Interpretation**: By the time you reach "Red" alphabetically, you've accounted for 73.3% of all cars sold. This shows that red, blue, and black together make up 73.3% of the dealership's inventory.

---

### Part 4: Binning Price Data

**Binning Strategy**: Create meaningful price categories

| Category | Price Range (NOK) |
|----------|------------------|
| Affordable | < 200,000 |
| Mid-range | 200,000 â€“ 350,000 |
| Expensive | 350,000 â€“ 500,000 |
| Luxury | > 500,000 |

**Frequency Distribution by Price Category**:

| Category | Count | Percentage |
|----------|-------|-----------|
| Affordable (< 200K) | 4 | 13.3% |
| Mid-range (200K-350K) | 3 | 10.0% |
| Expensive (350K-500K) | 3 | 10.0% |
| Luxury (> 500K) | 20 | 66.7% |
| **Total** | **30** | **100%** |

**Excel Method (Nested IF)**:
```excel
=IF(D2<200000, "Affordable", IF(D2<350000, "Mid-range", IF(D2<500000, "Expensive", "Luxury")))

Example for Price 547,580.14:
â†’ 547,580.14 is not < 200,000
â†’ 547,580.14 is not < 350,000
â†’ 547,580.14 is not < 500,000
â†’ Result: "Luxury"
```

Then use COUNTIF to count frequency:
```excel
=COUNTIF($F$2:$F$31, "Affordable")    â†’ 4
=COUNTIF($F$2:$F$31, "Mid-range")     â†’ 3
=COUNTIF($F$2:$F$31, "Expensive")     â†’ 3
=COUNTIF($F$2:$F$31, "Luxury")        â†’ 20
```

**Python Method (pd.cut)**:
```python
# Define bins and labels
bins = [0, 200000, 350000, 500000, float('inf')]
labels = ['Affordable', 'Mid-range', 'Expensive', 'Luxury']

# Bin the data
df['PriceCategory'] = pd.cut(df['Price'], bins=bins, labels=labels, right=False)

# Count frequencies
price_freq = df['PriceCategory'].value_counts().reindex(labels)
price_pct = (price_freq / len(df) * 100).round(1)

print("Price Category Distribution:")
print(price_freq)
print("\nPercentages:")
print(price_pct)

# Summary statistics by category
print("\nPrice Statistics by Category:")
print(df.groupby('PriceCategory')['Price'].agg(['count', 'mean', 'min', 'max']))
```

**Output:**
```
Price Category Distribution:
Affordable        4
Mid-range         3
Expensive         3
Luxury           20

Percentages:
Affordable      13.3%
Mid-range       10.0%
Expensive       10.0%
Luxury          66.7%

Price Statistics by Category:
               count      mean        min        max
Affordable         4  149427.00   82071.31  185742.39
Mid-range          3  269388.47  201352.85  271786.76
Expensive          3  446390.47  424513.15  487342.31
Luxury            20  681816.50  506469.01  890996.07
```

---

### Part 5: Business Interpretation

**ðŸ“ˆ Key Finding**: 66.7% (20 out of 30) of cars sold are in the **Luxury category** (above 500,000 NOK).

**What does this tell us about the neighbourhood?**

ðŸ‘‰ **Answer: The dealership is serving a WEALTHY neighbourhood.**

**Reasoning**:
- The dealership's primary inventory consists of luxury vehicles costing over 500,000 NOK
- The average price for luxury cars is 681,816 NOK
- Only 13.3% of sales are in the "Affordable" category (< 200,000 NOK)
- The dealer is not focused on budget-conscious consumers
- This pricing strategy and inventory mix indicates the dealership targets high-income customers in a wealthy area

**Additional Context**:
- Mid-range and Expensive categories combined represent only 20% of sales
- This is a luxury-focused dealership, not a mass-market dealer
- The neighbourhood likely has above-average income levels, allowing residents to purchase premium vehicles
- A low-income neighbourhood would typically see more "Affordable" cars; an average-income area would have more "Mid-range" sales

---

### ðŸ’¡ Summary: What the Data Shows

This car dealership analysis demonstrates all four distribution techniques in a real business context:

1. **Frequency counts** revealed that Red is the most popular colour (11 cars) and Sedans dominate (17 cars)
2. **Relative frequencies** showed these represent 36.7% and 56.7% respectively, making them easy to communicate
3. **Cumulative frequencies** indicated that 73.3% of cars come from just 3 color categories
4. **Binning** categorized continuous price data into meaningful segments, revealing 66.7% of cars are luxury vehicles
5. **Business insight** concluded the dealership serves wealthy customers based on pricing distribution

This is how data analysts use distribution analysis to understand business operations and customer segments!

            """,
            "key_points": [
                "Features are the basic building blocks used to describe outcomes in a data set",
                "Descriptive statistics summarize features to provide quick insights",
                "Different feature types require different descriptive statistics",
                "Central tendency is measured with mean, median, and mode",
                "Spread is measured with range, variance, standard deviation, and IQR",
                "Distribution shape can be symmetric, right-skewed, or left-skewed",
                "Use mean and standard deviation for symmetric data; use median and IQR for skewed data"
            ],
            "visual_elements": {
                "diagrams": True,
                "tables": True,
                "highlighted_sections": True
            }
        }
    ],
    "FI1BBSF05": [
        {
            "lesson_number": "1.1",
            "title": "Microsoft 365 - Excel as a Spreadsheet",
            "content": """
### Introduction

In this session, we'll review the fundamental ideas of spreadsheets and their background. We will focus on **Microsoft Excel**, including its main features, user interface, and organizational structure.

Since we have already been working with data in Excel throughout earlier activities, some components in this lesson may feel familiar. They are intentionally repeated and summarized, but now with **additional context** and **clearer explanations** to strengthen your foundation.

---

### What You Will Learn in This Lesson

- A brief history of spreadsheets and how they evolved
- Why Excel remains one of the most important tools for structured data work
- How Excel's interface is organized (workbook, worksheet, rows, columns, cells, ribbon, formulas bar)
- How spreadsheets support data organization and early-stage analysis
- How Excel compares with alternative spreadsheet tools
- How Excel compares with other data analysis tools

---

### Course Direction

This course begins with a short history of spreadsheets and ends by discussing practical **Excel alternatives**. Along the way, we will continuously contrast Excel with other data-analysis environments and organize new concepts as they are introduced.

By the end of this lesson, you should be able to explain where Excel fits in a modern data workflow and why spreadsheet fundamentals are critical for further analytical work.

---

### History of Spreadsheets

Spreadsheets are popular tools for introductory and intermediate-level data analysis. It is estimated that around **90% of companies** use spreadsheets in financial planning, strategic decision-making, or aggregate forecasting.

It is important to note that spreadsheets are not limited to financial data. They can also be used for simple computations in many other domains, including scientific data analysis, as long as dataset size is manageable and advanced modelling is not required.

So when did spreadsheets begin, and how did they evolve?

The first spreadsheet software was developed in the late 1970s. One of the most successful early programs was **VisiCalc**. Although its developers did not originally use the word â€œspreadsheet,â€ today we would describe it as a straightforward spreadsheet tool.

VisiCalc was released in **1979** for the **Apple II** computer. It was easy to use and made it possible to sort and store data in tabular rows and columns. It was created to replace manual spreadsheet management methods (Heather, 2022).

In many business contexts, financial projection rules repeat each month, quarter, or year. Repeating these tasks manually requires substantial time, effort, and cost. Spreadsheets changed this by allowing users to define basic calculations once and let computers recompute results whenever data changes or new data arrives.

This was tremendously impactful and contributed to the rapid success of spreadsheet software.

Another major advantage was accessibility: many data manipulations and calculations could be programmed relatively simply. As a result, spreadsheets became useful for programmers, analysts, accountants, and financial professionals alike.

Users did not need deep programming or computer science experience to begin building useful models in spreadsheet tools. At the same time, data analysts still benefit from technical understanding of how spreadsheet systems work, especially when working professionally in organizations.

VisiCalc in particular enabled non-programmers to build quantitative models on computersâ€”an important milestone in the democratization of data work.

However, early spreadsheet programs lacked a graphical interface that would make them more user-friendly. This was mainly due to the limitations of operating systems at the time.

With the emergence of **Graphical User Interfaces (GUI)**, visual elements began replacing text-dominated interactions, and working with spreadsheet software became significantly easier.

Excel stood out because it was easier to use than existing alternatives. It introduced features such as:
- **Drop-down menus** (lists of choices shown when users click menu titles)
- **WYSIWYG formatting** (What You See Is What You Get), meaning edited content closely matches how it appears in the final output

Microsoft has updated Excel many times since its original release. These updates continuously improved how calculations are performed, what data types can be handled, and how common business calculations and summaries can be created more efficiently.

Soon after its release, Excel established itself as the leading spreadsheet tool for business and remains one of the most flexible and widely used spreadsheet applications.

Its primary purpose is to support **fundamental to intermediate analysis** and **business presentation workflows**.

---

### Alternative Spreadsheet Software Suites

Excel is, without a doubt, the undisputed leader in spreadsheet applications. It has established a reputation as the preferred tool for many tasks, including financial modelling and data analysis.

However, Excel is not the only breakthrough in the wider field of digital productivity. There are several alternative spreadsheet software suites available, each with distinct strengths and benefits.

On the desktop side, tools such as Airtable, LibreOffice Calc, and Apple Numbers are often seen as strong alternatives to Excel.

In the cloud era, online spreadsheet solutions like Google Sheets and Zoho Sheet have been major game-changers. Compared with many desktop-only tools, online spreadsheet applications have grown significantly in popularity because of easier sharing and collaboration.

#### Common Alternatives

- **Google Sheets**: A cloud-based spreadsheet by Google with functionality similar to Excel and strong real-time collaboration.
- **Apple Numbers**: A spreadsheet tool for Apple devices with a user-friendly interface and integration with Apple apps.
- **LibreOffice Calc**: Part of the open-source LibreOffice suite, with a broad spreadsheet feature set and compatibility with Excel formats.
- **Apache OpenOffice Calc**: An open-source spreadsheet alternative that supports multiple file formats.
- **Zoho Sheet**: A web-based spreadsheet with collaborative functions and integration with Zoho productivity tools.
- **Quip**: A collaborative platform combining documents, spreadsheets, and task management.
- **Airtable**: A flexible, database-driven spreadsheet environment for visual organization and analysis.
- **Smartsheet**: A spreadsheet-style platform focused on project planning and collaboration.
- **WPS Office Spreadsheets**: A free office suite spreadsheet with an interface and feature set similar to Excel.
- **OnlyOffice**: A full office suite offering spreadsheet, document, and presentation tools compatible with Excel files.

These alternatives provide different combinations of compatibility, functionality, and collaboration options, allowing users to choose tools that best match their workflow needs.

In this course, we will focus on **Microsoft 365 Excel**, since much of the core functionality and spreadsheet logic is transferable across most major software suites.

---

### The Spreadsheet Interface

What is a spreadsheet? Imagine a tool that allows us to ingest and view both numerical values and character-string data in a **table-like format** made of **rows (horizontal)** and **columns (vertical)**.

This is what we call a spreadsheet (also referred to as a worksheet).

Each value in a cell can be:
- an independent value entered directly by the user, or
- a calculated value produced by an arithmetic expression, function, or formula.

This cell-based structure is what makes spreadsheet software effective for organizing data, applying logic, and performing repeatable calculations.

---

### Formula Bar

Excel has a dedicated region above the worksheet grid called the **formula bar**. Users can insert formulas and functions to perform calculations, or inspect and edit the contents of the selected cell.

#### Main Features of the Formula Bar

- **Cell reference**: When a cell is selected, its address appears in the formula bar (for example, selecting `B2` shows `B2`).
- **Edit cell content**: Users can click in the formula bar and directly edit the selected cell content; updates are reflected immediately in the worksheet.
- **Formula input**: Users can enter formulas and functions using operators (`+`, `-`, `*`, `/`) and functions such as `SUM`, `AVERAGE`, and `IF`.
- **Formula autocomplete**: As users type, Excel suggests functions and named ranges to reduce typing time and errors.
- **Error messages**: If a formula has a syntax issue or invalid reference, Excel surfaces an error so users can identify and correct it.
- **Formula auditing**: Users can trace precedents and dependents to understand relationships between cells and formulas.
- **Multi-line editing**: Long formulas or text can be edited across multiple lines; users can press `Alt+Enter` to insert line breaks.

In short, the formula bar is the central area for viewing, entering, and editing formulas and functions, enabling data transformation, calculations, and robust spreadsheet logic.

---

### Status Bar

At the bottom of the Excel window is a horizontal strip called the **status bar**. It provides quick access to options and useful worksheet context.

#### Main Components of the Status Bar

- **Ready indicator**: The left side often shows `Ready`, meaning Excel is idle and available for input.
- **Worksheet information**: Depending on settings, it can show context about the active worksheet and selected cell.
- **Calculation mode**: If Excel is in manual calculation mode, the status bar shows an indicator so users know formulas will not recalculate automatically.
- **Num Lock indicator**: Shows when the numeric keypad is active.
- **AutoSum / Average / Count**: When selecting a range, quick aggregate results are displayed directly on the status bar.
- **Zoom slider**: On the right side, users can zoom in or out by adjusting worksheet magnification.
- **View indicators**: Page Layout and Page Break Preview indicators allow fast switching between worksheet views.
- **Customization options**: Right-clicking the status bar opens options to add or remove displayed items.

Excel's status bar helps users monitor worksheet state, perform quick checks, adjust display settings, and personalize what information is visible.

---

### Worksheet vs Workbook

There is a clear distinction between two commonly used spreadsheet terms:

- a **worksheet**, and
- a **workbook**.

A **worksheet** is a single page that contains data in a table of rectangular cells. The selected worksheet name appears on its tab and can be renamed by double-clicking the tab label.

By clicking the **+** button next to sheet tabs, users can add a new worksheet to the current Excel file.

A **workbook** is a collection of worksheets. It consists of one or more sheets, and the sheet tabs are used to navigate between them.

In short:
- Worksheet = one sheet/page of data
- Workbook = the full Excel file containing one or more worksheets

---

### Rows and Columns

In Excel, a worksheet is organized into rows and columns, creating a grid-like structure where data is entered and stored.

#### Rows

Rows are horizontal divisions in a worksheet and are numbered from top to bottom.

- Each row has a unique number shown on the left side of the worksheet.
- Excel provides **1,048,576 rows** by default (from `1` to `1,048,576`).
- Rows are used to organize data horizontally.

#### Columns

Columns are vertical divisions in a worksheet.

- Columns are labeled alphabetically (`A` to `Z`, then `AA`, `AB`, `AC`, and so on).
- Excel provides **16,384 columns** by default (from `A` to `XFD`).
- Columns are used to organize data vertically.

#### Intersection of Rows and Columns

The intersection of a row and a column is called a **cell**.

- Each cell is uniquely identified by column letter + row number.
- Example: `A1` means column `A`, row `1`.

#### Usage in Data Work

Rows and columns are the core structure for entering and managing spreadsheet data.

- Columns often represent variables/fields (for example: Name, Date, Amount).
- Rows often represent records/entries.
- Users can resize rows and columns for readability.
- Rows and columns can be selected, copied, moved, hidden, or deleted as needed.

Excel's grid structure makes it highly effective for calculations, analysis, and presentation by giving users a clear tabular system for organizing and manipulating information.

---

### What Did I Learn in This Lesson?

This lesson provided the following insights:

- We came to understand what a spreadsheet actually is.
- We explored the history of spreadsheets.
- We learned about the basic elements of a spreadsheet interface.
- We examined the basic functionalities of a spreadsheet.
- We learned more about the difference between workbooks and worksheets.
- We explored some spreadsheet alternatives to Excel.

---

### The Task - Answers (Visual Study Guide)

<div style="display:flex; gap:12px; margin: 8px 0 16px 0;">
    <svg width="220" height="90" viewBox="0 0 220 90" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Core Concepts visual card">
        <rect x="1" y="1" width="218" height="88" rx="10" fill="#E8F3FF" stroke="#4A90D9" stroke-width="2"/>
        <text x="14" y="34" fill="#1F4E79" font-size="16" font-weight="700">Core Concepts</text>
        <text x="14" y="58" fill="#1F4E79" font-size="12">Definition â€¢ Uses â€¢ Basics</text>
    </svg>
    <svg width="220" height="90" viewBox="0 0 220 90" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Excel Interface visual card">
        <rect x="1" y="1" width="218" height="88" rx="10" fill="#EAFBF1" stroke="#39A96B" stroke-width="2"/>
        <text x="14" y="34" fill="#1D6B44" font-size="16" font-weight="700">Excel Interface</text>
        <text x="14" y="58" fill="#1D6B44" font-size="12">Ribbon â€¢ Formula Bar â€¢ Status Bar</text>
    </svg>
    <svg width="220" height="90" viewBox="0 0 220 90" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Practice Actions visual card">
        <rect x="1" y="1" width="218" height="88" rx="10" fill="#FFF4E8" stroke="#E58E26" stroke-width="2"/>
        <text x="14" y="34" fill="#8A4A00" font-size="16" font-weight="700">Practice Actions</text>
        <text x="14" y="58" fill="#8A4A00" font-size="12">Resize â€¢ Move â€¢ Hide â€¢ Analyze</text>
    </svg>
</div>

#### Quick Snapshot

| Topic | Fast Answer |
|---|---|
| Spreadsheet | A grid-based tool with rows, columns, and cells for data + formulas |
| Workbook vs Worksheet | Workbook = file, Worksheet = one sheet inside the file |
| Excel Capacity | 1,048,576 rows and 16,384 columns (A to XFD) |
| Cell ID | Column letter + row number (e.g., `B2`) |

---

#### Core Concepts

**1) What is a spreadsheet?**  
A worksheet-based tool that organizes data in rows and columns, where each cell can hold text, numbers, or formulas.

**2) Common applications of spreadsheets**  
Budgeting, financial planning, forecasting, reporting, data cleaning, KPI tracking, inventory management, and basic scientific/statistical analysis.

**3) Basic functionalities**  
Data entry, formatting, sorting/filtering, formulas/functions, charting, summarization, simple analysis, and tabular reporting.

---

#### Excel Interface

**4) Main elements of the Excel menu bar**  
Common ribbon tabs include **File, Home, Insert, Page Layout, Formulas, Data, Review, View,** and **Help** (plus optional add-in tabs).

**5) Main features/functions of the formula bar**  
Shows selected cell reference/content, enables direct editing, formula/function input, autocomplete, error visibility, formula auditing, and multi-line editing.

**6) Information/options on the status bar**  
Ready/Mode indicators, worksheet context, calculation mode status, quick aggregates (Sum/Average/Count), zoom slider, view shortcuts, and customization options.

---

#### Worksheets, Workbooks, and Structure

**7) Difference between worksheet and workbook**  
A worksheet is one sheet of cells; a workbook is the full Excel file containing one or more worksheets.

**8) How to add a new worksheet**  
Click the **+** button next to worksheet tabs.

**9) Deleting the last worksheet**  
Excel does not allow deletion of the last remaining sheet; at least one worksheet must stay in a workbook.

**10) How rows and columns are organized**  
Rows are horizontal and numbered; columns are vertical and lettered.

**11) Default rows and columns in Excel**  
**1,048,576 rows** and **16,384 columns** (A to XFD).

**12) How to identify a specific cell**  
By cell reference: column letter + row number (for example, `B2`, `A1`, `XFD1048576`).

**13) Purpose of rows and columns**  
They provide structure for organizing records (rows) and fields/variables (columns) in a consistent tabular layout.

---

#### Practical Actions

**14) Change column width / row height**  
Drag column/row boundaries, double-click boundaries for AutoFit, or use Home â†’ Format options.

**15) Select, copy, relocate, hide, or remove rows/columns**  
Select row/column headers, then use right-click menu, keyboard shortcuts (Copy/Cut/Paste), drag-and-drop for relocation, and Hide/Unhide or Delete/Insert commands.

**16) Advantages of the grid-like structure**  
It provides clarity, consistency, and scalability for data entry, calculations, analysis, filtering, and presentation.

---

<svg width="680" height="82" viewBox="0 0 680 82" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Progress check banner">
    <rect x="1" y="1" width="678" height="80" rx="10" fill="#F3F0FF" stroke="#7E57C2" stroke-width="2"/>
    <text x="20" y="34" fill="#4A2B87" font-size="18" font-weight="700">Progress Check</text>
    <text x="20" y="56" fill="#4A2B87" font-size="13">Can you explain 1, 4, 7, 11 and 16 without notes? You are mastering Lesson 1.1.</text>
</svg>

#### Motivation Check

If you can explain items **1, 4, 7, 11, and 16** without looking, you already understand the core of this lesson.
            """,
            "key_points": [
                "Spreadsheets are widely used in business planning, strategy, and forecasting",
                "Spreadsheet use extends beyond finance into domains like scientific analysis",
                "VisiCalc (1979, Apple II) was the first major spreadsheet software breakthrough",
                "Spreadsheet automation reduced repetitive manual calculation work",
                "Spreadsheet tools enabled non-programmers to build quantitative models",
                "GUI and WYSIWYG capabilities made spreadsheet software significantly more user-friendly",
                "Excel became the dominant business spreadsheet platform through continuous product updates",
                "Alternative desktop and cloud spreadsheet suites offer different collaboration and compatibility strengths",
                "A spreadsheet interface organizes data in rows, columns, and formula-driven cells",
                "The formula bar is used to view, edit, and build formulas and functions",
                "The status bar provides live worksheet indicators, quick aggregates, and display controls",
                "A worksheet is a single sheet, while a workbook is a file containing one or more worksheets",
                "Rows, columns, and their intersections (cells) form Excel's core data structure"
            ],
            "visual_elements": {
                "diagrams": False,
                "tables": True,
                "highlighted_sections": True
            }
        },
        {
            "lesson_number": "1.2",
            "title": "Entering, Editing and Importing Data",
            "content": """
### 1.2 Lesson - Entering, Editing and Importing Data

### Introduction

Fundamental Excel operations that let you input, change, and import external data into your spreadsheets include entering, editing, and importing data. These steps are essential for efficiently organising, analysing, and presenting data. In this lesson, we will examine the fundamental approaches and procedures for entering, editing, and importing data in Excel.

### Entering Data

The first step in utilising the capability of this flexible spreadsheet program is entering data in Excel. Users can systematically enter and arrange data in Excel, facilitating effective data analysis, calculations, and visualisation. Excel has a user-friendly interface that makes data entry easy, whether you are working with numerical data, text, dates, or formulas. Excel users may unlock the ability to manage and analyse data accurately and precisely by learning the foundations of data entry, such as cell selection, data kinds, and techniques for efficient input.

#### Direct typing

By clicking on a cell, you can enter data immediately. This approach can be used to enter text, numbers, dates, or easy formulas. You can go to the following cell in the chosen direction by pressing Enter or Tab.

#### Formula bar

You can select a cell and enter or edit data in the formula bar at the top of the Excel window. The formula bar allows you to see and modify the contents of the active cell more clearly, especially when dealing with lengthy data.

### Importing and Exporting Datasets

Importing and exporting datasets in Excel is a fundamental skill that allows you to manage and analyse data efficiently. Importing data involves bringing external datasets into Excel, while exporting refers to saving Excel data in a format that can be used by other applications or shared with others.

Importing data into Excel allows you to work with data from various sources such as databases, text files, CSV (Comma Separated Values) files, or other Excel workbooks. This enables you to combine and analyse data from multiple sources, perform calculations, create visualisations, and generate reports.

On the other hand, exporting data from Excel allows you to share your work or collaborate with others who may not have Excel or need the data in a different format. Exporting options include saving Excel files as different file types like CSV files, PDF (Portable Document Format), or XPS (XML Paper Specification), among others. Each format has its advantages depending on the intended use of the exported data.

Importing and exporting data in Excel offers flexibility, versatility, and compatibility in managing and sharing datasets. Whether you need to analyse external data within Excel or distribute your findings to others, mastering these capabilities empowers you to make the most of your data-driven tasks. In the following sections, we will explore the specific methods and techniques for importing and exporting data in Excel.

### To import a CSV dataset

Letâ€™s start by importing a CSV file. This is a very popular way to save files due to its simplicity and the fact that it is widely supported, lightweight, platform-independent and easily integrated with programming languages. To import data in Excel from a CSV (Comma Separated Values) file, you can follow these steps:

1. Open a new or existing Excel workbook.
2. Click on the Data tab in the Excel ribbon menu.
3. Select From Text/CSV from the options.
4. Navigate to the location where your CSV file is stored.
5. Choose the CSV file you want to import and click on the Open button.
6. Excel will open the load page. See Figure 2 - Text import wizard.

### Hereâ€™s how to proceed with the wizard

In the Text Import Wizard, youâ€™ll see a preview of the CSV file data. It allows you to review the data and make necessary adjustments before importing.

#### Show how itâ€™s done

1. Confirm the file origin/encoding so special characters are displayed correctly.
2. Verify the delimiter (for example comma, semicolon, or tab) so columns split properly.
3. Check that headers are recognised and that each column appears in the correct field.
4. If needed, set column data types (Text, Date, Number) to avoid wrong automatic conversions.
5. Click Load to import directly, or use Transform Data to clean/shape the dataset before loading.
6. Choose where to place the imported data (new worksheet or existing location), then finish the import.

### Delimiter check before loading

Select the Delimiter option if itâ€™s not already selected. This indicates that specific delimiters, such as commas or tabs, separate your data. Excel will carry this out automatically, but it is advisable to check to make sure everything is done correctly.

Choose the delimiter that matches the one used in your CSV file (typically a comma). You can see a preview of how your data will be separated based on your selection.

Click on the Load button to import the data.

Excel will import the CSV data and display it in the selected location within your workbook. You can now work with the imported data, apply formulas, create charts, or perform any other desired data analysis tasks.

### Real scenario example

Imagine you are a junior data analyst in a retail company. Every Monday, the sales system exports a CSV file with columns like `Date`, `Store`, `Product`, `Units_Sold`, and `Revenue`.

When you import this file into Excel:

1. You first verify the delimiter is comma so each field lands in the correct column.
2. You check the preview to confirm dates and numbers are not merged into one column.
3. You click Load and place the data in a new worksheet called `Weekly_Sales`.
4. You create a PivotTable to summarise total revenue by store.
5. You add a chart to compare week-over-week performance.

This is a common real-world workflow: import CSV correctly, validate column structure, then analyse and present insights.

### Editing Data

In Excel, the Copy, Cut, Paste, and Delete functions are essential for managing and manipulating data.

### Popular Excel functions

Letâ€™s first discuss the Copy option.
Copying allows you to duplicate the content of a selected cell or range of cells. To copy, select the cell(s) you want to copy and either right-click and choose Copy or use the shortcut Ctrl+C. The copied content is stored in Excelâ€™s clipboard, ready to be pasted elsewhere.

The next option is Cut.
Cutting works similarly to copying, but it removes the selected content from its original location and stores it in the clipboard. To cut, select the cell(s) you want to move, right-click and choose Cut or use the shortcut Ctrl+X. The cut content is temporarily stored in the clipboard until you paste it elsewhere.

The next option is Paste.
Pasting allows you to insert the copied or cut content into a new location in the worksheet. To paste, select the destination cell or range, right-click, and choose Paste or use the shortcut Ctrl+V. Excel pastes the content from the clipboard into the selected location, duplicating or moving the data.

- **Paste Values**: This option pastes only the values from the copied or cut cells, excluding any formatting or formulas.
- **Paste Formulas**: This option pastes the formulas from the copied or cut cells, allowing you to replicate calculations or references to other cells.
- **Paste Formats**: This option pastes only the formatting (e.g., font, colour, borders) from the copied or cut cells without the actual content.
- **Paste Special**: This option provides additional paste options, such as pasting only values, formulas, formats, or specific formatting attributes.

Another option is Delete.
Deleting removes the selected cell(s) or range(s) from the worksheet. To delete, select the cell(s) or range(s) you want to remove and either right-click and choose Delete or use the shortcut Delete key. Excel shifts the remaining cells up or left to fill the empty space created by the deletion.

- **Delete Contents**: This option removes the content from the selected cells while leaving the formatting intact.
- **Delete Cells**: This option removes the content and formatting from the selected cells, shifting the surrounding cells to fill the gap.

### How to select a cell

Selection in Excel refers to the process of choosing a specific cell or range of cells to perform various operations on them. There are two main types of selection in Excel: selecting a single cell and selecting a range of cells.

#### Click

To select a single cell, simply click on the desired cell with the mouse pointer. The selected cell is highlighted, indicating that it is the active cell.

#### Keyboard navigation

Use the arrow keys on the keyboard to navigate and move the selection to different cells. The active cell moves in the direction of the arrow key pressed.

### How to select a range of cells

#### Click and drag

To select a range of cells, click on the starting cell, hold down the mouse button, and drag the mouse to the ending cell of the range. All the cells within the selected range will be highlighted.

#### Shift key

Click on the starting cell, then hold down the Shift key and click on the ending cell of the range. This method is helpful in selecting a range that is not adjacent to each other.

#### Ctrl key

Hold the Ctrl key and click on individual cells to select non-contiguous cells or ranges.

Additionally, there are shortcuts that can help you quickly select cells or ranges:

#### Ctrl+A

Selects the entire worksheet if pressed once. If pressed again, it selects the current region around the active cell.

#### Ctrl+Shift+Arrow Keys

Extends the selection to the last non-empty cell in the direction of the arrow key.

You can carry out various activities on a cell or range once youâ€™ve selected it, including entering data, formatting, copying, cutting, pasting, deleting, or using functions and formulae. You may easily edit data in Excel by using these actions, which have the selected cell or range as their target.

### AutoFill

With Excelâ€™s AutoFill tool, you can rapidly and automatically fill a group of cells with information or formatting based on an established pattern. It is especially helpful when inserting repeated or sequential data into a column or row. AutoFill can be used as follows:

1. Enter the initial value or series of values in a cell.
2. Select the cell(s) containing the value(s) you want to AutoFill.
3. Position the mouse cursor over the small square at the bottom right corner of the selected cell(s). The cursor will change to a thin black crosshair, indicating the AutoFill handle.
4. Click and drag the AutoFill handle across the range where you want the values to be filled.

Excel intelligently detects the pattern of the selected data and automatically fills the remaining cells based on that pattern. The pattern can include numbers, dates, text, or a combination. For example:

#### Sequential numbers

If you enter a sequence like 1, 2, 3 and then drag the AutoFill handle, Excel will fill the cells with the subsequent numbers (4, 5, 6, and so on).

#### Dates

If you enter a date like 01/01/2023 and drag the AutoFill handle, Excel will fill the cells with the subsequent dates in the series.

#### Text

If you enter a word or phrase and drag the AutoFill handle, Excel will fill the cells with the same text.

AutoFill can also be used with specific patterns like weekdays, months, years, custom lists, etc. Additionally, you can customise the AutoFill behaviour by dragging the AutoFill handle with the right mouse button, which opens a menu with different options like copying cells, filling series, formatting only, or creating formulas based on the pattern.

Overall, AutoFill is a handy feature in Excel that saves time and effort by automatically filling cells based on existing data patterns, allowing you to quickly populate a range of cells with consistent and structured information.

### Different Types and Sources

Excel supports importing various types of datasets from different sources. Here are some of the common dataset types and sources that Excel can import:

#### CSV (Comma Separated Values)

CSV files are plain text files that store tabular data, where each value is separated by a comma or other specified delimiter.

#### Excel workbooks

Excel can import data from other Excel workbooks or worksheets. To mention a few, `.XLSM`, `.XLSB`, `.XLS` and `.XLTX`. This allows you to combine or extract data from multiple sources within Excel itself.

#### Text files

Excel can import data from plain text files, such as TXT files. You can specify the delimiter used to separate the data columns.

#### Access databases

Excel can import data from Microsoft Access databases (`.mdb`, `.accdb`). It allows you to choose tables, queries, or entire databases to import.

#### Web pages

Excel can import data from web pages using the From Web feature. You can specify the URL and Excel will extract tables or data from the web page.

#### XML files

Excel can import data from XML (eXtensible Markup Language) files. XML files store structured data, and Excel can map the XML elements to cells.

#### SQL databases

Excel supports importing data from SQL databases, including Microsoft SQL Server, MySQL, and others. You can specify a connection string or use the built-in data connection wizard.

#### SharePoint lists

If you have data stored in SharePoint lists, Excel can connect to the SharePoint site and import the list data into a worksheet.

#### Online services

Excel can connect to various online services, such as Microsoft Azure Data Lake, Power BI, Dynamics 365, and more, allowing you to import data from these services.

#### Other data sources

Excel provides options to import data from other sources like ODBC (Open Database Connectivity) data sources, Microsoft Query, and even external data sources through add-ins or custom connections.

### Demonstrate a few more ways we can import datasets

#### 1) Import from another Excel workbook

1. Go to **Data** â†’ **Get Data** â†’ **From File** â†’ **From Workbook**.
2. Select the source workbook and open it.
3. In the Navigator, choose the sheet or table you need.
4. Click **Load** (or **Transform Data** first if you need cleaning).

Use case: combining monthly reports from different departments into one analysis workbook.

#### 2) Import from a text file (TXT)

1. Go to **Data** â†’ **Get Data** â†’ **From File** â†’ **From Text/CSV**.
2. Select the TXT file.
3. Set the correct delimiter (tab, comma, semicolon, etc.).
4. Confirm encoding and column split in preview.
5. Click **Load**.

Use case: loading system log exports or survey files saved as tab-delimited text.

#### 3) Import from a web page

1. Go to **Data** â†’ **Get Data** â†’ **From Other Sources** â†’ **From Web**.
2. Paste the page URL and confirm.
3. Select the table(s) detected by Excel.
4. Click **Load** to bring the web table into Excel.

Use case: importing public statistics tables (for example exchange rates, population, or market indicators).

#### 4) Import from a SQL database

1. Go to **Data** â†’ **Get Data** â†’ **From Database**.
2. Choose your database type (for example SQL Server or MySQL connector).
3. Enter server/database details and credentials.
4. Select the required table or write a query.
5. Load data to worksheet or the Data Model.

Use case: connecting directly to production sales/order databases for recurring reporting.

#### 5) Import XML data

1. Go to **Data** â†’ **Get Data** â†’ **From File** â†’ **From XML** (or legacy XML import option depending on Excel version).
2. Select the XML file.
3. Map elements to tabular columns if prompted.
4. Load and validate field mapping.

Use case: importing structured exports from ERP/integration systems that provide XML output.

### Review, Transform and Load Data into Excel

When importing a dataset into Excel, the Review and Transform Data feature allows you to perform data cleaning and transformation tasks before the data is loaded into your worksheet. This step is part of the Power Query Editor, a powerful data preparation tool in Excel.

### How to open Power Query Editor

Figure 7: Launch Power Query editor.

1. Ensure that your dataset is loaded in Excel.
2. Select the Data tab.
3. Select Get Data.
4. Navigate down to Launch Power Query Editor.

Figure 8: Dataset loaded into Power Query Editor.

In this view, you can see the imported table in the main preview grid and the **Queries & Connections** pane on the right, which confirms that the query has been loaded and is ready for transformation steps.

### Hereâ€™s an overview of the review and transform data process

#### Accessing Power Query Editor

After selecting the dataset you want to import, you can choose to load it directly into a worksheet or use the Transform Data option to open the dataset in the Power Query Editor.

#### Data preview

In the Power Query Editor, you will see a preview of the imported data. This allows you to inspect the data structure, column names, and sample values to ensure it is imported correctly.

#### Cleaning and transforming data

The Power Query Editor provides a wide range of data transformation options. You can perform tasks such as removing unwanted columns, filtering rows, splitting columns, merging data from multiple sources, changing data types, and applying calculations or formulas to create new columns. Letâ€™s look at a few of these techniques:

##### Removing columns

Select the column to be deleted by right-clicking on the column header and then selecting Remove.

Figure 9: Power Query Editor: Removing a column.

##### Filtering and removing empty rows

Select the column and click the drop-down arrow. Here you can apply the filter options and remove empty rows.

Figure 10: Power Query Editor: Filter options and removing empty rows.

##### Splitting columns

Select the column to be split. Under the Transform tab, select Split Column. Here, you will find multiple options for splitting the content into separate columns.

Figure 11: Power Query Editor: Splitting columns.

##### Merge columns

Select multiple columns to be merged. To do this, click + CTRL for each column. Under the Transform tab, select Merge Columns. Select a separator (if required) and rename the new column.

Figure 12: Power Query Editor: Merging columns.

##### Changing data types

Select the column to be changed. The data type is shown to the left of the column header. By clicking on it, you can change the data type from the drop-down list.

Figure 13: Power Query Editor: Changing data types.

##### Apply calculations to new columns

Select Custom Column under the Add Column tab. Name the new column and create the formula. The available columns are displayed and can be inserted into your calculation as required.

Figure 14: Power Query Editor: Applying calculations to new columns.

##### Adding an Index Column

Select the Index Column drop-down arrow under the Add Column tab. Choose whether to start from 0 or 1. The column will be added as the last column. To move the column, simply click, hold, and drag the column.

Figure 15: Power Query Editor: Adding an index column.

##### Data shape and structure

The Power Query Editor enables you to reshape the data by pivoting, unpivoting, grouping, aggregating, and sorting. These operations help you organise the data to suit your analysis needs.

##### Data quality and error handling

You can address data quality issues by handling missing values, correcting errors, or detecting and removing duplicates. This is simply done by selecting the Remove rows drop-down arrow under the Home tab. The Power Query Editor provides tools for data profiling, data type detection, and data cleansing.

Figure 16: Power Query Editor: Remove rows.

##### Query settings and refresh options

Once you have completed the necessary transformations, you can configure query settings such as column renaming, data type conversions, or sorting orders. You can also set up refresh options to automatically update the data when the source changes.

##### Loading the transformed data

After you finish reviewing and transforming the data, you can choose to load it directly into a new worksheet, an existing worksheet, or the Excel Data Model. Loading the data transfers the transformed dataset into Excel for further analysis and reporting.

Figure 17: Load transformed dataset into Excel.

### ETL Dataset Power Query Editor

Use this setup when you want a repeatable ETL flow (Extract, Transform, Load) in Excel.

#### Setup workflow

1. Open Excel and create a new workbook.
2. Go to the Data tab on the Excel ribbon at the top of the window.
3. Get, Transform, and Load the dataset.
4. Review the imported data in the Power Query Editor window. You can preview, filter, and make any necessary changes to the data.
5. Apply transformations to the dataset using the available options in the Power Query Editor. For example, you can remove unnecessary columns, filter rows based on specific criteria, or change data types.
6. Once you have finished transforming the data, click the Close & Load button in the Power Query Editor. Excel will load the transformed data into a new worksheet or a specified location in your current workbook.

#### Re-open and continue editing

1. To make changes to the data using Power Query, select the imported dataset in Excel.
2. Go to the Data tab and click on the Launch Power Query Editor button in the Get Data section. This will open the Queries & Connections sidebar.
3. Right-click on the dataset and select Edit from the context menu. The Power Query Editor window will open again, allowing you to make further changes to the data.
4. Apply additional transformations or adjustments to the dataset as needed in the Power Query Editor.
5. Once you are satisfied with the changes, click the Close & Load button in the Power Query Editor to load the final transformed data back into Excel.

#### Quick setup checklist (using your CSV)

- Confirm file structure has headers: Title, Developer(s), Publisher(s), Genre, Release date, ref.
- In Power Query, remove empty trailing rows and duplicate header-like rows.
- Set data types explicitly (for example, Text for names/genre, Date where possible for release date).
- Keep Applied Steps readable (rename key steps) so refreshes are easy to audit.
- Use Refresh to update when the source CSV changes.

### Practice Task - df_1.csv ETL Mini-Exercise

Use `df_1.csv` and complete the following in Power Query:

1. Import the CSV with correct delimiter and header recognition.
2. Remove the trailing duplicate header-like row (the row containing `title, developer(s), publisher(s), ...`).
3. Remove rows where `Publisher(s)` is blank.
4. Change `Release date` to Date where possible (leave invalid/ambiguous values as null if needed).
5. Add an Index column starting from 1.
6. Add a Custom Column named `Dev-Pub` that combines `Developer(s)` and `Publisher(s)` with ` - `.
7. Load the transformed result to a new worksheet named `Games_Clean`.

#### Expected answer points (self-check)

- The dataset loads without column-shift issues.
- The duplicate header-like trailing row is removed.
- Blank publishers are filtered out.
- `Release date` has the correct type for parseable dates.
- Index starts at 1 and increments by 1.
- `Dev-Pub` appears with expected combined text values.
- The query is saved and can be refreshed after source-file updates.

### Export Datasets

Exporting datasets in Excel refers to the process of saving data from Excel to an external file format that can be used by other applications or shared with others. Excel provides various options for exporting datasets:

#### Save As

You can use the Save As feature in Excel to export your dataset in different file formats. Click on File in the menu, select Save As, and choose the desired file format, such as Excel workbook (`.xlsx`), CSV (Comma delimited) (`.csv`), or PDF (`.pdf`). Selecting the appropriate file format will determine how the data is structured and presented in the exported file.

Figure 18: Export Excel dataset.

#### Export to PDF

Excel allows you to export your dataset as a PDF file. This is useful when you want to share the data in a non-editable format that preserves formatting and layout. Go to File, select Save As, choose PDF as the file format, and click Save.

#### Copy and paste

You can manually copy the dataset from Excel and paste it into another application, such as Word, PowerPoint, or an email client. Select the cells or range of data you want to export, right-click, choose Copy, go to the desired application, and paste the data.

#### Export to text

Excel allows you to export your dataset as a plain text file (`.txt`). This format helps share data with applications that require simple, delimited text files. Go to File, select Save As, choose Text (Tab delimited) (`.txt`) or CSV (Comma delimited) (`.csv`) as the file format, and click Save.

#### Publish to Web

Excel provides a feature called Publish to Web that allows you to export your dataset as an interactive web page. This is useful for sharing your data online and enabling others to explore and interact with it. Go to File, select Publish to Web, choose the desired options, and click Publish. You will receive a link that you can share with others.

#### Export to database or external system

Excel also supports exporting datasets to databases or external systems. This can be done using specific data integration or export functionalities the target system provides. For example, you can export data from Excel to a Microsoft Access database or import the data into a Customer Relationship Management (CRM) system.

### The Task

#### Question 1

In this activity, we are going to test your understanding of this lesson:

1. Open a new Excel workbook.
2. Use the provided CSV example file: `avocado.csv`.
3. Import the dataset into Excel.
4. Open Power Query Editor.
5. Do the following transformations:
    - Make the first line a heading.
    - Remove the following columns: Index, Total Volume, 4046, 4225, 4770, Total Bags, Type, Year, and Region.
    - Sort by the Date column.
    - Add a column called Index, move it to column A, and number each row from 1 up to the end of the table.
    - Add a column called Total Bags Sold, and add columns Small Bags, Large Bags, and XLarge Bags.
6. Save the Workbook as Avocado Sales.xlsx.
7. Export the Workbook as a CSV file.

#### Show me (worked example using avocado.csv)

1. In Excel: **Data** â†’ **From Text/CSV** â†’ select `avocado.csv`.
2. In the preview, confirm delimiter and click **Transform Data**.
3. In Power Query: **Home** â†’ **Use First Row as Headers**.
4. Remove columns: `Index`, `Total Volume`, `4046`, `4225`, `4770`, `Total Bags`, `Type`, `Year`, `Region`.
5. Sort the `Date` column ascending.
6. Add index: **Add Column** â†’ **Index Column** â†’ **From 1**.
7. Move `Index` to the first position (column A) by dragging it left.
8. Add custom column `Total Bags Sold` with formula:
    `["Small Bags"] + ["Large Bags"] + ["XLarge Bags"]`
9. Click **Close & Load** to return data to Excel.
10. Save workbook as `Avocado Sales.xlsx`, then **File** â†’ **Save As** â†’ `CSV`.

#### Question 2

Let your creativity loose and find a dataset on any website you like.

1. Open a new Excel workbook.
2. Import the dataset(s) from your chosen website.
3. Do basic transformations to ensure a clean and workable dataset.
4. Ensure informative column headings.
5. Remove rows that contain blanks.
6. Remove rows that contain errors.
7. Add or remove columns if required.
8. Save your workbook as an .XLSX.

### Task Solution (Worked Answer)

Provided solution workbook:

- `SPF 0102 Task solution.xlsx` (path: `/workspaces/Study-buddy/SPF 0102 Task solution.xlsx`)
- Use this file to compare your transformation steps and final output against the worked process below.

#### Solved - Question 1 (avocado.csv)

Use this exact workflow to complete Question 1:

1. **Import**
    - Data â†’ From Text/CSV â†’ select `avocado.csv` â†’ click **Transform Data**.

2. **Promote first row to headers**
    - Home â†’ **Use First Row as Headers**.

3. **Remove required columns**
    - Remove: `Index`, `Total Volume`, `4046`, `4225`, `4770`, `Total Bags`, `Type`, `Year`, `Region`.
    - Keep these core columns: `Date`, `AveragePrice`, `Small Bags`, `Large Bags`, `XLarge Bags` (and any other required analysis columns).

4. **Sort by Date**
    - Click the `Date` column filter arrow â†’ Sort Ascending.

5. **Add Index and move to column A**
    - Add Column â†’ Index Column â†’ **From 1**.
    - Drag the `Index` column to the far left.

6. **Add Total Bags Sold column**
    - Add Column â†’ Custom Column
    - Name: `Total Bags Sold`
    - Formula:
      `["Small Bags"] + ["Large Bags"] + ["XLarge Bags"]`

7. **Load and save**
    - Home â†’ **Close & Load**.
    - Save workbook as `Avocado Sales.xlsx`.
    - File â†’ Save As â†’ `CSV (Comma delimited)`.

##### Exact clicks (expanded walkthrough)

Use this detailed sequence if you want to reproduce the final answer exactly:

1. **Data import**
    - Excel â†’ Data tab â†’ **From Text/CSV**.
    - Select `avocado.csv`.
    - In preview, confirm delimiter and encoding look correct.
    - Click **Transform Data** (not Load).

2. **Header + data types**
    - Home â†’ **Use First Row as Headers**.
    - Set types:
      - `Date` â†’ Date
      - `AveragePrice` â†’ Decimal Number
      - `Small Bags`, `Large Bags`, `XLarge Bags` â†’ Decimal Number

3. **Remove columns required by the task**
    - Ctrl-click each of these columns:
      `Index`, `Total Volume`, `4046`, `4225`, `4770`, `Total Bags`, `Type`, `Year`, `Region`.
    - Home â†’ **Remove Columns**.

4. **Sort date**
    - Click the drop-down on `Date`.
    - Choose **Sort Ascending**.

5. **Add task index**
    - Add Column â†’ Index Column â†’ **From 1**.
    - Drag `Index` to the first column position.

6. **Create Total Bags Sold**
    - Add Column â†’ **Custom Column**.
    - Column name: `Total Bags Sold`.
    - Formula:
      `["Small Bags"] + ["Large Bags"] + ["XLarge Bags"]`
    - Confirm the new column is Decimal Number.

7. **Load output**
    - Home â†’ **Close & Load To...**
    - Choose Table â†’ New worksheet.
    - Rename sheet to `avocado` (to match solution file structure).

8. **Save both deliverables**
    - Save workbook as `Avocado Sales.xlsx`.
    - File â†’ Save As â†’ `CSV (Comma delimited)` for export requirement.

##### Validation against the provided solved workbook

Compare your output to `SPF 0102 Task solution.xlsx`:

- Expected solved sheet name: `avocado`
- Expected columns (in order):
  `Index`, `Date`, `AveragePrice`, `Small Bags`, `Large Bags`, `XLarge Bags`, `Total bags Sold`
- Expected first data row pattern:
  `1, 2015-01-04, 1.75, 13061.1, 537.36, 0, 13598.46`
- Expected total rows in solved sheet: `18,250`

##### Troubleshooting tips (common issues)

- **Wrong column split**: go back to Source step and re-check delimiter.
- **Date not sorting correctly**: ensure `Date` type is Date (not Text).
- **Errors in Total Bags Sold**: confirm all three bag columns are numeric.
- **Missing rows after filters**: review Applied Steps for accidental filters.
- **Output differs from solution workbook**: compare column order and data types first.

##### Power Query M script template (advanced)

Use this template in **Advanced Editor** and adjust the source path if needed:

```powerquery
let
    Source = Csv.Document(
        File.Contents("/workspaces/Study-buddy/avocado.csv"),
        [Delimiter=",", Columns=14, Encoding=65001, QuoteStyle=QuoteStyle.Csv]
    ),
    PromotedHeaders = Table.PromoteHeaders(Source, [PromoteAllScalars=true]),
    RemovedColumns = Table.RemoveColumns(
        PromotedHeaders,
        {"Index", "Total Volume", "4046", "4225", "4770", "Total Bags", "Type", "Year", "region"}
    ),
    ChangedTypes = Table.TransformColumnTypes(
        RemovedColumns,
        {
            {"Date", type date},
            {"AveragePrice", type number},
            {"Small Bags", type number},
            {"Large Bags", type number},
            {"XLarge Bags", type number}
        }
    ),
    SortedByDate = Table.Sort(ChangedTypes, {{"Date", Order.Ascending}}),
    AddedIndex = Table.AddIndexColumn(SortedByDate, "Index", 1, 1, Int64.Type),
    ReorderedColumns = Table.ReorderColumns(
        AddedIndex,
        {"Index", "Date", "AveragePrice", "Small Bags", "Large Bags", "XLarge Bags"}
    ),
    AddedTotalBagsSold = Table.AddColumn(
        ReorderedColumns,
        "Total Bags Sold",
        each [#"Small Bags"] + [#"Large Bags"] + [#"XLarge Bags"],
        type number
    )
in
    AddedTotalBagsSold
```

If your CSV has `Region` (capital R) instead of `region`, replace that field name in `RemovedColumns` accordingly.

##### Expected result check

- `Index` starts at 1 and increments by 1.
- `Date` is sorted ascending.
- Removed columns are no longer present.
- `Total Bags Sold` equals `Small Bags + Large Bags + XLarge Bags` for every row.
- Workbook and CSV export both exist.

##### Reference solution file (verified)

Official solution workbook: `SPF 0102 Task solution.xlsx`

- Main solved sheet: `avocado`
- Structure: 7 columns and 18,250 rows
- Header row:
    - `Index`
    - `Date`
    - `AveragePrice`
    - `Small Bags`
    - `Large Bags`
    - `XLarge Bags`
    - `Total bags Sold`
- Example first data row:
    - `1, 2015-01-04, 1.75, 13061.1, 537.36, 0, 13598.46`

You can use this workbook to compare your result after completing the ETL steps.

#### Solved - Question 2 (example approach)

If you choose any public dataset, use this clean baseline solution:

1. Import dataset with correct delimiter/headers.
2. Promote headers and rename unclear column names.
3. Remove blank rows and obvious error rows.
4. Fix column data types (Text, Number, Date).
5. Remove irrelevant columns and add any required derived column.
6. Sort by one meaningful business field (for example Date).
7. Close & Load, then save as `.xlsx`.

##### Example answer quality criteria

- Headings are readable and consistent.
- No blank/error rows remain in core fields.
- Data types are correctly assigned.
- Dataset is ready for pivot tables/charts without additional cleaning.
            """,
            "key_points": [
                "Entering data is a core operation for building usable worksheets",
                "Editing data helps keep spreadsheet content accurate and up to date",
                "Importing data enables integration of external data sources into Excel",
                "Exporting data to formats like CSV, PDF, and XPS improves sharing and compatibility",
                "CSV import through Data > From Text/CSV supports preview, delimiter checks, and controlled loading",
                "Copy, Cut, Paste, and Delete are core editing operations in Excel",
                "Cell/range selection techniques and shortcuts improve editing speed and precision",
                "AutoFill can quickly extend sequences, dates, and repeated patterns",
                "Excel can import datasets from CSV, text files, workbooks, databases, web pages, XML, and online services",
                "Get Data workflows support both direct loading and pre-load transformation",
                "Power Query Editor enables review, cleaning, and transformation before loading data",
                "Query settings and refresh options help keep transformed datasets up to date",
                "ETL in Excel follows a repeatable cycle: import, transform, close and load, re-open, refine, and refresh",
                "Practical ETL tasks help validate import quality, cleaning logic, type handling, and refresh readiness",
                "Use Save As to export transformed datasets to formats like XLSX, CSV, and PDF",
                "Excel supports additional export paths including copy/paste workflows, text formats, web publishing, and external systems",
                "A complete worked solution includes import, cleaning, transformation, validation, and export checks",
                "These operations support efficient data organisation, analysis, and presentation"
            ],
            "visual_elements": {
                "diagrams": False,
                "tables": False,
                "highlighted_sections": True
            }
        },
        {
            "lesson_number": "1.3",
            "title": "Basic Formatting in Excel",
            "content": """
### 1.3. Lesson - Basic Formatting in Excel

### Introduction

The primary methods used to alter the appearance of cells and data within a spreadsheet are referred to as basic formatting in Excel.

Your Excel paperâ€™s visual appeal, readability, and general professionalism are greatly improved by formatting.

You may highlight vital information, efficiently organise data, and present it clearly and organised by using simple formatting options, including font styles, colours, cell borders, and alignment settings.

This primerâ€™s discussion of fundamental Excel formatting principles will give you the information and abilities to make your spreadsheets more aesthetically pleasing and understandable.

### Cell Data Formats

How data is displayed within individual cells in Excel is called cell data formats. With the help of Excelâ€™s many formatting choices, users may alter how various types of data - including numbers, dates, currencies, percentages, and more - appear.

These formats not only influence how the data is displayed but also make it possible for users to do calculations and handle data easily. Users can ensure that numbers are displayed with the correct number of decimal places and dates are formatted in accordance with regional standards. Currencies are displayed with the proper symbols and separators by applying the necessary data formats.

Right-click on the cell or a group of cells you wish to format, and then choose Format Cells from the context menu. Under the Home tab, basic formatting is also accessible.

### Numeric cell formats

In Excel, altering the appearance of numerical data within cells is called number formatting. When displaying numbers, users can adjust the decimal places, thousand separators, currency symbols, and percentage forms. For data to be presented accurately and understandably, number formatting is essential. Refer to Figure 1:

Figure 1: Excel number formatting options.

Excel provides a wide range of pre-defined number formats to cater to different needs. Some common number formats are shown in Figure 2:

Figure 2: Examples of numeric formatting.

#### Common pre-defined number formats

**General**  
This is the default format in Excel, where numbers are displayed as entered.

**Number**  
This format is used for general numeric values and allows users to specify decimal places, separators, and negative number formatting.

**Currency**  
This format is used for displaying monetary values and includes options for currency symbols, decimal places, and negative numbers.

**Accounting**  
Similar to the currency format, the accounting format aligns currency symbols and decimal places for better visual appeal.

**Date**  
This format is used for displaying dates and offers various options for date formats, such as month/day/year or day/month/year, depending on regional conventions.

**Time**  
This format is used for displaying time values and allows users to control the appearance of hours, minutes, and seconds.

**Percentage**  
This format multiplies the cell value by 100 and adds a percentage symbol (%).

**Scientific**  
This format is used for displaying numbers in scientific notation, useful for large or small values.

Excel has the versatility to generate unique number forms in addition to these pre-defined formats. Users can create their own formatting rules for numbers by combining various symbols, characters, and codes to produce desired formatting effects.

Excelâ€™s number formatting influences computations and formulas in addition to their visual appearance. Although the formatting changes how the data is shown, the underlying data is unaffected. It is important to remember that number formatting does not impact the dataâ€™s calculations or real value.

### Text cell formats

In Excel, text cell formatting describes the process of changing the way text behaves and appears inside cells. Text formatting, as opposed to number formatting, which deals with numerical data, is concerned with how text is presented, aligned, wrapped, and styled. Examples are demonstrated in figures 3-5 below:

Figure 3: Excel text formatting options.

Figure 4: Excel alignment formatting options.

Figure 5: Excel border formatting options.

In Excel, text cell formatting options allow users to make their text more visually appealing and easier to read. Some common text formatting features include:

#### Font styles

Excel provides a variety of font styles, such as bold, italic, underline, and strikethrough, to emphasise or highlight specific text.

#### Font colours

Users can change the colour of the text to make it stand out or match a particular theme or visual preference.

#### Font size

Excel allows users to adjust the text size to make it more prominent or fit within a designated space.

#### Alignment

Text alignment options enable users to control how the text is positioned within a cell, such as left-aligned, right-aligned, centred, or justified.

#### Text wrapping

With text wrapping, users can specify whether the text should wrap within a cell or overflow to adjacent cells, making it easier to read lengthy text entries.

Figure 6: Example of text wrapping.

#### Indentation

Users can apply indentation to align text within cells, such as using a hanging indent for bullet points or nested lists.

#### Borders

Excel offers border styles to create visual boundaries around cells, enabling users to separate or group text.

Figure 7: Example of cell borders.

#### Cell protection

Text cell formatting also includes options for cell protection, allowing users to lock or unlock cells to prevent accidental changes.

### The Task - Cell formatting in Excel

In this activity, you will practice basic cell formatting in Excel, specifically numeric and text formatting. Follow the instructions provided below to complete the tasks.

#### Task 1: Numeric formatting

1. Open a new Excel workbook.
2. In cell `A1`, enter the number `1234.56`.
3. Apply the **Currency** format to cell `A1`, displaying the number with a currency symbol and two decimal places.
4. In cell `B1`, enter the percentage value `0.75`.
5. Apply the **Percentage** format to cell `B1`, displaying the number as a percentage with two decimal places.
6. In cell `C1`, enter the date `May 10, 2023`.
7. Apply the **Short Date** format to cell `C1`, displaying the date in the format `MM/DD/YYYY`.

#### Task 2: Text formatting

1. In cell `A3`, enter the text `Sales Report`.
2. Apply **bold** formatting to the text in cell `A3`.
3. In cell `B3`, enter the text `Product Name`.
4. Apply **underline** formatting to the text in cell `B3`.
5. In cell `C3`, enter the text `Quantity Sold`.
6. Apply **italic** formatting to the text in cell `C3`.
7. In cell `D3`, enter the text `Total Revenue`.
8. Apply **strikethrough** formatting to the text in cell `D3`.

#### Task 3: Data alignment

1. Select cells `A1` to `D3`.
2. Apply **centre alignment** to the selected cells.
3. Adjust the column widths to fit the content in each cell.

#### Task 4: Cell border formatting

1. Select cells `A1` to `D3`.
2. Apply a border around the selected cells.
3. Adjust the border style and thickness as desired.

### Visual solved example (with explanation)

Use this exact click path to complete and verify the task quickly:

1. Enter values in `A1`, `B1`, `C1`.
2. Format `A1`: Home â†’ Number group â†’ Currency (2 decimals).  
    **Why:** makes monetary values clear and consistent.
3. Format `B1`: Home â†’ Number group â†’ Percentage (2 decimals).  
    **Why:** converts decimal ratios to readable percentages.
4. Format `C1`: Home â†’ Number group â†’ Short Date (`MM/DD/YYYY`).  
    **Why:** standardises date presentation.
5. Enter headings in row 3 (`A3:D3`) and apply bold/underline/italic/strikethrough as requested.  
    **Why:** demonstrates common text emphasis tools.
6. Select `A1:D3` â†’ Center alignment.  
    **Why:** creates a clean, report-like layout.
7. With `A1:D3` selected, apply `All Borders` (or Outside + Inside borders) and choose preferred line style.  
    **Why:** visually separates data blocks and improves readability.
8. AutoFit columns `A:D` (double-click column boundaries).  
    **Why:** ensures no clipped text or values.

#### Expected final view

| Cell | Input | Required format | Expected display example |
|---|---|---|---|
| A1 | 1234.56 | Currency, 2 decimals | $1,234.56 |
| B1 | 0.75 | Percentage, 2 decimals | 75.00% |
| C1 | May 10, 2023 | Short Date | 05/10/2023 |
| A3 | Sales Report | Bold | **Sales Report** |
| B3 | Product Name | Underline | <u>Product Name</u> |
| C3 | Quantity Sold | Italic | *Quantity Sold* |
| D3 | Total Revenue | Strikethrough | ~~Total Revenue~~ |

#### Quick self-check

- Currency has symbol and two decimals.
- Percentage shows two decimals.
- Date is shown as short date.
- A1:D3 is centered and bordered.
- Columns are wide enough to show all content.

### Sorting and Filtering

Excelâ€™s sophisticated sorting and filtering functions let users efficiently organise and analyse data. These capabilities allow users to extract pertinent information based on predetermined criteria, spot trends, and arrange data in a particular way. Letâ€™s delve deeper into sorting and filtering:

Figure 8: Excel sorting and filtering options.

### Sorting data

Sorting data in Excel rearranges the rows based on the values in one or more columns. Sorting can be done in ascending (smallest to largest) or descending (largest to smallest) order. Refer to Figure 8 to sort data in Excel:

1. Select the range of cells or the entire table.
2. Navigate to the Data tab and click on the Sort button.
3. Specify the column(s) to sort by and the sort order.
4. Excel will rearrange the data based on the selected criteria, ensuring all rows remain intact.

For activities like alphabetising names, ranking data, or locating the greatest or lowest values within a dataset, sorting is especially helpful. Excel additionally enables multi-level sorting, which arranges data in hierarchical order according to many columns.

### Filtering data

Filtering data in Excel displays only the rows that meet specific criteria, temporarily hiding the rest of the data. This helps users focus on relevant information and extract subsets of data. To apply filters to data in Excel:

1. Select the range of cells or the entire table.
2. Go to the Data tab and click on the Filter button.
3. Drop-down arrows will appear in the header row of each column.
4. Click on the drop-down arrow of a column to set filter criteria, such as text filters, number filters, date filters, or advanced filters.
5. Excel will display only the rows that meet the specified criteria while hiding the others.

Applying many filters at once can further refine filtered data. Excel also offers tools for sorting data inside the results of filtering, allowing users to focus their investigation efficiently.

For activities like locating duplicate entries, examining patterns within a dataset, or isolating data based on certain criteria, including date periods or particular values, filtering data is useful.

### Advanced filtering

Excel offers advanced filtering options for more complex filtering requirements. Advanced filtering allows users to define custom criteria using formulas, extract unique records, or copy filtered data to a new location within the worksheet or another sheet.

To access advanced filtering options, users can navigate to the Data tab, click Advanced in the Sort & Filter group, and specify the criteria in the Advanced Filter dialog box.

Advanced filtering is particularly useful for scenarios with insufficient standard filters, such as filtering data based on multiple conditions or extracting unique values from a large dataset.

Users may manage and analyse vast amounts of data, spot patterns, and derive useful insights using Excelâ€™s sorting and filtering features. These functions give users flexibility and control over how data is presented, allowing them to engage with the data in a way that best matches their individual needs and improves data-driven decision-making.

### Sorting and Filtering in Excel - Practice Activity

In this activity, you will practice sorting and filtering data in Excel. Follow the instructions provided below to complete the tasks.

#### Task 1: Sorting data

1. Open a new Excel workbook.
2. Enter the following data into columns `A`, `B`, and `C`, starting from row `1`:
    - Column `A`: Student Name (`Alex`, `Ben`, `Claire`, `David`, `Emma`)
    - Column `B`: Age (`21`, `19`, `22`, `20`, `21`)
    - Column `C`: Grade (`A`, `B`, `C`, `B`, `A`)
3. Select the entire dataset (`A1:C6`).
4. Sort the data in ascending order based on the `Student Name` column.
5. Sort the data in descending order based on the `Age` column.
6. Sort the data in alphabetical order based on the `Grade` column.

#### Task 2: Filtering data

1. Select the entire dataset (`A1:C6`).
2. Apply filters to the dataset.
3. Filter the data to display only students who are `21` years old.
4. Filter the data to display only students with a grade of `A`.
5. Filter the data to display only students whose names start with the letter `B`.

#### Task 3: Multiple criteria filtering

1. Select the entire dataset (`A1:C6`).
2. Apply filters to the dataset if they are not already applied.
3. Filter the data to display only students who are `20` years old **and** have a grade of `B`.

#### Task 4: Removing filters

1. Select the entire dataset (`A1:C6`).
2. Remove all filters from the dataset.

### Tutor solution (step-by-step with expected results)

#### Step 0: Build the dataset correctly

Enter this exact table:

| Student Name | Age | Grade |
|---|---:|:---:|
| Alex | 21 | A |
| Ben | 19 | B |
| Claire | 22 | C |
| David | 20 | B |
| Emma | 21 | A |

Select `A1:C6` before each sort/filter operation.

#### Step 1: Sort by Student Name (Aâ†’Z)

1. Data â†’ Sort.
2. Sort by: `Student Name`.
3. Order: `A to Z`.
4. Click OK.

**Expected order:** Alex, Ben, Claire, David, Emma.

#### Step 2: Sort by Age (Largestâ†’Smallest)

1. Data â†’ Sort.
2. Sort by: `Age`.
3. Order: `Largest to Smallest`.
4. Click OK.

**Expected order by Age:** 22, 21, 21, 20, 19.

#### Step 3: Sort by Grade (Aâ†’Z)

1. Data â†’ Sort.
2. Sort by: `Grade`.
3. Order: `A to Z`.
4. Click OK.

**Expected grouping:** `A` rows first, then `B`, then `C`.

#### Step 4: Apply filters

1. Data â†’ Filter (funnel icon).
2. Confirm drop-down arrows appear in row 1 headers.

#### Step 5: Filter for Age = 21

1. Click filter on `Age`.
2. Clear Select All.
3. Check only `21`.
4. Click OK.

**Expected visible rows:** Alex and Emma.

#### Step 6: Filter for Grade = A

1. Clear previous Age filter (Age drop-down â†’ Clear Filter from Age).
2. Click filter on `Grade`.
3. Select only `A`.
4. Click OK.

**Expected visible rows:** Alex and Emma.

#### Step 7: Filter names starting with B

1. Clear previous Grade filter.
2. Click filter on `Student Name`.
3. Text Filters â†’ Begins With...
4. Enter `B` and click OK.

**Expected visible row:** Ben only.

#### Step 8: Multiple criteria (Age = 20 and Grade = B)

1. Clear all current filters.
2. Set `Age` filter to `20`.
3. Set `Grade` filter to `B`.

**Expected visible row:** David only.

#### Step 9: Remove all filters

Option A: Data â†’ Clear (clears criteria, keeps filter arrows).  
Option B: Data â†’ Filter (turns filter mode off and removes arrows).

**Expected final state:** all 5 student rows are visible again.

#### Common mistakes (and fixes)

- Sorting only one column: always select the full table (`A1:C6`) so rows stay intact.
- Missing header checkbox: in Sort dialog, keep **My data has headers** enabled.
- Stacked filters by accident: use Data â†’ Clear before starting a new filter scenario.
- Wrong result count: check bottom status bar to verify visible row count.

### Window Formatting

Window formatting in Excel refers to the customisation and adjustment of the visible portion of the worksheet within the Excel application window. It allows users to modify the worksheetâ€™s view, zoom level, and layout to suit their preferences and optimise their working environment. Letâ€™s explore the various window formatting options in Excel.

### Zooming

Zooming in Excel adjusts the magnification level of the worksheet, making the content appear larger or smaller on the screen. It helps users adjust the level of detail visible on the screen. Zooming options can be accessed through the View tab or the Zoom slider in the bottom-right corner of the Excel window.

### Freeze panes

Freezing panes in Excel allow users to lock specific rows or columns in place while scrolling through large datasets. By freezing panes, users can keep important headings or labels visible at all times, enhancing readability and ease of navigation. This feature is accessed through the View tab, where users can freeze the top row, leftmost column, or a combination of rows and columns.

Figure 9: Excel freeze panes options.

### Splitting windows

Splitting windows in Excel divides the worksheet into separate panes, allowing users to view different parts of the same worksheet simultaneously. This feature is particularly useful when working with large datasets or comparing data from different sections of the worksheet. Users can split the window horizontally or vertically or create multiple windows for more complex analysis. Splitting windows can be accessed through the View tab, as demonstrated in Figure 10.

Figure 10: Example of Window Splitting.

### Page Layout view

Page Layout view in Excel provides a preview of how the worksheet will appear when printed. Demonstrated in Figure 11. It enables users to adjust the layout, margins, headers, footers, and other worksheet elements to ensure optimal printing results. Page Layout view can be accessed through the View tab and is useful for formatting and designing professional-looking printed documents.

Figure 11: Excel Page Layout options.

### Full-Screen View

Full-Screen view hides the Excel ribbon and other toolbars, maximising the available screen space for the worksheet. This immersive view is helpful when users need to focus solely on the data and minimise distractions. The Full-screen view can be accessed through the View tab or by pressing the F11 key.

### Custom Views

Custom Views allow users to save different combinations of window formatting settings, including zoom level, freeze panes, and other display options. Users can create and switch between Custom Views to quickly apply specific window formatting configurations based on their needs.

These Excel window layout options offer flexibility and customizability to improve working conditions and increase data analysis. Users may navigate through big datasets, compare data effectively, and present information that best matches their needs by changing the view, freezing panes, splitting windows, and using other window formatting tools.

### Page breaks

Window formatting in Excel also encompasses managing page breaks, which determine how data is divided and displayed across multiple pages when printing. Excel provides tools to control and adjust page breaks to ensure the desired layout and formatting. Letâ€™s explore page breaks in Excel:

#### Automatic page breaks

Excel automatically inserts page breaks based on the paper size, margins, and print settings. These automatic page breaks determine where the content of the worksheet will break onto the next page. Users can view and adjust these automatic page breaks in Page Break Preview.

#### Page Break Preview

This is a viewing mode in Excel that displays the worksheet with dashed lines indicating page breaks. It allows users to see how the data will be distributed across multiple pages when printed. In this view, users can manually adjust and move page breaks to control the layout. Page Break Preview can be accessed through the View tab.

#### Inserting manual page breaks

Excel allows users to insert manual page breaks to specify exactly where they want the data to break onto the next page. To insert a manual page break, users can:

1. Select the row or column below or to the right of where they want the page break to appear.
2. Navigate to the Page Layout tab and click on the Breaks button.
3. Choose Insert Page Break to insert a horizontal page break above the selected row or a vertical page break to the left of the selected column.

#### Removing page breaks

Users can also remove page breaks in Excel. To remove a manual page break, users can:

1. Select a cell in the row or column where the page break is located.
2. Navigate to the Page Layout tab and click on the Breaks button.
3. Choose Remove Page Break to eliminate the selected page break.

Users can control how their data is dispersed across printed pages in Excel by modifying and managing page breaks, guaranteeing optimal layout and readability, and avoiding unpleasant data cut-offs. The freedom to tailor worksheet printing, considering various paper sizes and formatting needs, is made possible via Excelâ€™s page break capabilities.

Overall, by including page breaks in Excelâ€™s window formatting settings, users can precisely control printed documentsâ€™ layout and appearance, making it simpler to understand and analyse data when working with physical copies.

### Printing

Printing in Excel refers to the process of generating physical copies of worksheets or selected data from an Excel workbook. Excel provides various options and settings to customise the printing experience and ensure that the printed output meets the desired requirements. Refer to Figure 12, and letâ€™s explore the steps and considerations involved in printing in Excel:

Figure 12: Excel Print options.

### Print Preview

Before printing, previewing the document to get an idea of how the content will appear on paper is recommended. Excelâ€™s Print Preview feature allows users to view the entire worksheet or a selected range as it will be printed. It allows for checking the layout, adjusting page breaks, and making necessary formatting changes before printing.

### Page Setup

Excelâ€™s Page Setup options enable users to customise the printed pagesâ€™ layout, margins, orientation, and scaling. To access the Page Setup settings:

1. Go to the Page Layout tab and click on the Page Setup group.
2. Adjust settings such as paper size, orientation (portrait or landscape), margins, and scaling options.

### Print area

Users can define a specific print area in Excel, which determines the range of cells or data that will be printed. This feature is helpful when only a portion of the worksheet needs to be printed. To set a print area:

1. Select the desired range of cells.
2. Go to the Page Layout tab and click on the Print Area button in the Page Setup group.
3. Choose Set Print Area to define the selected range as the print area.

### Print settings

Excel offers various settings to customise the printing process. These settings can be accessed in the Print dialog box, which appears when the user selects Print or presses Ctrl+P. Some common print settings include:

#### Number of copies

Specify the number of copies to be printed.

#### Print range

Choose to print the entire workbook, active sheets, or selected range.

#### Print order

Determine the order in which pages will be printed.

#### Print titles

Define rows or columns to repeat on each printed page for better readability.

### Print options

Excel provides additional print options allowing users to enhance the printed output further. These options include:

#### Gridlines and headings

Include or exclude gridlines and row/column headings in the printed output.

#### Background colours and images

Determine whether background colours and images should be printed.

#### Scaling

Adjust the size of the printed output by scaling the content to fit a specific number of pages.

#### Page breaks

Control and adjust automatic or manual page breaks for optimal printing.

### Print

Once all the desired settings have been configured, users can initiate the printing process by selecting the Print option. This sends the worksheet or selected range to the printer, producing physical copies of the data based on the specified settings.

By leveraging Excelâ€™s printing capabilities, users can generate well-formatted and readable hard copies of their worksheets, reports, charts, or any other data-driven documents. Understanding Excelâ€™s printing options and settings allows users to customise the printouts according to their preferences and effectively communicate their data.

### Window formatting in Excel - Practice Activity

In this activity, you will practice window formatting, page breaks, and printing functions in Excel. Follow the instructions provided below to complete the tasks.

#### Task 1: Window Formatting

1. Open a new Excel workbook.
2. Enter some sample data in cells `A1` to `E10` to create a dataset.
3. Adjust the zoom level to `75%`.
4. Split the window into two panes, vertically, at column `C` so that both sections of the dataset are visible simultaneously.
5. Freeze the top row and the leftmost column, ensuring they remain visible while scrolling.

#### Task 2: Page breaks

1. Navigate to the Page Break Preview.
2. Adjust the automatic page breaks to ensure data is distributed optimally across pages if necessary.
3. Insert a manual page break above row `7`, ensuring that the data above and below the page break is separated correctly.
4. Remove any unnecessary page breaks.

#### Task 3: Printing

1. Set the print area to include the dataset from `A1` to `E10`.
2. Adjust the page setup options to print the worksheet in landscape orientation.
3. Include the row and column headings on every printed page.
4. Configure the margins to ensure the data is properly aligned on the printed page.
5. Print the worksheet and review the output for accuracy.

#### Task 4: Custom views

1. Create a custom view called `Split View` that includes the split window formatting, zoom level, and frozen panes.
2. Create another custom view called `Print View` that includes the adjusted page breaks, print area, and page setup options.
3. Switch between the custom views to observe the changes in window formatting and printing settings.

### Tutor solution (step-by-step)

#### Step 1: Build your sample dataset

- Fill `A1:E1` with headers such as `ID`, `Name`, `Category`, `Amount`, `Date`.
- Fill rows `2:10` with sample values.
- Select `A1:E10` and apply `All Borders` for clear visual checking.

#### Step 2: Window formatting setup

1. Set zoom to `75%`:
    - Use the bottom-right zoom slider, or View â†’ Zoom â†’ 75.
2. Split window at column `C`:
    - Click cell `C1` then View â†’ Split.
    - This creates a vertical split so left and right sections can be viewed together.
3. Freeze top row and first column:
    - Click cell `B2`.
    - View â†’ Freeze Panes â†’ Freeze Panes.
    - Result: row 1 and column A stay visible while you scroll.

#### Step 3: Page break configuration

1. Open View â†’ Page Break Preview.
2. Review dashed/solid page break lines.
3. Insert manual break above row `7`:
    - Click row `7`.
    - Page Layout â†’ Breaks â†’ Insert Page Break.
4. Remove unwanted breaks:
    - Select a row/column containing the break.
    - Page Layout â†’ Breaks â†’ Remove Page Break.

#### Step 4: Printing setup

1. Set print area:
    - Select `A1:E10`.
    - Page Layout â†’ Print Area â†’ Set Print Area.
2. Set orientation:
    - Page Layout â†’ Orientation â†’ Landscape.
3. Include row/column headings:
    - Page Layout â†’ Page Setup launcher (small diagonal arrow) â†’ Sheet tab.
    - Check `Row and column headings`.
4. Adjust margins:
    - Page Layout â†’ Margins â†’ choose Normal/Narrow or Custom Margins as needed.
5. Preview and print:
    - Ctrl+P and review preview.
    - Confirm page split, headings, margins, and orientation before printing.

#### Step 5: Create custom views

1. Create `Split View`:
    - Keep split + freeze + zoom at 75% active.
    - View â†’ Custom Views â†’ Add.
    - Name: `Split View`.
2. Create `Print View`:
    - Switch to print-focused settings (print area, landscape, page breaks).
    - View â†’ Custom Views â†’ Add.
    - Name: `Print View`.
3. Test switching:
    - View â†’ Custom Views.
    - Select each view and click Show to verify the expected layout/settings.

#### Validation checklist

- Zoom is 75%.
- Split is vertical at column C.
- Row 1 and column A remain fixed while scrolling.
- Manual page break is placed above row 7.
- Print area is A1:E10.
- Orientation is Landscape.
- Row and column headings are enabled for print.
- Both custom views are saved and switch correctly.

### The Task

#### Question 1 - Numeric formatting

1. Open a new Excel workbook.
2. In cell `A1`, enter the number `9876.43`.
3. Apply the **Accounting** format to cell `A1`, displaying the number with currency symbols, decimal places, and aligned formatting.
4. In cell `B1`, enter the percentage value `0.25`.
5. Apply the **Percentage** format to cell `B1`, displaying the number as a percentage with two decimal places.
6. In cell `C1`, enter the date `June 13, 2023`.
7. Apply the **Long Date** format to cell `C1`, displaying the date in the format `MMMM DD, YYYY`.

#### Question 2 - Text formatting

1. In cell `A3`, enter the text `Inventory List`.
2. Apply bold formatting to the text in cell `A3`.
3. In cell `B3`, enter the text `Product Code`.
4. Apply underline formatting to the text in cell `B3`.
5. In cell `C3`, enter the text `Quantity in Stock`.
6. Apply italic formatting to the text in cell `C3`.
7. In cell `D3`, enter the text `Price per Unit`.
8. Apply strikethrough formatting to the text in cell `D3`.

#### Question 3 - Data alignment

1. Select cells `A1` to `D3`.
2. Apply centre alignment to the selected cells.
3. Adjust the column widths to fit the content in each cell.

#### Question 4 - Cell border formatting

1. Select cells `A1` to `D3`.
2. Apply a thick border around the selected cells.
3. Adjust the border style to a double line.

#### Question 5 - Sorting data

1. Add a new worksheet.
2. Enter the following data into columns `A`, `B`, and `C`, starting from row `1`:
    - Column `A`: Country (`USA`, `Canada`, `Germany`, `France`, `Australia`)
    - Column `B`: Population (`328`, `38`, `83`, `67`, `25`)
    - Column `C`: GDP (`21.43`, `1.64`, `4.44`, `2.71`, `1.37`)
3. Select the entire dataset (`A1:C6`).
4. Sort the data in ascending order based on the `Country` column.
5. Sort the data in descending order based on the `Population` column.
6. Sort the data in descending order based on the `GDP` column.

#### Question 6 - Filtering data

1. Select the entire dataset (`A1:C6`).
2. Apply filters to the dataset.
3. Filter the data to display only countries with a population greater than `50` million.
4. Filter the data to display only countries with a GDP of less than `5` trillion.
5. Filter the data to display only countries whose names start with the letter `C`.

#### Question 7 - Multiple criteria filtering

1. Select the entire dataset (`A1:C6`).
2. Apply filters to the dataset if they are not already applied.
3. Filter the data to display only countries with a population greater than `50` million and a GDP less than `3` trillion.

#### Question 8 - Removing filters

1. Select the entire dataset (`A1:C6`).
2. Remove all filters from the dataset.

#### Question 9 - Window formatting

1. Add a new worksheet.
2. Enter some sample data in cells `A1` to `F15` to create a dataset.
3. Adjust the zoom level to `90%`.
4. Split the window into two panes, horizontally, at row `6`, so that both sections of the dataset are visible simultaneously.
5. Freeze the top two rows and the leftmost column, ensuring they remain visible while scrolling.

#### Question 10 - Page breaks

1. Navigate to the Page Break Preview.
2. Adjust the automatic page breaks, if necessary, to ensure data is distributed optimally across pages.
3. Insert a manual page break above row `12`, ensuring that the data above and below the page break is separated correctly.
4. Remove any unnecessary page breaks.

#### Question 11 - Printing

1. Set the print area to include the dataset from `A1` to `F15`.
2. Adjust the page setup options to print the worksheet in portrait orientation.
3. Include the row and column headings on every printed page.
4. Configure the margins to ensure the data is properly aligned on the printed page.
5. Print the worksheet and review the output for accuracy.

### Tutor solution and validation guide

Use this answer strategy during practicals/exams:

1. Complete Questions 1-4 on worksheet 1, then take a quick screenshot for evidence.
2. Complete Questions 5-8 on worksheet 2, verifying sort/filter results after each step.
3. Complete Questions 9-11 on worksheet 3, then check print preview before final print/export.
4. Save your workbook and keep one validated output per question group.

#### Required output checks

- `A1` shows accounting format with aligned currency.
- `B1` shows `25.00%`.
- `C1` shows long date format for June 13, 2023.
- `A1:D3` is centered with thick/double border formatting.
- Country dataset sorts correctly by `Country`, `Population`, and `GDP` in required order.
- Filters return correct subsets (single and multiple criteria).
- Window split/freeze/zoom settings match the task.
- Page break exists above row `12`.
- Print setup is portrait with headings and correct margins.

#### Reference files

- `/workspaces/Study-buddy/SPF-0103 Lesson Task solution.pdf`
- `/workspaces/Study-buddy/SPF 0103 Lesson task solution Task 9 dataset.csv`

### Expected Answers Snapshot (Exam-Ready)

Use this quick rubric to verify your final workbook before submission.

| Question | What examiner expects to see |
|---|---|
| Q1 Numeric formatting | `A1` in Accounting format, `B1` as `25.00%`, `C1` in Long Date format (`June 13, 2023` style) |
| Q2 Text formatting | `A3` bold, `B3` underline, `C3` italic, `D3` strikethrough |
| Q3 Alignment | Range `A1:D3` centered and column widths adjusted (no clipped text) |
| Q4 Borders | Range `A1:D3` has thick outer border and double-line border style applied as requested |
| Q5 Sorting | Country dataset sorted correctly by Country ascending, Population descending, GDP descending |
| Q6 Filtering | Correct filtered subsets for `Population > 50`, `GDP < 5`, and names beginning with `C` |
| Q7 Multiple criteria | Only rows meeting both conditions (`Population > 50` AND `GDP < 3`) remain visible |
| Q8 Remove filters | All filters cleared and full dataset visible again |
| Q9 Window formatting | Worksheet with zoom `90%`, horizontal split at row `6`, top two rows + first column frozen |
| Q10 Page breaks | Page Break Preview reviewed, manual break inserted above row `12`, unnecessary breaks removed |
| Q11 Printing | Print area `A1:F15`, portrait orientation, row/column headings included, margins adjusted, preview checked |

#### Exam submission checklist

- Save workbook with clear sheet names (for example `Q1_Q4_Formatting`, `Q5_Q8_SortFilter`, `Q9_Q11_Print`).
- Capture at least one screenshot per question group as evidence.
- Re-open each worksheet and verify all settings persisted before final submission.
- Review Print Preview one final time to avoid page-break or margin penalties.

### What Did I Learn in This Lesson?

This lesson provided the following insights:

- We looked at cell data formats and how they influence the appearance and handling of data in Excel.
- We learnt how users can format numeric data in Excel, and about some of the common number formats available.
- We explored text cell formatting in Excel and how users can modify the appearance of text within cells.
- We learnt how users can perform sorting and filtering in Excel, and what the benefits of these functions are.
- We examined window formatting options in Excel and how users can customise the view, zoom level, and layout of their worksheets.
- We explored how freezing panes in Excel work and what advantages it offers for working with large datasets.
- We examined how users can split windows in Excel and why this feature is useful when working with extensive data or comparing different sections of a worksheet.
- We learned how Page Layout view in Excel helps users in formatting and designing printed documents.
- We covered Full-Screen view in Excel and how it contributes to a distraction-free working environment.
- We learnt how users can create and switch between custom views in Excel and what benefits this feature offers in terms of window formatting.
- We examined how page breaks work in Excel and what tools Excel provides to manage and adjust page breaks for optimal printing results.
- We covered inserting manual page breaks and removing page breaks in Excel when customising the layout of printed documents.
            """,
            "key_points": [
                "Basic formatting in Excel changes the appearance of cells and worksheet content",
                "Formatting improves visual appeal, readability, and professionalism",
                "Formatting helps highlight important information",
                "Font styles, colours, borders, and alignment are core formatting tools",
                "Clear and organised formatting supports better communication of data",
                "Cell data formats control how values like numbers, dates, and currencies are displayed",
                "Number formatting improves readability with decimal control, separators, and symbols",
                "Format Cells and Home tab tools provide quick access to formatting features",
                "Excel provides pre-defined number categories such as General, Number, Currency, Accounting, Date, Time, Percentage, and Scientific",
                "Text cell formatting controls text presentation, alignment, wrapping, and styling",
                "Text formatting options include font style, colour, size, alignment, wrapping, indentation, borders, and cell protection",
                "Sorting rearranges rows based on one or more selected columns",
                "Filtering shows only rows that match selected criteria",
                "Advanced filtering supports formula criteria, unique records, and copying filtered results",
                "Single and multiple-criteria filters help isolate precise data subsets for analysis",
                "Window formatting includes zoom controls and freeze panes for better navigation",
                "Splitting windows enables simultaneous viewing of multiple worksheet areas",
                "Page Layout view helps prepare worksheets for professional printing",
                "Full-Screen view reduces distractions by maximizing worksheet workspace",
                "Custom Views save and reapply preferred worksheet display configurations",
                "Page break tools help control printed page layout and readability",
                "Printing options in Excel allow customization of output for physical documents",
                "Print Preview, Page Setup, and Print Area improve print accuracy and layout control",
                "Print settings and options help tailor copies, ranges, scaling, and readability",
                "Custom views can preserve and switch between analysis-focused and print-focused layouts"
            ],
            "visual_elements": {
                "diagrams": False,
                "tables": False,
                "highlighted_sections": True
            }
        },
        {
            "lesson_number": "1.4",
            "title": "Data Validation and Conditional Formatting",
            "content": """
### 1.4. Lesson - Data Validation and Conditional Formatting

### Introduction

Data validation and conditional formatting are essential techniques used in data analysis and management to ensure data quality, consistency, and visual representation in various applications, such as spreadsheets, databases, and other data-driven systems. These techniques improve data quality, maintain data integrity, or do both to aid decision-making processes.

Data validation guarantees that only accurate and appropriate data is entered into a system by creating and enforcing rules or constraints on data entry. It helps avoid errors, inconsistencies, and inaccuracies by setting clear guidelines that data must adhere to before being accepted. This may involve restrictions on the range of data types, requirements for uniqueness, and more. Data validation is crucial for ensuring data dependability and integrity because it lessens the potential of erroneous or unreliable data entering a system.

Contrarily, conditional formatting allows users to format and highlight cells or ranges in line with established rules or criteria. Users can quickly identify patterns, trends, or outliers in a dataset by adding numerous formatting styles such as font colour, background colour, borders, and data bars to cells that meet specific criteria. When data analysts and consumers use conditional formatting to graphically portray data in a way that highlights important information, it is easier to understand and analyse massive datasets.

Various applications extensively use conditional formatting and data validation, including spreadsheet programs like Microsoft Excel, Google Sheets, and database management systems. These techniques are valuable tools for data analysts, managers, and users whose decisions depend on accurate and appealing data. By using conditional formatting and adopting data validation criteria, organisations may enhance the quality of their data, streamline their data analysis processes, and promote decision-making that is informed by data.

### Data Validation

Excel has a tool called Data Validation that guarantees the integrity and quality of data entered into cells. Users can specify precise requirements for data input, such as text lengths, numerical ranges, or the selection of values from a predetermined list. Excel can be configured to perform data validation, which checks entered data against predefined criteria and alerts users with error messages or warnings if the input does not comply. This feature helps to maintain data quality and reduces the likelihood of mistakes in spreadsheets.

This is how we access the Data Validation menu in Excel as demonstrated in Figures 1 and 2:

Figure 1: Access Data Validation in Excel.

1. Select the cell or range where you want to apply data validation.
2. Go to the Data tab and click on Data Validation.
3. In the Data Validation dialog box, select the option from the Allow drop-down list.
4. Specify the desired criteria.
5. Click OK to apply the validation.

Figure 2: Data Validation settings.

Here you to be my tutor:

Letâ€™s examine a few prevalent examples:

#### Whole number

This validation ensures that the value entered is an integer with no decimal places. It is helpful in situations that only allow whole numbers, including counting things or keeping track of quantities. Refer to Figure 3.

1. Select the cell or range where you want to apply data validation.
2. Go to the Data tab and click on Data Validation.
3. In the Data Validation dialog box, select Whole number from the Allow drop-down list.
4. Specify the desired criteria, such as the minimum and maximum values.
5. Click OK to apply the validation.

Figure 3: Data validation: Whole number.

#### Decimal number

Decimal places can be entered for numeric values with this validation. It helps guarantee that the provided data falls within a specified range or has a specific number of decimal places, making it appropriate for measurements or financial calculations. Demonstrated in Figure 4.

1. Follow the same steps as above for data validation.
2. In the Data Validation dialog box, select Decimal from the Allow drop-down list.
3. Set the criteria for decimal places or value range as per your requirements.

Figure 4: Data validation: Decimal number.

#### Time and date

Time or date entries are validated using this validator. When dealing with timetables, deadlines, or time-sensitive information, it ensures that the input adheres to the proper time or date format, preventing mistakes and promoting uniformity. Demonstrated in Figure 5.

1. Select the cell or range where you want to apply data validation.
2. Go to the Data tab and click on Data Validation.
3. In the Data Validation dialog box, select Time or Date from the Allow drop-down list.
4. Specify the appropriate time or date format and any additional criteria, if needed.

Figure 5: Data Validation: Time and date.

#### List

Input is limited via list validation to a predetermined range of values. It allows users to design a drop-down menu of choices, guaranteeing that the entered value corresponds to one of the designated options. This is beneficial for data input operations that call for reliable and consistent selection from a predetermined list of options. Demonstrated in Figures 6 and 7.

1. Select the cell or range where you want to create the drop-down list.
2. Go to the Data tab and click on Data Validation.
3. In the Data Validation dialog box, select List from the Allow drop-down list.
4. Enter the list of values you want to appear in the drop-down list, either in a range or separated by commas.
5. Optionally, you can choose to show an error message or an input message for the drop-down list.

Figure 7: List options within the cell.

#### Text length

Users can determine the minimum and maximum characters that can be entered by using text length validation. It aids in regulating the length of text input by setting character limits for names and keeping comments to a predetermined length. Demonstrated in Figure 8.

1. Follow the same steps as above for data validation.
2. In the Data Validation dialog box, select Text length from the Allow drop-down list.
3. Specify the desired minimum and maximum lengths for the text.

Figure 8: Data Validation: Text length.

### Conditional Formatting

Excelâ€™s sophisticated conditional formatting function lets users dynamically format cells according to predefined parameters or criteria. Conditional formatting highlights or styles cells automatically by specifying rules, such as comparing values, using colour scales, or utilising data bars, and offers visual clues to analyse and interpret data more efficiently. Users can use this tool to find trends, outliers, and patterns in their data, making it simpler to recognise essential information quickly and improving the visual appeal of their Excel worksheets as a whole.

#### Cell-specific

You can format cells based on conditions you establish, such as highlighting cells greater than a given value or containing a particular text, with conditional formatting based on specific cell values. In the below example, we have exam marks scored by students in three subjects. We want a visual representation of the marks from lowest to highest using a colour scale from red (lowest) to yellow (highest). Demonstrated in Figures 10, 11 and 12.

Figure 10: Conditional Formatting: New rule.
Figure 11: Creating new rule.
Figure 12: Result.

1. Select the cell(s) you want to apply conditional formatting to.
2. Go to the Home tab and click on Conditional Formatting in the Styles group.
3. Choose New Rule from the drop-down menu.
4. In the New Formatting Rule dialog box, select Format only cells that contain and specify the condition, such as equal to, greater than, or less than.
5. Set the formatting options for the cells that meet the condition.
6. Click OK to apply the conditional formatting.

#### Colour scales

Conditional formattingâ€™s colour scales assign various colours to cells depending on their values, generating a visual gradient that makes it simple to spot data variances and contrast relative values. Demonstrated in Figures 13, 14 and 15.

1. Select the cell(s) you want to apply conditional formatting to.
2. Go to the Home tab and click on Conditional Formatting in the Styles group.
3. Choose Colour Scales from the drop-down menu.
4. Select the desired colour scale option, such as a two-colour or three-colour scale.
5. Excel automatically applies the colour scale to the selected cells based on their values.

Figure 13: Colour Scales.
Figure 14: Formatting rules.
Figure 15: Result.

#### Data bars

Conditional formatting adds horizontal bars inside cells to reflect the values they hold in the case of data bars. Giving a visual depiction of the data distribution, the length of the bar matches the value. Choose the cell(s) to which conditional formatting should be applied. In the example below, we want to add Data Bars to visually indicate the distribution of the scores where the length of the line matches the value. Demonstrated in Figure 16.

1. Go to the Home tab and click on Conditional Formatting in the Styles group.
2. Choose Data Bars from the drop-down menu.
3. Select the desired data bar option, such as a gradient or solid fill.
4. Excel automatically applies data bars to the selected cells based on their values.

Figure 16: Conditional Formatting: Data Bars.

### Conditional Formatting (continued)

#### Icon sets

Based on the values of the cells, conditional formattingâ€™s icon sets add icons, like arrows or symbols, to the cells. These icons give you a brief visual representation of trends or categories so you can examine data more quickly. Choose the cell(s) to which conditional formatting should be applied. In the example below, we can use Icon Sets to represent trends for each subject visually. Up arrows (highest scores), Down arrows (lowest scores), Right arrows (average scores), Right-Up arrows (higher than average), and Right-Down arrows (lower than average). Demonstrated in Figure 17.

1. Go to the Home tab and click on Conditional Formatting in the Styles group.
2. Choose Icon Sets from the drop-down menu.
3. Select the desired icon set, such as arrows, symbols, or traffic lights.
4. Set the thresholds for each icon by specifying the values or percentages.
5. Excel automatically applies the appropriate icon to the selected cells based on their values.

Figure 17: Conditional Formatting: Icon Set.

#### Duplicate values

Formatting with conditions for duplicate values automatically identifies and manages duplicate items within a range by highlighting cells or rows that contain duplicate data. Choose the range of cells where duplicates should be found. Demonstrated in Figures 18 and 19.

1. Go to the Home tab and click on Conditional Formatting in the Styles group.
2. Choose Highlight Cells Rules from the drop-down menu.
3. Select Duplicate Values from the submenu.
4. Choose the formatting style for highlighting duplicate values.
5. Click OK to apply the conditional formatting, and Excel will highlight any duplicate values in the selected range.

Figure 18: Conditional Formatting: Duplicate Values.
Figure 19: Result.

#### Top and bottom values

With conditional formatting, you can easily spot outliers or important data points by emphasising the greatest or lowest numbers within a range. Demonstrated in Figures 20 and 21.

1. Select the range of cells you want to format.
2. Go to the Home tab and click on Conditional Formatting in the Styles group.
3. Choose Top/Bottom Rules from the drop-down menu.
4. Select Top 10 Items, Bottom 10 Items, or any other desired option.
5. Set the criteria, such as top or bottom values, percentage, or rank.
6. Choose the formatting style for highlighting the top or bottom values.
7. Click OK to apply the conditional formatting, and Excel will format the cells based on the specified criteria.

Figure 20: Conditional Formatting: Top/Bottom values.
Figure 21: Customise Top 5 with red.

#### Custom formatting

With custom conditional formatting, you may create your own formulae and rules to format cells in accordance with requirements or circumstances that are not addressed by the built-in formatting options. It gives you more freedom to apply conditional formatting tailored to your particular requirements. Choose the cell(s) to which conditional formatting should be applied. Demonstrated in Figures 22 and 23.

1. Go to the Home tab and click on Conditional Formatting in the Styles group.
2. Choose New Rule from the drop-down menu.
3. In the New Formatting Rule dialog box, select Use a formula to determine which cells to format.
4. Enter the formula that evaluates to either TRUE or FALSE for the desired condition.
5. Set the formatting options for the cells that meet the condition.
6. Click OK to apply the conditional formatting.

### Activity 2 - Conditional formatting in Excel (guided)

**Dataset:** `SPF+-+Sample+dataset+Activity+2.xlsx`

Path:
- `/workspaces/Study-buddy/SPF+-+Sample+dataset+Activity+2.xlsx`

Open the workbook and confirm columns:
- **A:** Name
- **B:** Mathematics
- **C:** Science
- **D:** English

#### Visual setup map

| Rule | Column | Menu path | Expected visual |
|---|---|---|---|
| Math below 60 | B | Conditional Formatting â†’ Highlight Cells Rules â†’ Less Than | Red cells |
| Science above 90 | C | Conditional Formatting â†’ Highlight Cells Rules â†’ Greater Than | Green cells |
| English between 70 and 80 | D | Conditional Formatting â†’ Highlight Cells Rules â†’ Between | Yellow cells |
| Math distribution | B | Conditional Formatting â†’ Color Scales (redâ†’green) | Low=red, high=green |
| Science 2-color scale | C | Conditional Formatting â†’ Color Scales â†’ More Rules | Lowest/highest contrast |
| English data bars | D | Conditional Formatting â†’ Data Bars | Bar length matches score |
| Math icon set | B | Conditional Formatting â†’ Icon Sets â†’ More Rules | Above avg / avg / below avg |
| Duplicate names | A | Conditional Formatting â†’ Highlight Cells Rules â†’ Duplicate Values | Duplicate names highlighted |
| Top 3 Science | C | Conditional Formatting â†’ Top/Bottom Rules â†’ Top 10 Items (change 10â†’3) | Top 3 standout format |
| Bottom 3 Mathematics | B | Conditional Formatting â†’ Top/Bottom Rules â†’ Bottom 10 Items (change 10â†’3) | Bottom 3 standout format |

#### Visual reference

![Conditional formatting quick visual](attached_assets/image_1767396605515.png)

#### Required steps

1. Highlight cells in **Mathematics** that have scores below `60` in red.
2. Highlight cells in **Science** that have scores above `90` in green.
3. Highlight cells in **English** that have scores between `70` and `80` in yellow.
4. Apply a colour scale to **Mathematics** (lower=red, higher=green).
5. Apply a two-colour scale to **Science** for lowest and highest scores.
6. Add **Data Bars** to **English**.
7. Apply **Icon Sets** to **Mathematics** to indicate above average, average, and below average.
8. Highlight duplicate values in **Name**.
9. Highlight the **top three** scores in **Science**.
10. Highlight the **bottom three** scores in **Mathematics**.
11. Add/change rows and verify formatting updates dynamically.

#### Tutor check (quick)

- If colors overlap unexpectedly, open **Conditional Formatting â†’ Manage Rules** and reorder priority.
- Use consistent formatting styles so each rule is visually distinct.
- After adding rows, verify every rule still applies to the full data range.
            """,
            "key_points": [
                "Data validation enforces rules to keep input accurate and consistent",
                "Validation constraints reduce entry errors and protect data integrity",
                "Conditional formatting highlights important patterns, trends, and outliers",
                "Visual rules improve readability and speed up data interpretation",
                "These techniques are widely used in Excel, Google Sheets, and data systems",
                "Better validation and formatting support stronger data-driven decisions",
                "Data Validation rules can constrain inputs by type, range, length, and list options",
                "Applying validation through the Data tab helps prevent spreadsheet input mistakes",
                "Whole number validation can enforce integer-only inputs within defined minimum/maximum limits",
                "Decimal validation supports value ranges and precision requirements",
                "Time and date validation enforces consistent temporal input formats",
                "List validation provides controlled dropdown choices for consistent data entry",
                "Text length validation enforces minimum and maximum character limits"
            ],
            "visual_elements": {
                "diagrams": False,
                "tables": False,
                "highlighted_sections": True
            }
        }
    ]
}



courses_data = [
    {
        "code": "FI1BBDF05", 
        "name": "Data Analysis Fundamentals", 
        "type": "Core Course", 
        "credits": 5, 
        "semester": "2025 Spring",
        "weeks": 3,
        "hours": 126,
        "description": "This course delivers an introductory overview of Data Analysis. It provides the foundational material required to build a strong theoretical understanding of why data analysis is required in industry and how using analytics tools can shape decision making in the real world.",
        "knowledge": [
            "History of data and data sources",
            "Significance of data in the real world",
            "Introduction to business intelligence and big data",
            "Data strategies: exploration, visualization, trends and estimates",
            "Data warehouses, data silos, and open data platforms"
        ],
        "skills": [
            "Apply problem division and solving into each stage in the data lifecycle",
            "Apply theoretical data analysis strategies into real world scenarios",
            "Find information relevant to problem scenarios and suggest solutions",
            "Identify where data can be collected first-hand and alternative sources",
            "Use online data collection tools such as Google Forms",
            "Identify and source data ethically with GDPR standards"
        ],
        "competence": [
            "Understand ethical principles for successful data analysis projects",
            "Understand ethical principles of collecting and maintaining data",
            "Carry out data strategies from real world scenarios",
            "Develop data analysis terminology"
        ]
    },
    {
        "code": "FI1BBSF05", 
        "name": "Spreadsheet Fundamentals", 
        "type": "Core Course", 
        "credits": 5, 
        "semester": "2025 Spring",
        "weeks": 3,
        "hours": 126,
        "description": "This course teaches a foundation level introduction to the spreadsheet work environment, specifically Microsoft Excel. Learn to gather, clean, manage, and organize data. Also covers Google Sheets for collaborative work.",
        "knowledge": [
            "Concepts and processes to gather, clean, manage, and organize data in spreadsheets",
            "Data management techniques: storing, sorting, and presenting data",
            "Cloud-based spreadsheet software (Google Sheets)",
            "Data flow pipelines to link spreadsheet software to external tools",
            "Why spreadsheets are useful in society and value-creation"
        ],
        "skills": [
            "Apply spreadsheet software to gather, sort, store, manage and organize data",
            "Use conditional formatting and pivot tables to summarize key data points",
            "Find information to develop transformative spreadsheet projects",
            "Master two spreadsheet software suites (offline and online)",
            "Master basic workbook manipulation tools",
            "Use basic field formulas to automate data tasks"
        ],
        "competence": [
            "Create workbooks to manage data from start to finish",
            "Build relations with clients using real world data sets",
            "Develop collaborative workbooks using cloud-based software"
        ]
    },
    {
        "code": "FI1BBDD75", 
        "name": "Data Driven Decision-Making", 
        "type": "Core Course", 
        "credits": 7.5, 
        "semester": "2025 Spring",
        "weeks": 4,
        "hours": 168,
        "description": "This course establishes core concepts of decision-making techniques applied to data models. Learn the data analysis lifecycle, techniques (Descriptive, Predictive, Prescriptive, Diagnostic), and qualitative vs quantitative data.",
        "knowledge": [
            "Data structure models and where to apply applicable data sets",
            "Concepts and processes for data cleaning using real world data",
            "Real-world use case stories and company impacts",
            "Key Performance Indicators (KPI) and data types",
            "Four data analysis philosophies: descriptive, diagnostic, predictive, prescriptive",
            "Error detection, elimination, and correction"
        ],
        "skills": [
            "Apply data driven decision making to problems like market price prediction",
            "Strategically select appropriate data models to solve scenarios",
            "Apply data lifecycle to create iterative solutions and analyze KPIs",
            "Identify erroneous data and eliminate/correct them",
            "Master theoretical models to real world data"
        ],
        "competence": [
            "Understand the fidelity of data within a project",
            "Develop work methods using KPIs to guide decision-making",
            "Deliver insights to gauge if models are accurate for intended use"
        ]
    },
    {
        "code": "FI1BBST05", 
        "name": "Statistical Tools", 
        "type": "Core Course", 
        "credits": 5, 
        "semester": "2025 Spring",
        "weeks": 3,
        "hours": 126,
        "description": "This course provides knowledge of using integrated spreadsheet tools and introductory statistical modelling software. Builds on Spreadsheet Fundamentals competence.",
        "knowledge": [
            "Spreadsheet data tools for statistical analysis using built-in functions",
            "Statistical methodologies to extract KPIs from numerical values",
            "Advanced data analytics tool packs in spreadsheet software",
            "Correlation, regression, ANOVA, histogram and covariance analysis",
            "Power Query for automation",
            "Z-scores and z-testing for outlier reduction"
        ],
        "skills": [
            "Perform statistical analysis on data sets using spreadsheet tools",
            "Install and use advanced data analysis suite",
            "Use Power Query to automate tasks",
            "Apply z-values to reduce errors and eliminate outliers"
        ],
        "competence": [
            "Carry out work using advanced spreadsheet tools",
            "Develop effective work methods for analysis within spreadsheets"
        ]
    },
    {
        "code": "FI1BBP175", 
        "name": "Semester Project 1", 
        "type": "Core Course", 
        "credits": 7.5, 
        "semester": "2025 Spring",
        "weeks": 4,
        "hours": 168,
        "description": "Apply first semester knowledge to a practical data analysis project. Demonstrate understanding of data fundamentals, spreadsheets, and decision-making.",
        "knowledge": [
            "Project planning and scope definition",
            "Data collection for real-world problems",
            "Applying analytical techniques learned",
            "Documentation and reporting standards",
            "Presentation skills for data findings"
        ],
        "skills": [
            "Execute a complete data analysis project",
            "Apply spreadsheet and statistical tools",
            "Present findings to an audience",
            "Document work professionally"
        ],
        "competence": [
            "Plan and execute data analysis tasks independently",
            "Work according to ethical requirements",
            "Deliver professional project documentation"
        ]
    },
    {
        "code": "FI1BBEO10", 
        "name": "Evaluation of Outcomes", 
        "type": "Core Course", 
        "credits": 10, 
        "semester": "2025 Fall",
        "weeks": 8,
        "hours": 336,
        "description": "Learn to review, assess, and appraise the results of analytical models. Covers statistical inferences, confidence levels, and iterative error elimination.",
        "knowledge": [
            "Key Performance Indicators (KPI) as heuristics in decision making",
            "Statistical inferences: sampled sets, linear regression, variance, five-point summaries, z-testing",
            "Confidence levels and multiple probability outcomes",
            "Iterative error elimination processes and tools",
            "Ensambling data techniques",
            "Version control for collaborative data work",
            "ETL systems in data analysis lifecycle"
        ],
        "skills": [
            "Apply statistical inferences to identify and solve problems",
            "Apply iterative error elimination to improve results",
            "Create multiple outcome scenarios with confidence levels",
            "Critically assess and analyse data models",
            "Improve reliability using ensambling techniques"
        ],
        "competence": [
            "Independently assess and critique analysis approaches",
            "Develop ethical approach to solving data problems",
            "Facilitate solution discussions among project members"
        ]
    },
    {
        "code": "FI1BBDV75", 
        "name": "Data Visualisation", 
        "type": "Core Course", 
        "credits": 7.5, 
        "semester": "2025 Fall",
        "weeks": 5,
        "hours": 210,
        "description": "Learn visualization and graphing techniques to represent data using graphical illustrations. Create intuitive graphs for professional settings and presentations.",
        "knowledge": [
            "Concepts, processes and tools for creating data visualizations",
            "Selecting correct visualization for problem domains",
            "Design principles for effective data visualizations",
            "User experience techniques for accessible visualizations"
        ],
        "skills": [
            "Select data subsets for visualization",
            "Communicate to non-technical audiences",
            "Master tools and techniques to visualize data",
            "Create slideshow presentations",
            "Identify problem areas and provide insights"
        ],
        "competence": [
            "Understand ethical requirements for data visualizations",
            "Develop ethical attitude in presentations and publications",
            "Apply visualization techniques based on audience",
            "Develop work methods to create graphics for clients"
        ]
    },
    {
        "code": "FI1BBAR05", 
        "name": "Analysis Reporting", 
        "type": "Core Course", 
        "credits": 5, 
        "semester": "2025 Fall",
        "weeks": 3,
        "hours": 126,
        "description": "Learn conclusive report writing methodologies to communicate results clearly and concisely. Cover technical vs non-technical reporting.",
        "knowledge": [
            "Report structure and organization",
            "Executive summaries writing",
            "Technical vs non-technical reporting",
            "Data documentation best practices",
            "Presenting findings to stakeholders"
        ],
        "skills": [
            "Write clear, concise analysis reports",
            "Structure reports for different audiences",
            "Document data analysis professionally",
            "Present findings effectively"
        ],
        "competence": [
            "Communicate results to various stakeholders",
            "Develop professional documentation standards",
            "Deliver insights in accessible formats"
        ]
    },
    {
        "code": "FI1BBP275", 
        "name": "Exam Project 1", 
        "type": "Core Course", 
        "credits": 7.5, 
        "semester": "2025 Fall",
        "weeks": 6,
        "hours": 252,
        "description": "Complete a comprehensive exam project demonstrating first-year competencies in data analysis, visualization, and reporting.",
        "knowledge": [
            "End-to-end data analysis workflow",
            "Professional presentation standards",
            "Portfolio development",
            "Self-assessment and reflection"
        ],
        "skills": [
            "Execute comprehensive data analysis project",
            "Present findings professionally",
            "Document work for portfolio",
            "Receive and apply peer feedback"
        ],
        "competence": [
            "Demonstrate first-year learning outcomes",
            "Work independently on complex projects",
            "Deliver professional-quality deliverables"
        ]
    },
    {
        "code": "FI2BCDC75", 
        "name": "Databases and Cloud Services", 
        "type": "Core Course", 
        "credits": 7.5, 
        "semester": "2026 Spring",
        "weeks": 4,
        "hours": 168,
        "description": "Learn core concepts of databases, SQL language, and cloud-based data services. Cover ETL practices, data warehouses, and on-premises vs cloud databases.",
        "knowledge": [
            "Data warehouses and ETL (Extract, Transform, Load) practices",
            "Database components for building and maintaining databases",
            "SQL data language for interfacing with databases",
            "On-premises vs cloud-based database decision-making",
            "History and traditions of databases and cloud services"
        ],
        "skills": [
            "Use cloud-based native tools to interact with databases",
            "Determine between on-premises and cloud-based solutions",
            "Apply ETL practices to stage data into access layers",
            "Use SQL to create, read, update and delete data",
            "Find and refer to database documentation"
        ],
        "competence": [
            "Plan and carry out database-related tasks independently or in groups",
            "Develop effective methods for database solutions",
            "Work according to ethical requirements and principles"
        ]
    },
    {
        "code": "FI2BCPP10", 
        "name": "Programming Fundamentals", 
        "type": "Core Course", 
        "credits": 10, 
        "semester": "2026 Spring",
        "weeks": 6,
        "hours": 252,
        "description": "Introduction to programming using Python 3.x. Learn data types, operators, collections, objects, file I/O, libraries, and APIs. Use Jupyter Notebook for documentation.",
        "knowledge": [
            "Computational thinking to solve data analysis problems",
            "Processes and techniques in Python programming",
            "Tools to export code examples to Markdown",
            "Data access layers and APIs",
            "History of programming languages"
        ],
        "skills": [
            "Use control structures and objects for iterative solutions",
            "Use APIs to access databases from programs",
            "Integrate databases with programming environment",
            "Use programming syntax and interactive interpreter",
            "Use alternative text editing syntax in coded reports",
            "Find materials about programming to develop robust programs"
        ],
        "competence": [
            "Create well-documented programs to solve real-world problems",
            "Write fast, powerful scripts ethically",
            "Collaborate with other analysts and programmers"
        ]
    },
    {
        "code": "FI2BCPA05", 
        "name": "Programmatic Data Analysis", 
        "type": "Core Course", 
        "credits": 5, 
        "semester": "2026 Spring",
        "weeks": 3,
        "hours": 126,
        "description": "Apply programming skills to automate and enhance data analysis workflows. Use pandas, numpy, and create reproducible analysis pipelines.",
        "knowledge": [
            "Data manipulation with pandas library",
            "Data cleaning with code",
            "Automated data pipelines",
            "Statistical analysis with Python",
            "Reproducible analysis workflows",
            "Version control basics (Git)"
        ],
        "skills": [
            "Manipulate data programmatically",
            "Clean and transform data with code",
            "Automate repetitive analysis tasks",
            "Perform statistical analysis with Python"
        ],
        "competence": [
            "Create reproducible analysis workflows",
            "Develop efficient data processing methods",
            "Collaborate using version control"
        ]
    },
    {
        "code": "FI2BCP175", 
        "name": "Semester Project 2", 
        "type": "Core Course", 
        "credits": 7.5, 
        "semester": "2026 Spring",
        "weeks": 4,
        "hours": 168,
        "description": "Apply second-year skills including databases, programming, and programmatic analysis to a comprehensive technical project.",
        "knowledge": [
            "Advanced project management",
            "Technical implementation standards",
            "Code documentation practices",
            "Testing and validation methods"
        ],
        "skills": [
            "Execute technical data analysis project",
            "Use databases and programming together",
            "Document code professionally",
            "Test and validate results"
        ],
        "competence": [
            "Work on complex technical projects",
            "Deliver professional technical deliverables",
            "Collaborate in development teams"
        ]
    },
    {
        "code": "FI2BCIT75", 
        "name": "Industry Tools", 
        "type": "Core Course", 
        "credits": 7.5, 
        "semester": "2026 Fall",
        "weeks": 5,
        "hours": 210,
        "description": "Learn industry-standard tools used by professional data analysts including Business Intelligence tools, ETL processes, and data warehousing.",
        "knowledge": [
            "Business Intelligence tools",
            "ETL processes and tools",
            "Data warehousing concepts",
            "Reporting automation",
            "Industry-standard software",
            "Tool selection criteria"
        ],
        "skills": [
            "Use BI tools for data analysis",
            "Implement ETL processes",
            "Work with data warehouses",
            "Automate reporting tasks"
        ],
        "competence": [
            "Select appropriate tools for projects",
            "Apply industry best practices",
            "Develop efficient work methods"
        ]
    },
    {
        "code": "FI2BCCT05", 
        "name": "Critical Data Thinking", 
        "type": "Core Course", 
        "credits": 5, 
        "semester": "2026 Fall",
        "weeks": 4,
        "hours": 168,
        "description": "Develop critical thinking skills for evaluating data and analysis quality. Cover data quality, bias, source credibility, and ethical data practices.",
        "knowledge": [
            "Data quality assessment methods",
            "Bias identification and mitigation",
            "Source credibility evaluation",
            "Logical reasoning with data",
            "Common data fallacies",
            "GDPR and ethical data practices"
        ],
        "skills": [
            "Assess data quality critically",
            "Identify and address bias in data",
            "Evaluate source credibility",
            "Apply logical reasoning to analysis"
        ],
        "competence": [
            "Think critically about data and results",
            "Maintain ethical standards in analysis",
            "Question assumptions and validate findings"
        ]
    },
    {
        "code": "FI2BCBD05", 
        "name": "Big Data and Advanced Topics", 
        "type": "Core Course", 
        "credits": 5, 
        "semester": "2026 Fall",
        "weeks": 4,
        "hours": 168,
        "description": "Explore big data technologies and advanced analytical concepts including distributed computing, data lakes, and machine learning basics.",
        "knowledge": [
            "Big data concepts and characteristics (Volume, Velocity, Variety)",
            "Introduction to distributed computing",
            "Data lakes vs data warehouses",
            "Machine learning basics",
            "Advanced analytics overview",
            "Future trends in data analysis"
        ],
        "skills": [
            "Work with big data concepts",
            "Understand distributed systems basics",
            "Apply basic machine learning concepts",
            "Evaluate advanced analytics solutions"
        ],
        "competence": [
            "Assess when big data solutions are needed",
            "Stay current with industry trends",
            "Apply advanced concepts appropriately"
        ]
    },
    {
        "code": "FI2BCID05", 
        "name": "Interactive Dashboards", 
        "type": "Core Course", 
        "credits": 5, 
        "semester": "2026 Fall",
        "weeks": 3,
        "hours": 126,
        "description": "Create interactive dashboards for data exploration. Cover dashboard design, universal design principles, real-time data integration, and tools like Tableau/Power BI.",
        "knowledge": [
            "Dashboard theory and design principles",
            "Universal design for accessibility",
            "Interactive elements and filters",
            "Real-time data integration",
            "Dashboard tools (Tableau, Power BI)",
            "Performance optimization"
        ],
        "skills": [
            "Design effective dashboards",
            "Create interactive data visualizations",
            "Integrate real-time data sources",
            "Optimize dashboard performance"
        ],
        "competence": [
            "Develop dashboards for various audiences",
            "Apply universal design principles",
            "Create accessible interactive experiences"
        ]
    },
    {
        "code": "FI2BCP275", 
        "name": "Exam Project 2", 
        "type": "Core Course", 
        "credits": 7.5, 
        "semester": "2026 Fall",
        "weeks": 6,
        "hours": 126,
        "description": "Complete a final capstone project demonstrating all program competencies. Full data analysis lifecycle from problem identification to stakeholder presentation.",
        "knowledge": [
            "Full data analysis lifecycle",
            "Professional documentation standards",
            "Stakeholder presentation techniques",
            "Portfolio finalization",
            "Career preparation"
        ],
        "skills": [
            "Execute end-to-end data analysis project",
            "Apply all learned techniques",
            "Present to stakeholders professionally",
            "Build professional portfolio"
        ],
        "competence": [
            "Demonstrate program competencies",
            "Work independently on complex projects",
            "Prepare for industry employment"
        ]
    },
]

knowledge_outcomes = [
    "Concepts and theories used in data analysis",
    "Processes and tools used for data analysis",
    "Databases, cloud services and native cloud tools used in data analysis",
    "Programming and programmatic data analysis",
    "Processes and tools for data visualization",
    "Problem identification methodologies for problem solving and data error discovery",
    "Conclusive report writing methodologies for clear communication",
    "Real-world situations to guide decision-making in data analysis",
    "Industry-relevant tools used in field data analysis",
    "Essential concepts in data science and engineering related to Big Data",
    "Dashboard theory, universal design principles and interactive dashboards",
    "Regulations, data analysis lifecycle and quantitative vs qualitative data",
    "GDPR guidelines, data maintenance and critical data thinking",
    "History, traditions and distinctive nature of the data analysis discipline"
]

skills_outcomes = [
    "Apply knowledge of data model results to business problems",
    "Apply data collection and cleaning from various sources to secure storage",
    "Master relevant tools, techniques and material for data analysis and presentation",
    "Master tools and techniques to generate and visualise data through reports and infographics",
    "Apply knowledge of suitable data analysis use-cases to project problems",
    "Explain vocational choices of tools, methods and techniques for data analysis",
    "Reflect over own vocational practice and adjust under supervision",
    "Find information about data analysis techniques relevant to projects",
    "Study workplace environments and identify issues through data analysis",
    "Find and interact with data from large data sources and cloud-based systems",
    "Find applicable data models for data sets during project planning"
]

competence_outcomes = [
    "Understand the ethical principles for sourced, stored, and used data",
    "Develop an ethical attitude as a responsible data analyst",
    "Plan and carry out data analysis tasks according to GDPR principles and practices",
    "Exchange points of view with others in data analysis and discuss good practices",
    "Contribute to organisational quality assurance and optimisation through data analysis",
    "Contribute to solving practical problems through computational thinking techniques",
    "Contribute to data safety by considering security measures in each project phase",
    "Develop products of relevance to data analysis and optimize own work methods"
]

# Initialize session state
if 'completed_courses' not in st.session_state:
    st.session_state.completed_courses = []
if 'knowledge_progress' not in st.session_state:
    st.session_state.knowledge_progress = [False] * len(knowledge_outcomes)
if 'skills_progress' not in st.session_state:
    st.session_state.skills_progress = [False] * len(skills_outcomes)
if 'competence_progress' not in st.session_state:
    st.session_state.competence_progress = [False] * len(competence_outcomes)
if 'training_progress' not in st.session_state:
    st.session_state.training_progress = {}
if 'quiz_scores' not in st.session_state:
    st.session_state.quiz_scores = {}
if 'current_lesson' not in st.session_state:
    st.session_state.current_lesson = 0
if 'quiz_answers' not in st.session_state:
    st.session_state.quiz_answers = {}
if 'show_exercise_answer' not in st.session_state:
    st.session_state.show_exercise_answer = {}

# Initialize new features session state
if 'study_notes' not in st.session_state:
    st.session_state.study_notes = {}
if 'flashcards' not in st.session_state:
    st.session_state.flashcards = {}
if 'flashcard_stats' not in st.session_state:
    st.session_state.flashcard_stats = {"total_cards": 0, "cards_reviewed": 0, "cards_mastered": 0}
if 'exam_mode' not in st.session_state:
    st.session_state.exam_mode = False
if 'exam_questions' not in st.session_state:
    st.session_state.exam_questions = []
if 'exam_answers' not in st.session_state:
    st.session_state.exam_answers = {}
if 'exam_start_time' not in st.session_state:
    st.session_state.exam_start_time = None
if 'code_snippets' not in st.session_state:
    st.session_state.code_snippets = {}
if 'code_snippet_favorites' not in st.session_state:
    st.session_state.code_snippet_favorites = []
if 'study_timer_active' not in st.session_state:
    st.session_state.study_timer_active = False
if 'study_timer_start' not in st.session_state:
    st.session_state.study_timer_start = None
if 'study_timer_duration' not in st.session_state:
    st.session_state.study_timer_duration = 0
if 'study_sessions' not in st.session_state:
    st.session_state.study_sessions = []
if 'study_time_by_course' not in st.session_state:
    st.session_state.study_time_by_course = {}
if 'important_dates' not in st.session_state:
    st.session_state.important_dates = []

def generate_practice_question(course, question_type="general"):
    course_info = f"Course: {course['name']}\nDescription: {course['description']}\nKnowledge topics: {', '.join(course['knowledge'][:3])}\nSkills: {', '.join(course['skills'][:3])}"
    
    prompts = {
        "general": f"Generate one practice question for this data analysis course:\n{course_info}\n\nFormat: Start with the question, then on a new line write 'ANSWER:' followed by a clear answer (2-3 sentences). Make it practical and test real understanding.",
        "knowledge": f"Generate a knowledge-based question testing theoretical understanding:\n{course_info}\n\nFormat: Question first, then 'ANSWER:' on new line with explanation.",
        "skills": f"Generate a practical skills-based question:\n{course_info}\n\nFormat: Describe a scenario or task, then 'ANSWER:' with the expected approach or solution.",
        "case_study": f"Generate a mini case study question:\n{course_info}\n\nFormat: Present a brief business scenario (2-3 sentences), ask what the student should do, then 'ANSWER:' with the recommended approach."
    }
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an educational tutor for a Data Analyst vocational program. Generate clear, practical questions that test understanding of data analysis concepts."},
                {"role": "user", "content": prompts.get(question_type, prompts["general"])}
            ],
            max_tokens=400
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error generating question: {str(e)}"

def evaluate_answer(question, correct_answer, user_answer):
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a supportive educational tutor. Evaluate student answers and provide constructive feedback. Be encouraging but accurate."},
                {"role": "user", "content": f"Question: {question}\n\nCorrect answer concept: {correct_answer}\n\nStudent's answer: {user_answer}\n\nProvide brief feedback (2-3 sentences): Is the answer correct or partially correct? What did they get right? What could be improved?"}
            ],
            max_tokens=200
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error evaluating answer: {str(e)}"

st.sidebar.title("Navigation")
sidebar_pages = [
    "Overview", "Course Plan", "Training Center", "Playground", "Learn & Practice",
    "Study Notes", "Flashcards", "Exam Simulator", "Code Library", "Formula Reference",
    "Study Timer", "Progress", "Learning Outcomes", "About"
]
sidebar_page_icons = {
    "Overview": ":material/home:",
    "Course Plan": ":material/map:",
    "Training Center": ":material/local_library:",
    "Playground": ":material/construction:",
    "Learn & Practice": ":material/auto_stories:",
    "Study Notes": ":material/note:",
    "Flashcards": ":material/style:",
    "Exam Simulator": ":material/fact_check:",
    "Code Library": ":material/code:",
    "Formula Reference": ":material/functions:",
    "Study Timer": ":material/timer:",
    "Progress": ":material/trending_up:",
    "Learning Outcomes": ":material/workspace_premium:",
    "About": ":material/info:"
}

if "sidebar_page_select" not in st.session_state:
    st.session_state.sidebar_page_select = "Overview"

if st.session_state.get("nav_target_page") in sidebar_pages:
    st.session_state.sidebar_page_select = st.session_state["nav_target_page"]

page = st.sidebar.radio(
    "Select page:",
    sidebar_pages,
    key="sidebar_page_select",
    format_func=lambda page_name: f"{sidebar_page_icons.get(page_name, ':material/chevron_right:')} {page_name}"
)

if st.session_state.get("nav_target_page") == page:
    st.session_state.pop("nav_target_page", None)

if page == "Overview":
    mui_title("school", "Data Analyst 2 - Study App")
    st.markdown("---")
    
    col1, col2, col3, col4 = st.columns(4)
    
    total_credits = sum(c["credits"] for c in courses_data)
    completed_credits = sum(c["credits"] for c in courses_data if c["code"] in st.session_state.completed_courses)
    
    with col1:
        st.metric("Total Credits", f"{int(total_credits)}")
    with col2:
        st.metric("Completed", f"{completed_credits:.1f}")
    with col3:
        st.metric("Remaining", f"{total_credits - completed_credits:.1f}")
    with col4:
        progress_pct = (completed_credits / total_credits * 100) if total_credits > 0 else 0
        st.metric("Progress", f"{progress_pct:.0f}%")
    
    st.markdown("---")
    mui_subheader("category", "Course Types Overview")

    course_type_explanations = {
        "Core Course": "Mandatory course that builds essential program competencies.",
        "Elective Course": "Optional course that lets you specialise in selected topics.",
        "Project Course": "Practice-focused course where you apply skills to real deliverables."
    }
    type_rows = []
    for course_type, explanation in course_type_explanations.items():
        count = sum(1 for c in courses_data if c.get("type") == course_type)
        type_rows.append({
            "Type": course_type,
            "What it means": explanation,
            "Courses in plan": count
        })

    st.dataframe(pd.DataFrame(type_rows), use_container_width=True, hide_index=True)
    st.caption("This overview connects program structure to your detailed course lessons and practice tasks.")

    st.markdown("---")
    mui_subheader("timeline", "Study Path")
    
    semesters = ["2025 Spring", "2025 Fall", "2026 Spring", "2026 Fall"]
    
    cols = st.columns(4)
    for i, sem in enumerate(semesters):
        with cols[i]:
            st.markdown(f"**{sem}**")
            sem_courses = [c for c in courses_data if c["semester"] == sem]
            sem_credits = sum(c["credits"] for c in sem_courses)
            st.caption(f"{sem_credits:.0f} credits")
            
            for course in sem_courses:
                is_completed = course["code"] in st.session_state.completed_courses
                status = "Completed" if is_completed else "Not started"
                st.markdown(f"{status} {course['name']}")
    
    st.markdown("---")
    col1, col2 = st.columns(2)
    
    with col1:
        mui_subheader("track_changes", "Training Progress")
        if st.session_state.training_progress:
            for topic, data in st.session_state.training_progress.items():
                short_topic = topic[:40] + "..." if len(topic) > 40 else topic
                lessons_done = data.get('lessons_completed', 0)
                total_lessons = len(training_modules.get(topic, {}).get('lessons', []))
                st.progress(lessons_done / total_lessons if total_lessons > 0 else 0)
                st.caption(f"{short_topic}: {lessons_done}/{total_lessons} lessons")
        else:
            st.info("Start training in the Training Center!")
    
    with col2:
        mui_subheader("link", "Useful Links")
        st.markdown("[Study Catalog](https://studiekatalog.edutorium.no/voc/en/programme/PDAN/2025-autumn)")
    
    st.markdown("---")
    
    # Define render_mui_icon function for Overview page
    def render_mui_icon(icon_name, size=20):
        """Render Material Icon as HTML"""
        return f'<span class="material-icons md-{size}" style="vertical-align: middle; font-size: {size}px;">{icon_name}</span>'
    
    st.markdown(f"### {render_mui_icon('event', 28)} Important Dates", unsafe_allow_html=True)
    
    # Add new date form
    with st.expander("Add Important Date", expanded=False):
        st.markdown(f"{render_mui_icon('add_circle', 20)} **Add a new important date**", unsafe_allow_html=True)
        date_col1, date_col2 = st.columns(2)
        with date_col1:
            new_date = st.date_input("Date:", key="new_important_date")
            new_date_type = st.selectbox(
                "Type:",
                ["Exam", "Assessment", "Assignment Deadline", "Project Deadline", "Course Start", "Course End", "Other"],
                key="new_date_type"
            )
        with date_col2:
            new_date_title = st.text_input("Title/Description:", placeholder="e.g., Final Exam - Data Analysis", key="new_date_title")
            new_date_course = st.selectbox(
                "Related Course (optional):",
                ["(None)"] + [f"{c['code']} - {c['name']}" for c in courses_data],
                key="new_date_course"
            )
        
        if st.button("Add Date", type="primary", icon=":material/add:", key="add_important_date"):
            if new_date_title:
                date_entry = {
                    "date": new_date.isoformat(),
                    "type": new_date_type,
                    "title": new_date_title,
                    "course": new_date_course if new_date_course != "(None)" else None
                }
                st.session_state.important_dates.append(date_entry)
                st.success(f"Added: {new_date_type} - {new_date_title}")
                st.rerun()
            else:
                st.warning("Please enter a title/description.")
    
    # Display important dates
    if st.session_state.important_dates:
        # Sort dates
        sorted_dates = sorted(st.session_state.important_dates, key=lambda x: x["date"])
        
        # Get today's date for comparison
        today = date.today()
        
        # Separate upcoming and past dates
        upcoming_dates = []
        past_dates = []
        
        for date_entry in sorted_dates:
            event_date = date.fromisoformat(date_entry["date"])
            if event_date >= today:
                upcoming_dates.append(date_entry)
            else:
                past_dates.append(date_entry)
        
        # Display upcoming dates
        if upcoming_dates:
            st.markdown(f"**{render_mui_icon('schedule', 18)} Upcoming Dates:**", unsafe_allow_html=True)
            for idx, date_entry in enumerate(upcoming_dates):
                event_date = date.fromisoformat(date_entry["date"])
                days_until = (event_date - today).days
                
                # Icon and color coding based on urgency
                if days_until <= 7:
                    urgency_icon = render_mui_icon('error', 20)
                    urgency_color = "#dc3545"
                elif days_until <= 30:
                    urgency_icon = render_mui_icon('warning', 20)
                    urgency_color = "#ffc107"
                else:
                    urgency_icon = render_mui_icon('check_circle', 20)
                    urgency_color = "#28a745"
                
                # Type icon mapping
                type_icons = {
                    "Exam": "quiz",
                    "Assessment": "grading",
                    "Assignment Deadline": "assignment",
                    "Project Deadline": "folder",
                    "Course Start": "play_arrow",
                    "Course End": "stop",
                    "Other": "event"
                }
                type_icon = render_mui_icon(type_icons.get(date_entry['type'], 'event'), 18)
                
                date_col1, date_col2, date_col3 = st.columns([3, 2, 1])
                with date_col1:
                    course_info = f" ({date_entry['course']})" if date_entry.get('course') else ""
                    st.markdown(f"{urgency_icon} {type_icon} **{date_entry['type']}:** {date_entry['title']}{course_info}", unsafe_allow_html=True)
                with date_col2:
                    st.markdown(f"{render_mui_icon('calendar_today', 16)} {event_date.strftime('%B %d, %Y')}", unsafe_allow_html=True)
                with date_col3:
                    if days_until == 0:
                        st.markdown(f"**{render_mui_icon('today', 16)} Today!**", unsafe_allow_html=True)
                    elif days_until == 1:
                        st.markdown(f"**{render_mui_icon('schedule', 16)} Tomorrow**", unsafe_allow_html=True)
                    else:
                        st.markdown(f"{render_mui_icon('schedule', 16)} {days_until} days", unsafe_allow_html=True)
                    
                    # Delete button
                    delete_btn = st.button("Delete", icon=":material/delete:", key=f"delete_date_{idx}", help="Delete this date")
                    if delete_btn:
                        st.session_state.important_dates.remove(date_entry)
                        st.rerun()
        
        # Display past dates (collapsed)
        if past_dates:
            with st.expander(f"Past Dates ({len(past_dates)})", expanded=False):
                st.markdown(f"{render_mui_icon('history', 18)} **Completed dates**", unsafe_allow_html=True)
                for idx, date_entry in enumerate(past_dates):
                    event_date = date.fromisoformat(date_entry["date"])
                    days_ago = (today - event_date).days
                    
                    # Type icon mapping
                    type_icons = {
                        "Exam": "quiz",
                        "Assessment": "grading",
                        "Assignment Deadline": "assignment",
                        "Project Deadline": "folder",
                        "Course Start": "play_arrow",
                        "Course End": "stop",
                        "Other": "event"
                    }
                    type_icon = render_mui_icon(type_icons.get(date_entry['type'], 'event'), 18)
                    
                    past_col1, past_col2, past_col3 = st.columns([3, 2, 1])
                    with past_col1:
                        course_info = f" ({date_entry['course']})" if date_entry.get('course') else ""
                        st.markdown(f"{render_mui_icon('check_circle', 16)} {type_icon} **{date_entry['type']}:** {date_entry['title']}{course_info}", unsafe_allow_html=True)
                    with past_col2:
                        st.markdown(f"{render_mui_icon('calendar_today', 16)} {event_date.strftime('%B %d, %Y')}", unsafe_allow_html=True)
                    with past_col3:
                        st.markdown(f"{render_mui_icon('schedule', 16)} {days_ago} days ago", unsafe_allow_html=True)
                        
                        # Delete button
                        delete_btn = st.button("Delete", icon=":material/delete:", key=f"delete_past_date_{idx}", help="Delete this date")
                        if delete_btn:
                            st.session_state.important_dates.remove(date_entry)
                            st.rerun()
    else:
        st.markdown(f"{render_mui_icon('info', 18)} **No important dates added yet.** Use the form above to add dates like exams, deadlines, etc.", unsafe_allow_html=True)
        st.info("Tip: Add important dates like exam/assessment dates, assignment deadlines, and project milestones to keep track of your schedule.")

elif page == "Training Center":
    mui_title("local_library", "Training Center")
    st.markdown("*Hands-on learning with step-by-step lessons, exercises, and quizzes*")
    st.markdown("---")
    
    # Organize topics by semester and course (with official course codes)
    course_to_semester = {
        "Data Analysis Fundamentals": ("Semester 1", "FI1BBDF05"),
        "Spreadsheet Fundamentals": ("Semester 1", "FI1BBSF05"),
        "Statistical Tools": ("Semester 1", "FI1BBST05"),
        "Programming Fundamentals": ("Semester 1", "FI1BBPF20"),
        "Databases and Cloud Services": ("Semester 2", "FI1BBDC20"),
        "Data Visualisation": ("Semester 2", "FI1BBDV15"),
        "Data Driven Decision-Making": ("Semester 2", "FI1BBDD75"),
        "Semester Project 1": ("Semester 2", "FI1BBP175"),
        "Evaluation of Outcomes": ("Semester 3", "FI1BBEO10")
    }
    
    # Group topics by semester and course
    organized_topics = {}
    for topic, module_data in training_modules.items():
        course = module_data['course']
        if course in course_to_semester:
            semester, code = course_to_semester[course]
        else:
            semester, code = "Other", ""
        
        if semester not in organized_topics:
            organized_topics[semester] = {}
        if course not in organized_topics[semester]:
            organized_topics[semester][course] = {"code": code, "topics": []}
        organized_topics[semester][course]["topics"].append(topic)
    
    # Create formatted options with grouping
    st.markdown("### Choose a Topic to Learn")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        # Semester filter
        available_semesters = sorted(organized_topics.keys())
        selected_semester = st.selectbox(
            "Semester:",
            options=["All Semesters"] + available_semesters
        )
    
    with col2:
        # Course filter based on semester (with course codes)
        if selected_semester == "All Semesters":
            all_courses = []
            for sem in available_semesters:
                for course in organized_topics[sem].keys():
                    if course not in all_courses:
                        all_courses.append(course)
            available_courses = all_courses
        else:
            available_courses = list(organized_topics[selected_semester].keys())
        
        # Create display names with course codes
        course_display_map = {}
        for course in available_courses:
            if course in course_to_semester:
                code = course_to_semester[course][1]
                display = f"{code} - {course}"
            else:
                display = course
            course_display_map[display] = course
        
        course_options = ["All Courses"] + list(course_display_map.keys())
        selected_course_display = st.selectbox(
            "Course:",
            options=course_options
        )
        
        # Map back to actual course name
        if selected_course_display == "All Courses":
            selected_course = "All Courses"
        else:
            selected_course = course_display_map[selected_course_display]
    
    # Get filtered topics
    filtered_topics = []
    for semester, courses in organized_topics.items():
        if selected_semester != "All Semesters" and semester != selected_semester:
            continue
        for course, data in courses.items():
            if selected_course != "All Courses" and course != selected_course:
                continue
            for topic in data["topics"]:
                # Format: "Topic Name (Course - Semester)"
                display_name = f"{topic}"
                filtered_topics.append((display_name, topic, course, semester))
    
    # Sort by semester, then course, then topic
    semester_order = {"Semester 1": 1, "Semester 2": 2, "Semester 3": 3, "Semester 4": 4, "Other": 5}
    filtered_topics.sort(key=lambda x: (semester_order.get(x[3], 99), x[2], x[0]))
    
    if not filtered_topics:
        st.warning("No topics found for the selected filters.")
        st.stop()
    
    # Create display options with context
    topic_options = []
    for display, topic, course, semester in filtered_topics:
        topic_options.append(f"{topic}")
    
    # Show topic count
    st.caption(f"{len(filtered_topics)} topics available")
    
    selected_display = st.selectbox(
        "Select Topic:",
        options=topic_options,
        format_func=lambda x: x
    )
    
    # Find the actual topic
    selected_topic = selected_display
    
    # Find course and semester for display
    topic_course = None
    topic_semester = None
    for display, topic, course, semester in filtered_topics:
        if topic == selected_topic:
            topic_course = course
            topic_semester = semester
            break
    
    module = training_modules[selected_topic]
    
    # Get course code for display
    topic_code = ""
    if topic_course and topic_course in course_to_semester:
        topic_code = course_to_semester[topic_course][1]
    
    # Show context with course code
    st.markdown(f"**{topic_semester}** | **{topic_code} - {topic_course}**")
    st.markdown(f"*{module['description']}*")
    
    # Initialize progress for this topic
    if selected_topic not in st.session_state.training_progress:
        st.session_state.training_progress[selected_topic] = {
            'lessons_completed': 0,
            'exercises_completed': [],
            'quiz_score': None
        }
    
    progress = st.session_state.training_progress[selected_topic]
    
    # Progress bar
    total_items = len(module['lessons']) + len(module['exercises']) + 1  # +1 for quiz
    completed_items = progress['lessons_completed'] + len(progress['exercises_completed']) + (1 if progress['quiz_score'] is not None else 0)
    st.progress(completed_items / total_items)
    st.caption(f"Progress: {completed_items}/{total_items} items completed")
    
    st.markdown("---")
    
    tab1, tab2, tab3 = st.tabs(["Lessons", "Exercises", "Quiz"])
    
    with tab1:
        mui_subheader("menu_book", "Step-by-Step Lessons")
        
        for i, lesson in enumerate(module['lessons']):
            with st.expander(f"Lesson {i+1}: {lesson['title']}", expanded=(i == 0)):
                st.markdown(lesson['content'])
                
                st.markdown("---")
                st.markdown("**Key Takeaways:**")
                for point in lesson['key_points']:
                    st.markdown(f"âœ“ {point}")
                
                if st.button(f"Mark Lesson {i+1} Complete", key=f"lesson_{selected_topic}_{i}"):
                    if progress['lessons_completed'] <= i:
                        st.session_state.training_progress[selected_topic]['lessons_completed'] = i + 1
                        st.success(f"Lesson {i+1} completed!")
                        st.rerun()
                
                if progress['lessons_completed'] > i:
                    st.success("Completed")
    
    with tab2:
        mui_subheader("edit_note", "Hands-On Exercises")
        
        for i, exercise in enumerate(module['exercises']):
            with st.expander(f"Exercise {i+1}: {exercise['title']}", expanded=False):
                st.markdown(f"**Type:** {exercise['type'].title()}")
                st.markdown("---")
                st.markdown(f"**{exercise['question']}**")
                
                # Hint button
                if st.button(f"Show Hint", key=f"hint_{selected_topic}_{i}"):
                    st.info(f"Hint: {exercise['hint']}")
                
                # User answer input
                user_answer = st.text_area(
                    "Your answer:",
                    key=f"exercise_answer_{selected_topic}_{i}",
                    placeholder="Type your answer here..."
                )
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("Check Answer", icon=":material/task_alt:", key=f"check_{selected_topic}_{i}"):
                        if user_answer.strip():
                            with st.spinner("Evaluating..."):
                                feedback = evaluate_answer(exercise['question'], exercise['answer'], user_answer)
                                st.success(feedback)
                                if i not in progress['exercises_completed']:
                                    st.session_state.training_progress[selected_topic]['exercises_completed'].append(i)
                        else:
                            st.warning("Please enter an answer first.")
                
                with col2:
                    show_key = f"show_{selected_topic}_{i}"
                    if st.button("Show Answer", icon=":material/visibility:", key=f"reveal_{selected_topic}_{i}"):
                        st.session_state.show_exercise_answer[show_key] = True
                
                if st.session_state.show_exercise_answer.get(f"show_{selected_topic}_{i}", False):
                    st.info(f"**Answer:** {exercise['answer']}")
                
                if i in progress['exercises_completed']:
                    st.success("Attempted")
    
    with tab3:
        mui_subheader("quiz", "Knowledge Quiz")
        st.markdown("Test your understanding with this quiz!")
        
        quiz = module['quiz']
        
        if progress['quiz_score'] is not None:
            st.success(f"Quiz completed! Score: {progress['quiz_score']}/{len(quiz)}")
            if st.button("Retake Quiz", icon=":material/replay:"):
                st.session_state.training_progress[selected_topic]['quiz_score'] = None
                st.session_state.quiz_answers = {}
                st.rerun()
        else:
            for i, q in enumerate(quiz):
                st.markdown(f"**Q{i+1}: {q['question']}**")
                answer = st.radio(
                    "Select your answer:",
                    options=q['options'],
                    key=f"quiz_{selected_topic}_{i}",
                    index=None
                )
                st.session_state.quiz_answers[f"{selected_topic}_{i}"] = q['options'].index(answer) if answer else None
                st.markdown("---")
            
            if st.button("Submit Quiz", type="primary", icon=":material/send:"):
                score = 0
                for i, q in enumerate(quiz):
                    user_ans = st.session_state.quiz_answers.get(f"{selected_topic}_{i}")
                    if user_ans == q['correct']:
                        score += 1
                
                st.session_state.training_progress[selected_topic]['quiz_score'] = score
                st.success(f"Quiz completed! Score: {score}/{len(quiz)}")
                
                # Show explanations
                st.markdown("### Results:")
                for i, q in enumerate(quiz):
                    user_ans = st.session_state.quiz_answers.get(f"{selected_topic}_{i}")
                    correct = user_ans == q['correct']
                    icon = "âœ…" if correct else "âŒ"
                    st.markdown(f"{icon} **Q{i+1}:** {q['question']}")
                    if not correct:
                        st.markdown(f"   Correct answer: {q['options'][q['correct']]}")
                    st.markdown(f"   *{q['explanation']}*")

elif page == "Course Plan":
    mui_title("map", "Course Plan")
    st.markdown("---")
    
    df = pd.DataFrame([{
        "Code": c["code"],
        "Course": c["name"],
        "Credits": c["credits"],
        "Weeks": c["weeks"],
        "Hours": c["hours"],
        "Semester": c["semester"]
    } for c in courses_data])
    
    col1, col2 = st.columns(2)
    with col1:
        semester_filter = st.multiselect(
            "Filter by semester:",
            options=["2025 Spring", "2025 Fall", "2026 Spring", "2026 Fall"],
            default=[]
        )
    with col2:
        search = st.text_input("Search courses:", "")
    
    filtered_df = df.copy()
    if semester_filter:
        filtered_df = filtered_df[filtered_df["Semester"].isin(semester_filter)]
    if search:
        filtered_df = filtered_df[
            filtered_df["Course"].str.lower().str.contains(search.lower()) |
            filtered_df["Code"].str.lower().str.contains(search.lower())
        ]
    
    st.dataframe(filtered_df, use_container_width=True, hide_index=True)
    
    st.markdown("---")
    mui_subheader("bar_chart", "Credits per Semester")
    
    semester_credits = df.groupby("Semester")["Credits"].sum().reset_index()
    semester_order = ["2025 Spring", "2025 Fall", "2026 Spring", "2026 Fall"]
    semester_credits["Semester"] = pd.Categorical(semester_credits["Semester"], categories=semester_order, ordered=True)
    semester_credits = semester_credits.sort_values("Semester")
    
    st.bar_chart(semester_credits.set_index("Semester"))

elif page == "Learn & Practice":
    mui_title("auto_stories", "Learn & Practice")
    st.markdown("---")
    
    # Add custom CSS for lesson styling and Mermaid.js
    st.markdown("""
    <style>
    .lesson-highlight {
        background: linear-gradient(120deg, #a8e6cf 0%, #dcedc1 100%);
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #4CAF50;
        margin: 10px 0;
    }
    .key-concept {
        background: #fff3cd;
        padding: 12px;
        border-radius: 6px;
        border-left: 4px solid #ffc107;
        margin: 8px 0;
        color: #333;
    }
    .key-concept * {
        color: #333 !important;
    }
    .important-info {
        background: #e3f2fd;
        padding: 12px;
        border-radius: 6px;
        border-left: 4px solid #2196F3;
        margin: 8px 0;
        color: #333;
    }
    .important-info * {
        color: #333 !important;
    }
    .visual-box {
        background: #f5f5f5;
        padding: 15px;
        border-radius: 8px;
        border: 2px solid #ddd;
        margin: 15px 0;
        font-family: 'Courier New', monospace;
    }
    .mermaid-container {
        background: white;
        padding: 20px;
        border-radius: 8px;
        margin: 20px 0;
        text-align: center;
        border: 1px solid #e0e0e0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Note: Mermaid.js will be loaded per diagram using st.components.v1.html
    
    selected_course = st.selectbox(
        "Select a course to study:",
        options=[f"{c['code']} - {c['name']}" for c in courses_data],
        index=0
    )
    
    course_code = selected_course.split(" - ")[0]
    course = next(c for c in courses_data if c["code"] == course_code)
    
    st.markdown("---")
    
    # Check if this course has training modules
    course_topics_with_training = []
    for topic in training_modules.keys():
        if training_modules[topic]['course'] == course['name']:
            course_topics_with_training.append(topic)
    
    if course_topics_with_training:
        st.info(f"This course has {len(course_topics_with_training)} topic(s) with hands-on training available in the Training Center.")
    
    # Check if course has lessons
    has_lessons = course_code in course_lessons
    if has_lessons:
        st.success(f"This course has {len(course_lessons[course_code])} detailed lesson(s) available.")
    
    tab1, tab2 = st.tabs(["Course Content", "Practice Questions"])
    
    with tab1:
        st.subheader(f"{course['name']}")
        st.markdown(f"**{course['credits']} credits** | {course['weeks']} weeks | {course['hours']} hours | {course['semester']} | **Type: {course['type']}**")

        selected_course_type = course.get("type", "Core Course")
        selected_type_explanations = {
            "Core Course": "Mandatory course linked to core program outcomes and progression.",
            "Elective Course": "Optional course used to deepen a chosen specialization area.",
            "Project Course": "Application-focused course where skills are demonstrated through practical deliverables."
        }
        st.caption(f"{selected_course_type}: {selected_type_explanations.get(selected_course_type, 'Course classification used in the study plan.')} This helps connect the overview structure with the lesson content below.")
        st.markdown(f"*{course['description']}*")

        linked_dates = [d for d in st.session_state.important_dates if d.get("course") == selected_course]
        if linked_dates:
            st.markdown("### Course-linked Important Dates")
            st.caption("These dates are connected to this course and support lesson/exam planning.")

            today = date.today()
            linked_dates_sorted = sorted(linked_dates, key=lambda x: x["date"])
            linked_upcoming = [d for d in linked_dates_sorted if date.fromisoformat(d["date"]) >= today]
            linked_past = [d for d in linked_dates_sorted if date.fromisoformat(d["date"]) < today]

            type_icons = {
                "Exam": "quiz",
                "Assessment": "grading",
                "Assignment Deadline": "assignment",
                "Project Deadline": "folder",
                "Course Start": "play_arrow",
                "Course End": "stop",
                "Other": "event"
            }

            if linked_upcoming:
                st.markdown("**Upcoming for this course**")
                for d in linked_upcoming:
                    event_date = date.fromisoformat(d["date"])
                    days_until = (event_date - today).days
                    icon = render_mui_icon(type_icons.get(d["type"], "event"), 18)
                    st.markdown(
                        f"{icon} **{d['type']}** â€” {d['title']} ({event_date.strftime('%B %d, %Y')}, in {days_until} days)",
                        unsafe_allow_html=True
                    )

            if linked_past:
                with st.expander(f"Past course dates ({len(linked_past)})", expanded=False):
                    for d in linked_past:
                        event_date = date.fromisoformat(d["date"])
                        days_ago = (today - event_date).days
                        icon = render_mui_icon(type_icons.get(d["type"], "event"), 18)
                        st.markdown(
                            f"{icon} **{d['type']}** â€” {d['title']} ({event_date.strftime('%B %d, %Y')}, {days_ago} days ago)",
                            unsafe_allow_html=True
                        )

            st.markdown("---")
        else:
            st.info("No important dates linked to this course yet. Add one from Overview and choose this course in Related Course.")
        
        st.markdown("---")
        
        # Check if course has lessons
        if course_code in course_lessons:
            st.markdown("### Course Lessons")
            st.markdown("Explore detailed lessons with visual explanations and key concepts.")
            st.markdown("")
            
            for lesson in course_lessons[course_code]:
                with st.expander(f"Lesson {lesson['lesson_number']}: {lesson['title']}", expanded=True):
                    # Parse and render lesson content with Mermaid diagrams
                    content = convert_emoji_headings_to_visuals(lesson['content'])
                    
                    # Split content by Mermaid diagrams
                    parts = re.split(r'(<div class="mermaid">.*?</div>)', content, flags=re.DOTALL)
                    
                    for part in parts:
                        if part.strip().startswith('<div class="mermaid">'):
                            # Extract Mermaid code
                            mermaid_match = re.search(r'<div class="mermaid">(.*?)</div>', part, re.DOTALL)
                            if mermaid_match:
                                mermaid_code = mermaid_match.group(1).strip()
                                # Render Mermaid diagram using HTML component
                                # Render Mermaid diagram with zoom and pan controls
                                mermaid_html = f"""
                                <!DOCTYPE html>
                                <html>
                                <head>
                                    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
                                    <style>
                                        * {{
                                            box-sizing: border-box;
                                        }}
                                        html, body {{
                                            margin: 0; 
                                            padding: 0; 
                                            width: 100%;
                                            height: 100%;
                                            background: white; 
                                            overflow: hidden;
                                        }}
                                        .diagram-container {{
                                            width: 100%;
                                            height: 100%;
                                            min-height: 500px;
                                            overflow: auto;
                                            border: 1px solid #e0e0e0;
                                            border-radius: 8px;
                                            background: #fafafa;
                                            position: relative;
                                        }}
                                        .mermaid-wrapper {{
                                            width: 100%;
                                            min-height: 100%;
                                            padding: 40px;
                                            display: flex;
                                            justify-content: center;
                                            align-items: flex-start;
                                        }}
                                        .mermaid {{
                                            text-align: center;
                                            width: 100%;
                                            max-width: 100%;
                                        }}
                                        .mermaid svg {{
                                            max-width: 100%;
                                            height: auto;
                                        }}
                                        .controls {{
                                            position: sticky;
                                            top: 10px;
                                            right: 10px;
                                            z-index: 1000;
                                            display: flex;
                                            gap: 5px;
                                            float: right;
                                            margin: 10px;
                                        }}
                                        .control-btn {{
                                            background: #4A90D9;
                                            color: white;
                                            border: none;
                                            padding: 8px 12px;
                                            border-radius: 4px;
                                            cursor: pointer;
                                            font-size: 12px;
                                            box-shadow: 0 2px 4px rgba(0,0,0,0.2);
                                        }}
                                        .control-btn:hover {{
                                            background: #2E5C8A;
                                        }}
                                    </style>
                                </head>
                                <body>
                                    <div class="diagram-container" id="diagram-container">
                                        <div class="controls">
                                            <button class="control-btn" onclick="zoomIn()">Zoom In +</button>
                                            <button class="control-btn" onclick="zoomOut()">Zoom Out -</button>
                                            <button class="control-btn" onclick="resetZoom()">Reset</button>
                                        </div>
                                        <div class="mermaid-wrapper" id="mermaid-wrapper">
                                            <div class="mermaid" id="mermaid-diagram">
{mermaid_code}
                                            </div>
                                        </div>
                                    </div>
                                    <script>
                                        let currentZoom = 1;
                                        const container = document.getElementById('diagram-container');
                                        const wrapper = document.getElementById('mermaid-wrapper');
                                        
                                        function zoomIn() {{
                                            currentZoom = Math.min(currentZoom + 0.2, 3);
                                            wrapper.style.transform = `scale(${{currentZoom}})`;
                                            wrapper.style.transformOrigin = 'center center';
                                        }}
                                        
                                        function zoomOut() {{
                                            currentZoom = Math.max(currentZoom - 0.2, 0.5);
                                            wrapper.style.transform = `scale(${{currentZoom}})`;
                                            wrapper.style.transformOrigin = 'center center';
                                        }}
                                        
                                        function resetZoom() {{
                                            currentZoom = 1;
                                            wrapper.style.transform = 'scale(1)';
                                            container.scrollTop = 0;
                                            container.scrollLeft = 0;
                                        }}
                                        
                                        // Enable panning with mouse drag
                                        let isDragging = false;
                                        let startX, startY, scrollLeft, scrollTop;
                                        
                                        container.addEventListener('mousedown', (e) => {{
                                            isDragging = true;
                                            startX = e.pageX - container.offsetLeft;
                                            startY = e.pageY - container.offsetTop;
                                            scrollLeft = container.scrollLeft;
                                            scrollTop = container.scrollTop;
                                            container.style.cursor = 'grabbing';
                                        }});
                                        
                                        container.addEventListener('mouseleave', () => {{
                                            isDragging = false;
                                            container.style.cursor = 'grab';
                                        }});
                                        
                                        container.addEventListener('mouseup', () => {{
                                            isDragging = false;
                                            container.style.cursor = 'grab';
                                        }});
                                        
                                        container.addEventListener('mousemove', (e) => {{
                                            if (!isDragging) return;
                                            e.preventDefault();
                                            const x = e.pageX - container.offsetLeft;
                                            const y = e.pageY - container.offsetTop;
                                            const walkX = (x - startX) * 2;
                                            const walkY = (y - startY) * 2;
                                            container.scrollLeft = scrollLeft - walkX;
                                            container.scrollTop = scrollTop - walkY;
                                        }});
                                        
                                        container.style.cursor = 'grab';
                                        
                                        if (typeof mermaid !== 'undefined') {{
                                            mermaid.initialize({{ 
                                                startOnLoad: true, 
                                                theme: 'default',
                                                themeVariables: {{ 
                                                    primaryColor: '#4A90D9',
                                                    primaryTextColor: '#fff',
                                                    primaryBorderColor: '#2E5C8A',
                                                    lineColor: '#4A90D9',
                                                    secondaryColor: '#FFD700',
                                                    tertiaryColor: '#FF6B6B'
                                                }},
                                                flowchart: {{
                                                    useMaxWidth: true,
                                                    htmlLabels: true,
                                                    curve: 'basis'
                                                }}
                                            }});
                                        }}
                                    </script>
                                </body>
                                </html>
                                """
                                html(mermaid_html, height=700, width=None, scrolling=False)
                        else:
                            # Render regular markdown content
                            if part.strip():
                                st.markdown(part, unsafe_allow_html=True)
                    
                    st.markdown("---")
                    
                    # Key Takeaways with visual emphasis
                    st.markdown("### Key Takeaways")
                    st.markdown('<div class="important-info">', unsafe_allow_html=True)
                    for i, point in enumerate(lesson['key_points'], 1):
                        st.markdown(f"**{i}.** {point}")
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Visual elements indicator
                    if lesson.get('visual_elements', {}).get('diagrams'):
                        st.caption("This lesson includes interactive Mermaid diagrams for better visual understanding")

                    if course_code == "FI1BBSF05" and str(lesson.get("lesson_number")) == "1.4":
                        st.markdown("---")
                        with st.expander("Activity 1.4.1 - Interactive Data Validation Lab", expanded=False):
                            st.caption("Add rows in the table below. The script runs only when all validation rules are correct.")

                            item_options_key = f"dv_item_options_{course_code}_{lesson['lesson_number']}"
                            table_state_key = f"dv_table_data_{course_code}_{lesson['lesson_number']}"
                            script_key = f"dv_script_{course_code}_{lesson['lesson_number']}"
                            run_key = f"dv_run_{course_code}_{lesson['lesson_number']}"

                            default_item_options_text = "Pen, Notebook, Eraser, Ruler, Marker"
                            item_options_text = st.text_input(
                                "Item Name options (exactly 5 comma-separated values)",
                                value=st.session_state.get(item_options_key, default_item_options_text),
                                key=item_options_key
                            )

                            parsed_item_options = [value.strip() for value in item_options_text.split(",") if value.strip()]
                            if len(parsed_item_options) != 5:
                                st.warning("Please provide exactly 5 item names for the Item Name list validation.")
                                active_item_options = ["Pen", "Notebook", "Eraser", "Ruler", "Marker"]
                            else:
                                active_item_options = parsed_item_options

                            status_options = ["In Progress", "Completed", "Cancelled"]

                            if table_state_key not in st.session_state:
                                st.session_state[table_state_key] = pd.DataFrame({
                                    "Item Name": [active_item_options[i % len(active_item_options)] for i in range(10)],
                                    "Quantity": [1, 2, 3, 4, 5, 6, 2, 3, 4, 5],
                                    "Unit Price": [10.50, 12.00, 8.75, 3.99, 5.40, 14.25, 9.10, 7.80, 6.30, 11.00],
                                    "Date": pd.to_datetime([
                                        "2026-01-01", "2026-01-02", "2026-01-03", "2026-01-04", "2026-01-05",
                                        "2026-01-06", "2026-01-07", "2026-01-08", "2026-01-09", "2026-01-10"
                                    ]).date,
                                    "Status": ["In Progress", "Completed", "Cancelled", "In Progress", "Completed", "Cancelled", "In Progress", "Completed", "Cancelled", "In Progress"]
                                })

                            st.markdown("**Required columns:** Item Name, Quantity, Unit Price, Date, Status")

                            edited_validation_df = st.data_editor(
                                st.session_state[table_state_key],
                                num_rows="dynamic",
                                use_container_width=True,
                                key=f"dv_editor_{course_code}_{lesson['lesson_number']}",
                                column_config={
                                    "Item Name": st.column_config.SelectboxColumn("Item Name", options=active_item_options, required=True),
                                    "Quantity": st.column_config.NumberColumn("Quantity", min_value=1, step=1, required=True),
                                    "Unit Price": st.column_config.NumberColumn("Unit Price", min_value=0.01, max_value=1000.00, step=0.01, required=True),
                                    "Date": st.column_config.DateColumn("Date", format="MM/DD/YYYY", required=True),
                                    "Status": st.column_config.SelectboxColumn("Status", options=status_options, required=True),
                                }
                            )
                            st.session_state[table_state_key] = edited_validation_df

                            def _validate_activity_141(df_input: pd.DataFrame, item_choices: list[str], status_choices: list[str]):
                                validation_issues = []

                                required_columns = ["Item Name", "Quantity", "Unit Price", "Date", "Status"]
                                missing_columns = [column for column in required_columns if column not in df_input.columns]
                                if missing_columns:
                                    validation_issues.append({
                                        "Row": "Global",
                                        "Issue": f"Missing required columns: {', '.join(missing_columns)}"
                                    })
                                    return validation_issues

                                if len(df_input) < 10:
                                    validation_issues.append({
                                        "Row": "Global",
                                        "Issue": "At least 10 rows are required for this activity."
                                    })

                                for row_idx, row in df_input.reset_index(drop=True).iterrows():
                                    display_row = row_idx + 1

                                    item_name = str(row.get("Item Name", "")).strip()
                                    if not item_name:
                                        validation_issues.append({"Row": display_row, "Issue": "Item Name is required."})
                                    else:
                                        if item_name not in item_choices:
                                            validation_issues.append({"Row": display_row, "Issue": "Item Name must be one of the 5 allowed list values."})
                                        if len(item_name) < 3 or len(item_name) > 10:
                                            validation_issues.append({"Row": display_row, "Issue": "Item Name must be between 3 and 10 characters."})

                                    quantity_value = row.get("Quantity")
                                    if pd.isna(quantity_value):
                                        validation_issues.append({"Row": display_row, "Issue": "Quantity is required."})
                                    else:
                                        quantity_numeric = pd.to_numeric(quantity_value, errors="coerce")
                                        if pd.isna(quantity_numeric) or float(quantity_numeric) <= 0 or not float(quantity_numeric).is_integer():
                                            validation_issues.append({"Row": display_row, "Issue": "Quantity must be a positive whole number."})

                                    unit_price_value = row.get("Unit Price")
                                    if pd.isna(unit_price_value):
                                        validation_issues.append({"Row": display_row, "Issue": "Unit Price is required."})
                                    else:
                                        unit_price_numeric = pd.to_numeric(unit_price_value, errors="coerce")
                                        if pd.isna(unit_price_numeric) or float(unit_price_numeric) < 0.01 or float(unit_price_numeric) > 1000.00:
                                            validation_issues.append({"Row": display_row, "Issue": "Unit Price must be between 0.01 and 1000.00."})

                                    date_value = row.get("Date")
                                    if pd.isna(date_value):
                                        validation_issues.append({"Row": display_row, "Issue": "Date is required (mm/dd/yyyy)."})
                                    else:
                                        parsed_date = pd.to_datetime(date_value, errors="coerce")
                                        if pd.isna(parsed_date):
                                            validation_issues.append({"Row": display_row, "Issue": "Date must be a valid date in mm/dd/yyyy format."})

                                    status_value = str(row.get("Status", "")).strip()
                                    if status_value not in status_choices:
                                        validation_issues.append({"Row": display_row, "Issue": "Status must be In Progress, Completed, or Cancelled."})

                                return validation_issues

                            issues = _validate_activity_141(edited_validation_df, active_item_options, status_options)

                            if issues:
                                st.error(f"Validation failed with {len(issues)} issue(s). Fix them before running the script.")
                                st.dataframe(pd.DataFrame(issues), use_container_width=True, hide_index=True)
                            else:
                                st.success("All rows are valid for Activity 1.4.1. You can run the script.")

                                try:
                                    from io import BytesIO
                                    from openpyxl import Workbook
                                    from openpyxl.worksheet.datavalidation import DataValidation
                                    from openpyxl.styles import PatternFill
                                    from openpyxl.formatting.rule import CellIsRule, FormulaRule

                                    export_df = edited_validation_df.copy()
                                    export_df["Date"] = pd.to_datetime(export_df["Date"], errors="coerce").dt.strftime("%m/%d/%Y")

                                    excel_buffer = BytesIO()
                                    with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
                                        export_df.to_excel(writer, sheet_name="Activity_1_4_1", index=False)
                                    excel_buffer.seek(0)

                                    st.download_button(
                                        "Export validated table to Excel (.xlsx)",
                                        data=excel_buffer.getvalue(),
                                        file_name="activity_1_4_1_validated.xlsx",
                                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                        icon=":material/download:"
                                    )

                                    validation_export_df = edited_validation_df.copy()
                                    validation_export_df["Date"] = pd.to_datetime(validation_export_df["Date"], errors="coerce").dt.date

                                    wb = Workbook()
                                    ws = wb.active
                                    ws.title = "Validation_List"

                                    headers = ["Item Name", "Quantity", "Unit Price", "Date", "Status"]
                                    ws.append(headers)
                                    for _, row in validation_export_df[headers].iterrows():
                                        ws.append([row["Item Name"], int(float(row["Quantity"])), float(row["Unit Price"]), row["Date"], row["Status"]])

                                    ws_custom = wb.create_sheet("Validation_Custom")
                                    ws_custom.append(headers)
                                    for _, row in validation_export_df[headers].iterrows():
                                        ws_custom.append([row["Item Name"], int(float(row["Quantity"])), float(row["Unit Price"]), row["Date"], row["Status"]])

                                    lists_ws = wb.create_sheet("_lists")
                                    for idx, item_name in enumerate(active_item_options, start=1):
                                        lists_ws.cell(row=idx, column=1, value=item_name)
                                    for idx, status_name in enumerate(status_options, start=1):
                                        lists_ws.cell(row=idx, column=2, value=status_name)
                                    lists_ws.sheet_state = "hidden"

                                    item_list_formula = f'"{",".join(active_item_options)}"'
                                    status_list_formula = f'"{",".join(status_options)}"'

                                    def apply_standard_validations(sheet):
                                        item_list_dv = DataValidation(type="list", formula1=item_list_formula, allow_blank=False)
                                        item_list_dv.errorTitle = "Invalid Item Name"
                                        item_list_dv.error = "Select an item from the drop-down list."

                                        quantity_dv = DataValidation(type="whole", operator="greaterThan", formula1="0", allow_blank=False)
                                        quantity_dv.errorTitle = "Invalid Quantity"
                                        quantity_dv.error = "Quantity must be a positive whole number."

                                        unit_price_dv = DataValidation(type="decimal", operator="between", formula1="0.01", formula2="1000", allow_blank=False)
                                        unit_price_dv.errorTitle = "Invalid Unit Price"
                                        unit_price_dv.error = "Unit Price must be between 0.01 and 1000.00."

                                        date_dv = DataValidation(type="date", operator="between", formula1="DATE(1900,1,1)", formula2="DATE(2099,12,31)", allow_blank=False)
                                        date_dv.errorTitle = "Invalid Date"
                                        date_dv.error = "Enter a valid date in mm/dd/yyyy format."

                                        status_dv = DataValidation(type="list", formula1=status_list_formula, allow_blank=False)
                                        status_dv.errorTitle = "Invalid Status"
                                        status_dv.error = "Status must be In Progress, Completed, or Cancelled."

                                        sheet.add_data_validation(item_list_dv)
                                        sheet.add_data_validation(quantity_dv)
                                        sheet.add_data_validation(unit_price_dv)
                                        sheet.add_data_validation(date_dv)
                                        sheet.add_data_validation(status_dv)

                                        item_list_dv.add("A2:A1048576")
                                        quantity_dv.add("B2:B1048576")
                                        unit_price_dv.add("C2:C1048576")
                                        date_dv.add("D2:D1048576")
                                        status_dv.add("E2:E1048576")

                                    apply_standard_validations(ws)

                                    custom_item_dv = DataValidation(
                                        type="custom",
                                        formula1="=AND(LEN(A2)>=3,LEN(A2)<=10,COUNTIF(_lists!$A$1:$A$5,A2)=1)",
                                        allow_blank=False
                                    )
                                    custom_item_dv.errorTitle = "Invalid Item Name"
                                    custom_item_dv.error = "Item Name must be in the list and 3-10 characters long."

                                    quantity_custom_dv = DataValidation(type="whole", operator="greaterThan", formula1="0", allow_blank=False)
                                    unit_price_custom_dv = DataValidation(type="decimal", operator="between", formula1="0.01", formula2="1000", allow_blank=False)
                                    date_custom_dv = DataValidation(type="date", operator="between", formula1="DATE(1900,1,1)", formula2="DATE(2099,12,31)", allow_blank=False)
                                    status_custom_dv = DataValidation(type="list", formula1=status_list_formula, allow_blank=False)

                                    ws_custom.add_data_validation(custom_item_dv)
                                    ws_custom.add_data_validation(quantity_custom_dv)
                                    ws_custom.add_data_validation(unit_price_custom_dv)
                                    ws_custom.add_data_validation(date_custom_dv)
                                    ws_custom.add_data_validation(status_custom_dv)

                                    custom_item_dv.add("A2:A1048576")
                                    quantity_custom_dv.add("B2:B1048576")
                                    unit_price_custom_dv.add("C2:C1048576")
                                    date_custom_dv.add("D2:D1048576")
                                    status_custom_dv.add("E2:E1048576")

                                    for sheet in [ws, ws_custom]:
                                        for row_idx in range(2, sheet.max_row + 1):
                                            sheet[f"D{row_idx}"].number_format = "mm/dd/yyyy"
                                            sheet[f"C{row_idx}"].number_format = "0.00"

                                        red_fill = PatternFill(start_color="F8D7DA", end_color="F8D7DA", fill_type="solid")
                                        yellow_fill = PatternFill(start_color="FFF3CD", end_color="FFF3CD", fill_type="solid")
                                        green_fill = PatternFill(start_color="D1E7DD", end_color="D1E7DD", fill_type="solid")
                                        orange_fill = PatternFill(start_color="FFE5B4", end_color="FFE5B4", fill_type="solid")
                                        blue_fill = PatternFill(start_color="DDEBFF", end_color="DDEBFF", fill_type="solid")

                                        max_row = max(sheet.max_row, 2)
                                        status_range = f"E2:E{max_row}"
                                        qty_range = f"B2:B{max_row}"
                                        unit_price_range = f"C2:C{max_row}"
                                        date_range = f"D2:D{max_row}"

                                        sheet.conditional_formatting.add(
                                            status_range,
                                            FormulaRule(formula=['$E2="Cancelled"'], fill=red_fill)
                                        )
                                        sheet.conditional_formatting.add(
                                            status_range,
                                            FormulaRule(formula=['$E2="In Progress"'], fill=yellow_fill)
                                        )
                                        sheet.conditional_formatting.add(
                                            status_range,
                                            FormulaRule(formula=['$E2="Completed"'], fill=green_fill)
                                        )

                                        sheet.conditional_formatting.add(
                                            qty_range,
                                            CellIsRule(operator="lessThanOrEqual", formula=["2"], fill=orange_fill)
                                        )
                                        sheet.conditional_formatting.add(
                                            unit_price_range,
                                            CellIsRule(operator="greaterThan", formula=["500"], fill=blue_fill)
                                        )
                                        sheet.conditional_formatting.add(
                                            date_range,
                                            FormulaRule(formula=["$D2<TODAY()-30"], fill=yellow_fill)
                                        )

                                    validation_buffer = BytesIO()
                                    wb.save(validation_buffer)
                                    validation_buffer.seek(0)

                                    st.download_button(
                                        "Export Excel with validation rules (.xlsx)",
                                        data=validation_buffer.getvalue(),
                                        file_name="activity_1_4_1_validation_rules.xlsx",
                                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                        icon=":material/checklist:"
                                    )
                                    st.caption("Workbook includes two sheets: Validation_List (dropdown-based) and Validation_Custom (custom Item Name length + list rule).")
                                except Exception as export_error:
                                    st.warning(f"Excel export is unavailable: {export_error}")

                            default_validation_script = """# df contains your validated Activity 1.4.1 table
df = df.copy()
df["Date"] = pd.to_datetime(df["Date"])
df["Line Total"] = df["Quantity"] * df["Unit Price"]

summary = df.groupby("Status", as_index=False)["Line Total"].sum().sort_values("Line Total", ascending=False)
print("Rows:", len(df))
print("Total value:", round(df["Line Total"].sum(), 2))

# Optionally expose a dataframe preview in the UI:
result_df = summary
"""

                            script_text = st.text_area(
                                "Activity script",
                                value=st.session_state.get(script_key, default_validation_script),
                                height=200,
                                key=script_key
                            )

                            if st.button("Run activity script", type="primary", icon=":material/play_arrow:", key=run_key):
                                if issues:
                                    st.warning("Script not executed. Resolve validation issues first.")
                                else:
                                    import io
                                    import contextlib

                                    runtime_scope = {
                                        "pd": pd,
                                        "df": edited_validation_df.copy()
                                    }
                                    output_buffer = io.StringIO()

                                    try:
                                        with contextlib.redirect_stdout(output_buffer):
                                            exec(script_text, {"__builtins__": __builtins__}, runtime_scope)

                                        execution_output = output_buffer.getvalue().strip()
                                        if execution_output:
                                            st.markdown("**Script output**")
                                            st.code(execution_output, language="text")

                                        result_df = runtime_scope.get("result_df")
                                        if isinstance(result_df, pd.DataFrame):
                                            st.markdown("**`result_df` preview**")
                                            st.dataframe(result_df, use_container_width=True)
                                        elif not execution_output:
                                            st.info("Script ran successfully. No printed output or `result_df` returned.")
                                    except Exception as run_error:
                                        st.error(f"Script error: {run_error}")

                    st.markdown("---")
                    with st.expander("Lesson PDF Viewer (PDF.js)", expanded=False):
                        st.caption("Upload a PDF tied to this lesson and view it directly here.")

                        lesson_upload_key = f"lesson_pdf_upload_{course_code}_{lesson['lesson_number']}"
                        lesson_page_key = f"lesson_pdf_page_{course_code}_{lesson['lesson_number']}"
                        lesson_zoom_key = f"lesson_pdf_zoom_{course_code}_{lesson['lesson_number']}"
                        lesson_sample_btn_key = f"lesson_pdf_sample_btn_{course_code}_{lesson['lesson_number']}"
                        lesson_sample_state_key = f"lesson_pdf_sample_bytes_{course_code}_{lesson['lesson_number']}"
                        lesson_sample_name_key = f"lesson_pdf_sample_name_{course_code}_{lesson['lesson_number']}"

                        sample_pdf_path = os.path.join(os.path.dirname(__file__), "SPF-0103 Lesson Task solution.pdf")
                        sample_pdf_available = os.path.exists(sample_pdf_path)

                        if sample_pdf_available:
                            if st.button("Open sample lesson PDF", key=lesson_sample_btn_key):
                                try:
                                    with open(sample_pdf_path, "rb") as sample_file:
                                        st.session_state[lesson_sample_state_key] = sample_file.read()
                                        st.session_state[lesson_sample_name_key] = os.path.basename(sample_pdf_path)
                                    st.success("Sample PDF loaded.")
                                except Exception as exc:
                                    st.error(f"Could not load sample PDF: {exc}")
                            st.caption(f"Sample detected: {os.path.basename(sample_pdf_path)}")
                        else:
                            st.caption("No sample repository PDF found for auto-load.")

                        uploaded_lesson_pdf = st.file_uploader(
                            "Upload lesson PDF",
                            type=["pdf"],
                            key=lesson_upload_key
                        )

                        lesson_pdf_bytes = None
                        lesson_pdf_name = None

                        if uploaded_lesson_pdf is not None:
                            uploaded_bytes = uploaded_lesson_pdf.read()
                            if uploaded_bytes:
                                lesson_pdf_bytes = uploaded_bytes
                                lesson_pdf_name = uploaded_lesson_pdf.name
                            else:
                                st.warning("The uploaded PDF is empty.")
                        elif st.session_state.get(lesson_sample_state_key):
                            lesson_pdf_bytes = st.session_state[lesson_sample_state_key]
                            lesson_pdf_name = st.session_state.get(lesson_sample_name_key, "sample.pdf")

                        if lesson_pdf_bytes:
                            st.caption(f"Loaded: {lesson_pdf_name} ({len(lesson_pdf_bytes):,} bytes)")

                            ctrl_col1, ctrl_col2 = st.columns(2)
                            with ctrl_col1:
                                lesson_initial_page = st.number_input(
                                    "Start page",
                                    min_value=1,
                                    value=1,
                                    step=1,
                                    key=lesson_page_key
                                )
                            with ctrl_col2:
                                lesson_initial_zoom = st.slider(
                                    "Initial zoom (%)",
                                    min_value=50,
                                    max_value=250,
                                    value=100,
                                    step=10,
                                    key=lesson_zoom_key
                                )

                            lesson_pdf_base64 = base64.b64encode(lesson_pdf_bytes).decode("utf-8")
                            lesson_viewer_html = f"""
                            <!doctype html>
                            <html>
                            <head>
                              <meta charset=\"utf-8\" />
                              <style>
                                body {{ margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; }}
                                .toolbar {{ display: flex; gap: 8px; align-items: center; padding: 10px; border-bottom: 1px solid #e5e7eb; background: #f8fafc; }}
                                .toolbar button {{ border: 1px solid #cbd5e1; background: #fff; border-radius: 6px; padding: 6px 10px; cursor: pointer; }}
                                .meta {{ margin-left: auto; color: #334155; font-size: 13px; }}
                                #canvas-wrap {{ height: 560px; overflow: auto; background: #f1f5f9; display: flex; justify-content: center; align-items: flex-start; padding: 16px 0; }}
                                #pdf-canvas {{ border: 1px solid #cbd5e1; box-shadow: 0 2px 10px rgba(0,0,0,0.08); background: #fff; }}
                                #error {{ color: #b91c1c; padding: 10px; font-size: 13px; }}
                              </style>
                            </head>
                            <body>
                              <div class=\"toolbar\">
                                <button id=\"prev\">Prev</button>
                                <button id=\"next\">Next</button>
                                <button id=\"zoom-out\">-</button>
                                <button id=\"zoom-in\">+</button>
                                <button id=\"reset\">Reset</button>
                                <div class=\"meta\" id=\"meta\">Loading PDF...</div>
                              </div>
                              <div id=\"canvas-wrap\"><canvas id=\"pdf-canvas\"></canvas></div>
                              <div id=\"error\"></div>

                              <script type=\"module\">
                                import * as pdfjsLib from "https://cdn.jsdelivr.net/npm/pdfjs-dist@5.4.624/build/pdf.min.mjs";
                                pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdn.jsdelivr.net/npm/pdfjs-dist@5.4.624/build/pdf.worker.min.mjs";

                                const base64 = "{lesson_pdf_base64}";
                                const binary = atob(base64);
                                const bytes = new Uint8Array(binary.length);
                                for (let i = 0; i < binary.length; i++) bytes[i] = binary.charCodeAt(i);

                                const canvas = document.getElementById("pdf-canvas");
                                const ctx = canvas.getContext("2d");
                                const meta = document.getElementById("meta");
                                const error = document.getElementById("error");

                                let pdfDoc = null;
                                let currentPage = {int(lesson_initial_page)};
                                let scale = {float(lesson_initial_zoom) / 100.0};
                                const baseScale = scale;

                                async function renderPage(pageNum) {{
                                  if (!pdfDoc) return;
                                  const page = await pdfDoc.getPage(pageNum);
                                  const viewport = page.getViewport({{ scale }});
                                  canvas.width = viewport.width;
                                  canvas.height = viewport.height;
                                  await page.render({{ canvasContext: ctx, viewport }}).promise;
                                  meta.textContent = `Page ${{currentPage}} / ${{pdfDoc.numPages}} â€¢ Zoom ${{Math.round(scale * 100)}}%`;
                                }}

                                async function loadPdf() {{
                                  try {{
                                    const loadingTask = pdfjsLib.getDocument({{ data: bytes }});
                                    pdfDoc = await loadingTask.promise;
                                    currentPage = Math.max(1, Math.min(currentPage, pdfDoc.numPages));
                                    await renderPage(currentPage);
                                  }} catch (e) {{
                                    error.textContent = `Failed to load PDF: ${{e?.message || e}}`;
                                  }}
                                }}

                                document.getElementById("prev").addEventListener("click", async () => {{
                                  if (!pdfDoc || currentPage <= 1) return;
                                  currentPage -= 1;
                                  await renderPage(currentPage);
                                }});

                                document.getElementById("next").addEventListener("click", async () => {{
                                  if (!pdfDoc || currentPage >= pdfDoc.numPages) return;
                                  currentPage += 1;
                                  await renderPage(currentPage);
                                }});

                                document.getElementById("zoom-in").addEventListener("click", async () => {{
                                  scale = Math.min(4, scale + 0.1);
                                  await renderPage(currentPage);
                                }});

                                document.getElementById("zoom-out").addEventListener("click", async () => {{
                                  scale = Math.max(0.3, scale - 0.1);
                                  await renderPage(currentPage);
                                }});

                                document.getElementById("reset").addEventListener("click", async () => {{
                                  scale = baseScale;
                                  await renderPage(currentPage);
                                }});

                                loadPdf();
                              </script>
                            </body>
                            </html>
                            """

                            html(lesson_viewer_html, height=630, width=None, scrolling=False)
                        else:
                            st.caption("Upload a lesson PDF or click the sample button to preview it here with PDF.js.")
            
            st.markdown("---")

            with st.expander("CSV Table Viewer", expanded=False):
                st.caption("Upload a CSV file, apply quick filters, and view/download the result.")

                csv_upload = st.file_uploader(
                    "Upload CSV",
                    type=["csv"],
                    key=f"learn_csv_upload_{course_code}"
                )

                if csv_upload is not None:
                    delimiter_choice = st.selectbox(
                        "Delimiter",
                        options=[",", ";", "\t", "|"],
                        index=0,
                        format_func=lambda value: {",": "Comma ( , )", ";": "Semicolon ( ; )", "\t": "Tab", "|": "Pipe ( | )"}[value],
                        key=f"learn_csv_delimiter_{course_code}"
                    )

                    try:
                        csv_dataframe = pd.read_csv(csv_upload, sep=delimiter_choice)
                    except Exception as csv_error:
                        st.error(f"Could not read CSV file: {csv_error}")
                        csv_dataframe = None

                    if csv_dataframe is not None:
                        st.caption(
                            f"Rows: {len(csv_dataframe):,} | Columns: {len(csv_dataframe.columns)} | File: {csv_upload.name}"
                        )

                        reset_key = f"learn_csv_reset_filters_{course_code}"
                        if st.button("Reset all CSV filters", key=reset_key):
                            reset_keys = [
                                f"learn_csv_filter_column_{course_code}",
                                f"learn_csv_filter_text_{course_code}",
                                f"learn_csv_sort_column_{course_code}",
                                f"learn_csv_sort_order_{course_code}",
                                f"learn_csv_range_column_{course_code}",
                                f"learn_csv_range_min_{course_code}",
                                f"learn_csv_range_max_{course_code}",
                            ]
                            for session_key in reset_keys:
                                if session_key in st.session_state:
                                    del st.session_state[session_key]
                            st.rerun()

                        working_dataframe = csv_dataframe.copy()

                        if len(working_dataframe.columns) > 0 and len(working_dataframe) > 0:
                            filter_col1, filter_col2 = st.columns(2)
                            with filter_col1:
                                filter_column = st.selectbox(
                                    "Filter column",
                                    options=list(working_dataframe.columns),
                                    key=f"learn_csv_filter_column_{course_code}"
                                )
                            with filter_col2:
                                filter_text = st.text_input(
                                    "Contains text",
                                    value="",
                                    key=f"learn_csv_filter_text_{course_code}"
                                )

                            if filter_text.strip():
                                filter_mask = working_dataframe[filter_column].astype(str).str.contains(filter_text, case=False, na=False)
                                working_dataframe = working_dataframe[filter_mask]

                            numeric_columns = working_dataframe.select_dtypes(include=["number"]).columns.tolist()
                            if numeric_columns:
                                st.markdown("**Numeric range filter (optional)**")
                                range_col1, range_col2, range_col3 = st.columns([2, 1, 1])

                                with range_col1:
                                    range_column = st.selectbox(
                                        "Numeric column",
                                        options=numeric_columns,
                                        key=f"learn_csv_range_column_{course_code}"
                                    )

                                numeric_series = pd.to_numeric(working_dataframe[range_column], errors="coerce").dropna()
                                if not numeric_series.empty:
                                    range_min = float(numeric_series.min())
                                    range_max = float(numeric_series.max())

                                    with range_col2:
                                        range_start = st.number_input(
                                            "Min",
                                            value=range_min,
                                            key=f"learn_csv_range_min_{course_code}"
                                        )
                                    with range_col3:
                                        range_end = st.number_input(
                                            "Max",
                                            value=range_max,
                                            key=f"learn_csv_range_max_{course_code}"
                                        )

                                    if range_start > range_end:
                                        st.warning("Min is greater than Max. Swap values to apply range filter.")
                                    else:
                                        range_mask = pd.to_numeric(working_dataframe[range_column], errors="coerce").between(range_start, range_end, inclusive="both")
                                        working_dataframe = working_dataframe[range_mask.fillna(False)]
                                else:
                                    st.caption("Selected numeric column has no valid numeric values for range filtering.")

                            sort_col1, sort_col2 = st.columns(2)
                            with sort_col1:
                                sort_column = st.selectbox(
                                    "Sort by",
                                    options=["(none)"] + list(working_dataframe.columns),
                                    key=f"learn_csv_sort_column_{course_code}"
                                )
                            with sort_col2:
                                sort_order = st.selectbox(
                                    "Order",
                                    options=["Ascending", "Descending"],
                                    key=f"learn_csv_sort_order_{course_code}"
                                )

                            if sort_column != "(none)":
                                working_dataframe = working_dataframe.sort_values(
                                    by=sort_column,
                                    ascending=(sort_order == "Ascending")
                                )

                        st.dataframe(working_dataframe, use_container_width=True)

                        filtered_csv_bytes = working_dataframe.to_csv(index=False).encode("utf-8")
                        st.download_button(
                            "Download filtered CSV",
                            data=filtered_csv_bytes,
                            file_name=f"{csv_upload.name.rsplit('.', 1)[0]}_filtered.csv",
                            mime="text/csv",
                            icon=":material/download:"
                        )

                        st.markdown("---")
                        st.markdown("**Run Python script on this CSV**")
                        st.caption("`df` contains the currently filtered/sorted table. Optionally assign a DataFrame to `result_df` to display it.")

                        csv_script_key = f"learn_csv_script_{course_code}"
                        csv_run_key = f"learn_csv_run_script_{course_code}"

                        default_script = """# `df` is available (filtered/sorted CSV)
print(df.head())

# Example transformation:
# result_df = df.copy()
# result_df['new_col'] = result_df.iloc[:, 0]
"""

                        script_text = st.text_area(
                            "Python script",
                            value=st.session_state.get(csv_script_key, default_script),
                            height=180,
                            key=csv_script_key
                        )

                        snippet_title_key = f"learn_csv_snippet_title_{course_code}"
                        default_snippet_title = f"CSV Script - {csv_upload.name.rsplit('.', 1)[0]}"
                        snippet_title = st.text_input(
                            "Snippet title",
                            value=st.session_state.get(snippet_title_key, default_snippet_title),
                            key=snippet_title_key
                        )

                        action_col1, action_col2 = st.columns(2)
                        with action_col1:
                            save_snippet_key = f"learn_csv_save_script_{course_code}"
                            snippet_saved_key = f"learn_csv_script_saved_{course_code}"
                            if st.button("Save script as snippet", icon=":material/save:", key=save_snippet_key):
                                if not isinstance(st.session_state.get("code_snippets"), dict):
                                    st.session_state.code_snippets = {}

                                snippet_id = f"csv_script_{int(time.time())}"
                                st.session_state.code_snippets[snippet_id] = {
                                    "title": snippet_title.strip() or default_snippet_title,
                                    "code": script_text,
                                    "language": "python",
                                    "category": "Python",
                                    "description": f"Saved from CSV Table Viewer ({csv_upload.name}) in {course_code}"
                                }
                                st.session_state["code_library_search_prefill"] = snippet_title.strip() or default_snippet_title
                                st.session_state["code_library_category_prefill"] = "Python"
                                st.session_state["code_library_language_prefill"] = "python"
                                st.session_state[snippet_saved_key] = True
                                st.success("Script saved to Code Library.")

                        with action_col2:
                            if st.button("Run script", type="primary", icon=":material/play_arrow:", key=csv_run_key):
                                import io
                                import contextlib

                                runtime_scope = {
                                    "pd": pd,
                                    "df": working_dataframe.copy()
                                }
                                output_buffer = io.StringIO()

                                try:
                                    with contextlib.redirect_stdout(output_buffer):
                                        exec(script_text, {"__builtins__": __builtins__}, runtime_scope)

                                    script_output = output_buffer.getvalue().strip()
                                    if script_output:
                                        st.markdown("**Script output**")
                                        st.code(script_output, language="text")

                                    result_df = runtime_scope.get("result_df")
                                    if isinstance(result_df, pd.DataFrame):
                                        st.markdown("**`result_df` preview**")
                                        st.dataframe(result_df, use_container_width=True)
                                    elif not script_output:
                                        st.info("Script completed. No printed output or `result_df` was provided.")
                                except Exception as script_error:
                                    st.error(f"Script error: {script_error}")

                        if st.session_state.get(f"learn_csv_script_saved_{course_code}"):
                            open_library_key = f"learn_csv_open_library_{course_code}"
                            if st.button("Open in Code Library", icon=":material/open_in_new:", key=open_library_key):
                                st.session_state["code_library_search_prefill"] = snippet_title.strip() or default_snippet_title
                                st.session_state["code_library_category_prefill"] = "Python"
                                st.session_state["code_library_language_prefill"] = "python"
                                st.session_state["nav_target_page"] = "Code Library"
                                st.session_state["sidebar_page_select"] = "Code Library"
                                st.rerun()
                else:
                    st.caption("Upload a CSV file to start viewing it in-table.")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### Knowledge")
            st.markdown("*After this course, you will have knowledge of:*")
            for item in course['knowledge']:
                # Check if topic has training
                has_training = item in training_modules
                training_badge = " ðŸŽ“" if has_training else ""
                st.markdown(f"- {item}{training_badge}")
        
        with col2:
            st.markdown("### Skills")
            st.markdown("*After this course, you will be able to:*")
            for item in course['skills']:
                st.markdown(f"- {item}")
        
        st.markdown("---")
        st.markdown("### General Competence")
        st.markdown("*After this course, you will:*")
        for item in course['competence']:
            st.markdown(f"- {item}")
    
    with tab2:
        mui_subheader("help_center", "Practice Questions")
        
        question_type = st.selectbox(
            "Question type:",
            options=["General", "Knowledge-based", "Skills-based", "Case Study"],
            index=0
        )
        
        type_map = {"General": "general", "Knowledge-based": "knowledge", "Skills-based": "skills", "Case Study": "case_study"}
        
        if 'current_question' not in st.session_state:
            st.session_state.current_question = None
        if 'show_answer' not in st.session_state:
            st.session_state.show_answer = False
        if 'user_answer' not in st.session_state:
            st.session_state.user_answer = ""
        if 'feedback' not in st.session_state:
            st.session_state.feedback = None
        
        if st.button("Generate New Question", type="primary", icon=":material/auto_awesome:"):
            with st.spinner("Generating question..."):
                st.session_state.current_question = generate_practice_question(course, type_map[question_type])
                st.session_state.show_answer = False
                st.session_state.user_answer = ""
                st.session_state.feedback = None
        
        if st.session_state.current_question:
            st.markdown("---")
            
            if "ANSWER:" in st.session_state.current_question:
                parts = st.session_state.current_question.split("ANSWER:")
                question_text = parts[0].strip()
                answer_text = parts[1].strip() if len(parts) > 1 else ""
            else:
                question_text = st.session_state.current_question
                answer_text = "Answer not available"
            
            st.markdown("### Question:")
            st.markdown(f"**{question_text}**")
            
            user_answer = st.text_area(
                "Your answer:",
                value=st.session_state.user_answer,
                height=100,
                placeholder="Type your answer here..."
            )
            st.session_state.user_answer = user_answer
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("Check My Answer", icon=":material/task_alt:"):
                    if user_answer.strip():
                        with st.spinner("Evaluating..."):
                            st.session_state.feedback = evaluate_answer(question_text, answer_text, user_answer)
                    else:
                        st.warning("Please enter an answer first.")
            
            with col2:
                if st.button("Show Answer", icon=":material/visibility:"):
                    st.session_state.show_answer = True
            
            if st.session_state.feedback:
                st.markdown("### Feedback:")
                st.success(st.session_state.feedback)
            
            if st.session_state.show_answer:
                st.markdown("### Correct Answer:")
                st.info(answer_text)

elif page == "Study Notes":
    st.markdown("""
    <style>
    .word-toolbar {
        background: linear-gradient(180deg, #f3f3f3 0%, #e8e8e8 100%);
        border: 1px solid #d0d0d0;
        border-radius: 4px;
        padding: 8px 12px;
        margin-bottom: 10px;
    }
    .toolbar-group {
        display: inline-flex;
        gap: 4px;
        padding: 0 8px;
        border-right: 1px solid #ccc;
    }
    .doc-container {
        background: white;
        border: 1px solid #d0d0d0;
        border-radius: 4px;
        padding: 20px;
        min-height: 400px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .note-card {
        background: #fafafa;
        border-left: 4px solid #4A90D9;
        padding: 12px;
        margin: 8px 0;
        border-radius: 0 4px 4px 0;
    }
    .note-card.important { border-left-color: #FFD700; }
    .note-card.critical { border-left-color: #FF6B6B; }
    .sidebar-item {
        padding: 8px 12px;
        border-radius: 4px;
        cursor: pointer;
        margin: 2px 0;
    }
    .sidebar-item:hover { background: #e8f4fd; }
    .status-bar {
        background: #f0f0f0;
        padding: 4px 12px;
        font-size: 12px;
        color: #666;
        border-top: 1px solid #d0d0d0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    NOTE_CATEGORIES = {
        "lecture": {"icon": "menu_book", "label": "Lecture Notes", "color": "#4A90D9"},
        "exercise": {"icon": "edit", "label": "Exercise Notes", "color": "#50C878"},
        "exam": {"icon": "assignment", "label": "Exam Prep", "color": "#FF6B6B"},
        "tips": {"icon": "lightbulb", "label": "Tips & Tricks", "color": "#FFD700"},
        "summary": {"icon": "description", "label": "Summary", "color": "#9B59B6"}
    }
    
    IMPORTANCE_LEVELS = {
        "normal": {"icon": "circle", "label": "Normal", "color": "#888"},
        "important": {"icon": "star", "label": "Important", "color": "#FFD700"},
        "critical": {"icon": "local_fire_department", "label": "Exam Critical", "color": "#FF6B6B"}
    }
    
    def render_mui_icon(icon_name, size=20):
        """Render Material Icon as HTML"""
        return f'<span class="material-icons md-{size}" style="vertical-align: middle; font-size: {size}px;">{icon_name}</span>'
    
    QUICK_INSERTS = {
        "heading": "## ",
        "subheading": "### ",
        "bullet": "- ",
        "numbered": "1. ",
        "bold": "**text**",
        "italic": "*text*",
        "table": "| Col1 | Col2 | Col3 |\n|------|------|------|\n| | | |",
        "checklist": "- [ ] Task item",
        "quote": "> Quote text",
        "code": "```\ncode here\n```",
        "divider": "\n---\n",
        "link": "[text](url)"
    }
    
    NOTE_TEMPLATES = {
        "blank": {"name": "Blank Document", "icon": "description", "content": ""},
        "concept": {"name": "Concept Summary", "icon": "lightbulb", "content": """<h1 style="color: #1a1a1a; border-bottom: 3px solid #4A90D9; padding-bottom: 12px;">Konsept: [Konseptnavn]</h1>

<h2 style="color: #2c3e50; border-left: 4px solid #4A90D9; padding-left: 12px; margin-top: 30px;">ðŸ“– Definisjon</h2>
<blockquote style="border-left: 4px solid #4A90D9; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50;">Hva er det?</strong> [Klar og konsis definisjon av konseptet]</p>
<p><strong style="color: #2c3e50;">Hvorfor er det viktig?</strong> [Forklar relevansen og betydningen]</p>
</blockquote>

<h2 style="color: #2c3e50; border-left: 4px solid #28a745; padding-left: 12px; margin-top: 30px;">ðŸŽ¯ Hovedpoeng</h2>
<ul style="line-height: 1.8;">
<li><strong style="color: #2c3e50;">Poeng 1:</strong> [Detaljert forklaring av fÃ¸rste hovedpoeng]
    <ul>
        <li><em style="color: #6c757d;">Detalj:</em> [Ytterligere informasjon]</li>
    </ul>
</li>
<li><strong style="color: #2c3e50;">Poeng 2:</strong> [Detaljert forklaring av andre hovedpoeng]
    <ul>
        <li><em style="color: #6c757d;">Detalj:</em> [Ytterligere informasjon]</li>
    </ul>
</li>
<li><strong style="color: #2c3e50;">Poeng 3:</strong> [Detaljert forklaring av tredje hovedpoeng]
    <ul>
        <li><em style="color: #6c757d;">Detalj:</em> [Ytterligere informasjon]</li>
    </ul>
</li>
</ul>

<h2 style="color: #2c3e50; border-left: 4px solid #ffc107; padding-left: 12px; margin-top: 30px;">ðŸ’¡ Eksempler</h2>
<ol style="line-height: 1.8;">
<li><strong style="color: #2c3e50;">Eksempel 1:</strong> [Konkret eksempel]
    <ul>
        <li><em style="color: #6c757d;">Situasjon:</em> [NÃ¥r dette gjelder]</li>
        <li><em style="color: #6c757d;">Anvendelse:</em> [Hvordan det brukes]</li>
    </ul>
</li>
<li><strong style="color: #2c3e50;">Eksempel 2:</strong> [Konkret eksempel]
    <ul>
        <li><em style="color: #6c757d;">Situasjon:</em> [NÃ¥r dette gjelder]</li>
        <li><em style="color: #6c757d;">Anvendelse:</em> [Hvordan det brukes]</li>
    </ul>
</li>
<li><strong style="color: #2c3e50;">Eksempel 3:</strong> [Konkret eksempel]
    <ul>
        <li><em style="color: #6c757d;">Situasjon:</em> [NÃ¥r dette gjelder]</li>
        <li><em style="color: #6c757d;">Anvendelse:</em> [Hvordan det brukes]</li>
    </ul>
</li>
</ol>

<h2 style="color: #2c3e50; border-left: 4px solid #dc3545; padding-left: 12px; margin-top: 30px;">âš ï¸ Vanlige Feil</h2>
<ul style="line-height: 1.8;">
<li><strong style="color: #dc3545;">Feil 1:</strong> [Beskriv feilen]
    <ul>
        <li><em style="color: #6c757d;">Hvorfor skjer det:</em> [Ã…rsak]</li>
        <li><em style="color: #28a745;">Hvordan unngÃ¥:</em> [LÃ¸sning]</li>
    </ul>
</li>
<li><strong style="color: #dc3545;">Feil 2:</strong> [Beskriv feilen]
    <ul>
        <li><em style="color: #6c757d;">Hvorfor skjer det:</em> [Ã…rsak]</li>
        <li><em style="color: #28a745;">Hvordan unngÃ¥:</em> [LÃ¸sning]</li>
    </ul>
</li>
</ul>

<h2 style="color: #2c3e50; border-left: 4px solid #17a2b8; padding-left: 12px; margin-top: 30px;">ðŸ”— Relaterte Konsepter</h2>
<ul style="line-height: 1.8;">
<li><strong style="color: #2c3e50;">[Relatert konsept 1]:</strong> [Beskriv sammenhengen og hvordan de henger sammen]</li>
<li><strong style="color: #2c3e50;">[Relatert konsept 2]:</strong> [Beskriv sammenhengen og hvordan de henger sammen]</li>
<li><strong style="color: #2c3e50;">[Relatert konsept 3]:</strong> [Beskriv sammenhengen og hvordan de henger sammen]</li>
</ul>

<h2 style="color: #2c3e50; border-left: 4px solid #6f42c1; padding-left: 12px; margin-top: 30px;">ðŸ“Š Praktisk Anvendelse</h2>
<blockquote style="border-left: 4px solid #6f42c1; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50;">NÃ¥r bruker man dette konseptet?</strong></p>
<ul style="margin-top: 10px;">
<li style="color: #28a745;">âœ“ [Situasjon 1]</li>
<li style="color: #28a745;">âœ“ [Situasjon 2]</li>
<li style="color: #28a745;">âœ“ [Situasjon 3]</li>
</ul>
</blockquote>

<h2 style="color: #2c3e50; border-left: 4px solid #6c757d; padding-left: 12px; margin-top: 30px;">ðŸ“ Notater</h2>
<blockquote style="border-left: 4px solid #6c757d; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><em style="color: #6c757d;">[Ekstra notater, observasjoner, eller viktige pÃ¥minnelser om dette konseptet]</em></p>
</blockquote>

<h2>ðŸ”— Ressurser</h2>
<ul>
<li>[Lenke til relevant materiale]</li>
<li>[Lenke til relevant materiale]</li>
</ul>"""},
        "case_study": {"name": "Case Study", "icon": "bar_chart", "content": """<h1 style="color: #1a1a1a; border-bottom: 3px solid #4A90D9; padding-bottom: 12px;">Case Study: [Prosjekt/Klientnavn]</h1>

<h2 style="color: #2c3e50; border-left: 4px solid #4A90D9; padding-left: 12px; margin-top: 30px;">ðŸ“‹ Executive Summary</h2>
<blockquote style="border-left: 4px solid #4A90D9; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><em style="color: #6c757d;">Kort oversikt over case study, mÃ¥l og nÃ¸kkelresultater (2-3 setninger).</em></p>
</blockquote>

<h2 style="color: #2c3e50; border-left: 4px solid #17a2b8; padding-left: 12px; margin-top: 30px;">ðŸ¢ Bakgrunn & Kontekst</h2>
<ul style="line-height: 1.8;">
<li><strong style="color: #2c3e50;">Organisasjon:</strong> [Bedrift/avdeling]</li>
<li><strong style="color: #2c3e50;">Bransje:</strong> [Bransjesektor]</li>
<li><strong style="color: #2c3e50;">Kontekst:</strong> [Forretningssituasjon eller utfordring]</li>
<li><strong style="color: #2c3e50;">Tidsramme:</strong> [Prosjektvarighet eller tidsperiode]</li>
<li><strong style="color: #2c3e50;">Stakeholders:</strong> [Hvem er involvert?]</li>
</ul>

<h2>â“ Problemstilling</h2>
<p><strong>PrimÃ¦r Utfordring:</strong> [Hovedproblemet som mÃ¥ lÃ¸ses]</p>
<p><strong>PÃ¥virkning:</strong> [Hvorfor dette problemet betyr noe - forretningspÃ¥virkning, kostnader, etc.]</p>
<p><strong>Suksesskriterier:</strong> [Hvordan vi mÃ¥ler suksess]</p>
<p><strong>Begrensninger:</strong> [Hvilke begrensninger finnes?]</p>

<h2>ðŸ“Š Tilgjengelig Data</h2>
<ul>
<li><strong>Datakilde 1:</strong> [Beskrivelse, stÃ¸rrelse, kvalitet]</li>
<li><strong>Datakilde 2:</strong> [Beskrivelse, stÃ¸rrelse, kvalitet]</li>
<li><strong>Datakilde 3:</strong> [Beskrivelse, stÃ¸rrelse, kvalitet]</li>
</ul>
<p><strong>Datakvalitet:</strong> [Eventuelle begrensninger, manglende verdier, eller datakvalitetsproblemer]</p>

<h2>ðŸ” Analyse TilnÃ¦rming</h2>
<ol>
<li><strong>Fase 1 - [Fasnavn]:</strong> [Beskrivelse av analyse-steg]
    <ul>
        <li><em>Metode:</em> [Hvilken metode ble brukt?]</li>
        <li><em>Resultat:</em> [Hva ble funnet?]</li>
    </ul>
</li>
<li><strong>Fase 2 - [Fasnavn]:</strong> [Beskrivelse av analyse-steg]
    <ul>
        <li><em>Metode:</em> [Hvilken metode ble brukt?]</li>
        <li><em>Resultat:</em> [Hva ble funnet?]</li>
    </ul>
</li>
<li><strong>Fase 3 - [Fasnavn]:</strong> [Beskrivelse av analyse-steg]
    <ul>
        <li><em>Metode:</em> [Hvilken metode ble brukt?]</li>
        <li><em>Resultat:</em> [Hva ble funnet?]</li>
    </ul>
</li>
</ol>

<h2>ðŸ’Ž NÃ¸kkelfunn</h2>
<ul>
<li><strong>Funn 1:</strong> [NÃ¸kkelinnsikt med stÃ¸ttende data]
    <ul>
        <li><em>Bevis:</em> [Kvantitativ eller kvalitativ stÃ¸tte]</li>
    </ul>
</li>
<li><strong>Funn 2:</strong> [NÃ¸kkelinnsikt med stÃ¸ttende data]
    <ul>
        <li><em>Bevis:</em> [Kvantitativ eller kvalitativ stÃ¸tte]</li>
    </ul>
</li>
<li><strong>Funn 3:</strong> [NÃ¸kkelinnsikt med stÃ¸ttende data]
    <ul>
        <li><em>Bevis:</em> [Kvantitativ eller kvalitativ stÃ¸tte]</li>
    </ul>
</li>
</ul>
<p><strong>Kvantitative Resultater:</strong> [Hvis relevant, legg inn tall og mÃ¥l]</p>

<h2 style="color: #2c3e50; border-left: 4px solid #ffc107; padding-left: 12px; margin-top: 30px;">ðŸ’¼ Anbefalinger</h2>
<ol style="line-height: 1.8;">
<li><strong style="color: #2c3e50;">Anbefaling 1:</strong> [Handlingspunkt med forventet pÃ¥virkning]
    <ul>
        <li><em style="color: #6c757d;">Prioritet:</em> [HÃ¸y/Middels/Lav]</li>
        <li><em style="color: #6c757d;">Forventet Effekt:</em> [Hva forventer vi Ã¥ oppnÃ¥?]</li>
    </ul>
</li>
<li><strong style="color: #2c3e50;">Anbefaling 2:</strong> [Handlingspunkt med forventet pÃ¥virkning]
    <ul>
        <li><em style="color: #6c757d;">Prioritet:</em> [HÃ¸y/Middels/Lav]</li>
        <li><em style="color: #6c757d;">Forventet Effekt:</em> [Hva forventer vi Ã¥ oppnÃ¥?]</li>
    </ul>
</li>
<li><strong style="color: #2c3e50;">Anbefaling 3:</strong> [Handlingspunkt med forventet pÃ¥virkning]
    <ul>
        <li><em style="color: #6c757d;">Prioritet:</em> [HÃ¸y/Middels/Lav]</li>
        <li><em style="color: #6c757d;">Forventet Effekt:</em> [Hva forventer vi Ã¥ oppnÃ¥?]</li>
    </ul>
</li>
</ol>

<h2>ðŸ“š LÃ¦rdommer</h2>
<ul>
<li><strong>Hva Fungerte Bra:</strong> [Suksessfulle tilnÃ¦rminger eller teknikker]</li>
<li><strong>Hva Kunne Forbedres:</strong> [OmrÃ¥der for fremtidig forbedring]</li>
<li><strong>NÃ¸kkel LÃ¦ring:</strong> [HovedlÃ¦ring fra denne case study]</li>
</ul>

<h2>ðŸ”— Relaterte Ressurser</h2>
<ul>
<li>[Lenke eller referanse til relaterte materialer]</li>
<li>[Lenke eller referanse til relaterte materialer]</li>
</ul>"""},
        "formula": {"name": "Formula Sheet", "icon": "functions", "content": """<h1 style="color: #1a1a1a; border-bottom: 3px solid #4A90D9; padding-bottom: 12px;">Formel Referanse: [Emne/Fag]</h1>

<h2 style="color: #2c3e50; border-left: 4px solid #4A90D9; padding-left: 12px; margin-top: 30px;">ðŸ“ Grunnleggende Formler</h2>

<h3 style="color: #2c3e50; margin-top: 25px;">Formel 1: [Formelnavn]</h3>
<blockquote style="border-left: 4px solid #4A90D9; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50;">Formel:</strong> <code style="background: #ffffff; padding: 4px 8px; border-radius: 4px; color: #e83e8c; font-size: 1.1em; border: 1px solid #dee2e6;">[Skriv formelen her, f.eks. E = mcÂ²]</code></p>
<p><strong style="color: #2c3e50;">Beskrivelse:</strong> [Hva beregner eller representerer denne formelen?]</p>
<p><strong style="color: #2c3e50;">NÃ¥r Brukes Den:</strong> [Situasjoner hvor denne formelen gjelder]</p>
<p><strong style="color: #2c3e50;">Enheter:</strong> [Inngangs- og utgangsenheter]</p>
<p><strong style="color: #2c3e50;">Eksempel:</strong> [Kort eksempel pÃ¥ bruk]</p>
</blockquote>

<h3 style="color: #2c3e50; margin-top: 25px;">Formel 2: [Formelnavn]</h3>
<blockquote style="border-left: 4px solid #4A90D9; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50;">Formel:</strong> <code style="background: #ffffff; padding: 4px 8px; border-radius: 4px; color: #e83e8c; font-size: 1.1em; border: 1px solid #dee2e6;">[Skriv formelen her]</code></p>
<p><strong style="color: #2c3e50;">Beskrivelse:</strong> [Hva beregner eller representerer denne formelen?]</p>
<p><strong style="color: #2c3e50;">NÃ¥r Brukes Den:</strong> [Situasjoner hvor denne formelen gjelder]</p>
<p><strong style="color: #2c3e50;">Enheter:</strong> [Inngangs- og utgangsenheter]</p>
<p><strong style="color: #2c3e50;">Eksempel:</strong> [Kort eksempel pÃ¥ bruk]</p>
</blockquote>

<h3 style="color: #2c3e50; margin-top: 25px;">Formel 3: [Formelnavn]</h3>
<blockquote style="border-left: 4px solid #4A90D9; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50;">Formel:</strong> <code style="background: #ffffff; padding: 4px 8px; border-radius: 4px; color: #e83e8c; font-size: 1.1em; border: 1px solid #dee2e6;">[Skriv formelen her]</code></p>
<p><strong style="color: #2c3e50;">Beskrivelse:</strong> [Hva beregner eller representerer denne formelen?]</p>
<p><strong style="color: #2c3e50;">NÃ¥r Brukes Den:</strong> [Situasjoner hvor denne formelen gjelder]</p>
<p><strong style="color: #2c3e50;">Enheter:</strong> [Inngangs- og utgangsenheter]</p>
<p><strong style="color: #2c3e50;">Eksempel:</strong> [Kort eksempel pÃ¥ bruk]</p>
</blockquote>

<h2>ðŸ”¤ NÃ¸kkelvariabler & Symboler</h2>
<ul>
<li><strong>Variabel 1 (symbol):</strong> [Definisjon og typisk omrÃ¥de/verdier]
    <ul>
        <li><em>Enhet:</em> [Enhet for variabelen]</li>
        <li><em>Typisk Verdi:</em> [Vanlig verdiomrÃ¥de]</li>
    </ul>
</li>
<li><strong>Variabel 2 (symbol):</strong> [Definisjon og typisk omrÃ¥de/verdier]
    <ul>
        <li><em>Enhet:</em> [Enhet for variabelen]</li>
        <li><em>Typisk Verdi:</em> [Vanlig verdiomrÃ¥de]</li>
    </ul>
</li>
<li><strong>Variabel 3 (symbol):</strong> [Definisjon og typisk omrÃ¥de/verdier]
    <ul>
        <li><em>Enhet:</em> [Enhet for variabelen]</li>
        <li><em>Typisk Verdi:</em> [Vanlig verdiomrÃ¥de]</li>
    </ul>
</li>
</ul>

<h2 style="color: #2c3e50; border-left: 4px solid #28a745; padding-left: 12px; margin-top: 30px;">ðŸ“ Arbeidet Eksempel</h2>
<blockquote style="border-left: 4px solid #28a745; background: #f0fff4; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50;">Problem:</strong> [Beskriv et eksempelproblem som skal lÃ¸ses]</p>
<p><strong style="color: #2c3e50;">Gitt:</strong></p>
<ul style="line-height: 1.8;">
<li>Variabel 1 = [verdi] [enhet]</li>
<li>Variabel 2 = [verdi] [enhet]</li>
<li>Variabel 3 = [verdi] [enhet]</li>
</ul>
</blockquote>
<p><strong style="color: #28a745;">LÃ¸sning:</strong></p>
<ol style="line-height: 1.8;">
<li><strong style="color: #2c3e50;">Steg 1:</strong> [FÃ¸rste beregning eller manipulasjon]
    <ul>
        <li><em style="color: #6c757d;">Beregning:</em> [Vis beregningen]</li>
    </ul>
</li>
<li><strong style="color: #2c3e50;">Steg 2:</strong> [Andre beregning eller manipulasjon]
    <ul>
        <li><em style="color: #6c757d;">Beregning:</em> [Vis beregningen]</li>
    </ul>
</li>
<li><strong style="color: #2c3e50;">Steg 3:</strong> [Siste beregning]
    <ul>
        <li><em style="color: #6c757d;">Beregning:</em> [Vis beregningen]</li>
    </ul>
</li>
</ol>
<blockquote style="border-left: 4px solid #28a745; background: #f0fff4; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #28a745; font-size: 1.1em;">Svar:</strong> [Sluttresultat med enheter]</p>
</blockquote>

<h2 style="color: #2c3e50; border-left: 4px solid #ffc107; padding-left: 12px; margin-top: 30px;">ðŸ’¡ Rask Referanse</h2>
<blockquote style="border-left: 4px solid #ffc107; background: #fffbf0; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<ul style="line-height: 1.8;">
<li><strong style="color: #ffc107;">Husk:</strong> [Viktig tips eller huskeregel]</li>
<li><strong style="color: #dc3545;">Vanlig Feil:</strong> [Hva man skal unngÃ¥ nÃ¥r man bruker disse formlene]</li>
<li><strong style="color: #17a2b8;">Pro Tips:</strong> [Hjelpsomt hint for anvendelse]</li>
<li><strong style="color: #2c3e50;">NÃ¥r Skal Man Bruke:</strong> [Situasjoner hvor disse formlene er mest relevante]</li>
</ul>
</blockquote>

<h2>ðŸ”— Relaterte Formler</h2>
<ul>
<li><strong>[Relatert formel eller konsept]:</strong> [Beskriv sammenhengen]</li>
<li><strong>[Relatert formel eller konsept]:</strong> [Beskriv sammenhengen]</li>
</ul>"""},
        "comparison": {"name": "Comparison Chart", "icon": "compare_arrows", "content": """<h1 style="color: #1a1a1a; border-bottom: 3px solid #4A90D9; padding-bottom: 12px;">Sammenligning: [Emne A] vs [Emne B]</h1>

<h2 style="color: #2c3e50; border-left: 4px solid #4A90D9; padding-left: 12px; margin-top: 30px;">ðŸ“Š Oversikt</h2>
<blockquote style="border-left: 4px solid #4A90D9; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><em style="color: #6c757d;">Kort introduksjon til hva som sammenlignes og hvorfor denne sammenligningen er viktig.</em></p>
</blockquote>

<h2 style="color: #2c3e50; border-left: 4px solid #007bff; padding-left: 12px; margin-top: 30px;">ðŸ”µ [Emne A]</h2>
<blockquote style="border-left: 4px solid #007bff; background: #f0f7ff; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50;">Definisjon:</strong> [Tydelig definisjon av emne A]</p>
<p><strong style="color: #2c3e50;">PrimÃ¦r BruksomrÃ¥de:</strong> [NÃ¥r og hvorfor bruker man emne A?]</p>
</blockquote>
<p><strong style="color: #28a745;">Fordeler:</strong></p>
<ul style="line-height: 1.8;">
<li style="color: #28a745;">âœ… <strong>Fordel 1:</strong> [Forklaring og hvorfor det er en fordel]</li>
<li style="color: #28a745;">âœ… <strong>Fordel 2:</strong> [Forklaring og hvorfor det er en fordel]</li>
<li style="color: #28a745;">âœ… <strong>Fordel 3:</strong> [Forklaring og hvorfor det er en fordel]</li>
</ul>
<p><strong style="color: #dc3545;">Ulemper:</strong></p>
<ul style="line-height: 1.8;">
<li style="color: #dc3545;">âŒ <strong>Ulempe 1:</strong> [Forklaring og konsekvenser]</li>
<li style="color: #dc3545;">âŒ <strong>Ulempe 2:</strong> [Forklaring og konsekvenser]</li>
</ul>
<p><strong>Best For:</strong> [Spesifikke scenarioer eller bruksomrÃ¥der]</p>
<p><strong>Eksempel:</strong> [Virkelig verden eksempel eller anvendelse]</p>
<p><strong>Kostnad/Kompleksitet:</strong> [Vurdering av kostnad eller kompleksitet]</p>

<h2 style="color: #2c3e50; border-left: 4px solid #28a745; padding-left: 12px; margin-top: 30px;">ðŸŸ¢ [Emne B]</h2>
<blockquote style="border-left: 4px solid #28a745; background: #f0fff4; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50;">Definisjon:</strong> [Tydelig definisjon av emne B]</p>
<p><strong style="color: #2c3e50;">PrimÃ¦r BruksomrÃ¥de:</strong> [NÃ¥r og hvorfor bruker man emne B?]</p>
</blockquote>
<p><strong style="color: #28a745;">Fordeler:</strong></p>
<ul style="line-height: 1.8;">
<li style="color: #28a745;">âœ… <strong>Fordel 1:</strong> [Forklaring og hvorfor det er en fordel]</li>
<li style="color: #28a745;">âœ… <strong>Fordel 2:</strong> [Forklaring og hvorfor det er en fordel]</li>
<li style="color: #28a745;">âœ… <strong>Fordel 3:</strong> [Forklaring og hvorfor det er en fordel]</li>
</ul>
<p><strong style="color: #dc3545;">Ulemper:</strong></p>
<ul style="line-height: 1.8;">
<li style="color: #dc3545;">âŒ <strong>Ulempe 1:</strong> [Forklaring og konsekvenser]</li>
<li style="color: #dc3545;">âŒ <strong>Ulempe 2:</strong> [Forklaring og konsekvenser]</li>
</ul>
<p><strong>Best For:</strong> [Spesifikke scenarioer eller bruksomrÃ¥der]</p>
<p><strong>Eksempel:</strong> [Virkelig verden eksempel eller anvendelse]</p>
<p><strong>Kostnad/Kompleksitet:</strong> [Vurdering av kostnad eller kompleksitet]</p>

<h2>ðŸŽ¯ Beslutningsmatrise</h2>
<p><strong>Velg [Emne A] nÃ¥r:</strong></p>
<ul>
<li>âœ“ <strong>Situasjon 1:</strong> [Beskrivelse av nÃ¥r emne A er best]</li>
<li>âœ“ <strong>Situasjon 2:</strong> [Beskrivelse av nÃ¥r emne A er best]</li>
<li>âœ“ <strong>Situasjon 3:</strong> [Beskrivelse av nÃ¥r emne A er best]</li>
</ul>
<p><strong>Velg [Emne B] nÃ¥r:</strong></p>
<ul>
<li>âœ“ <strong>Situasjon 1:</strong> [Beskrivelse av nÃ¥r emne B er best]</li>
<li>âœ“ <strong>Situasjon 2:</strong> [Beskrivelse av nÃ¥r emne B er best]</li>
<li>âœ“ <strong>Situasjon 3:</strong> [Beskrivelse av nÃ¥r emne B er best]</li>
</ul>

<h2 style="color: #2c3e50; border-left: 4px solid #ffc107; padding-left: 12px; margin-top: 30px;">ðŸ’¡ NÃ¸kkelinnsikt</h2>
<blockquote style="border-left: 4px solid #ffc107; background: #fffbf0; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<p><strong style="color: #2c3e50; font-size: 1.1em;">Hovedkonklusjon:</strong> [Den viktigste konklusjonen fra denne sammenligningen]</p>
<p><em style="color: #6c757d;">[Tilleggskontekst eller nyanser om nÃ¥r hvert alternativ er Ã¥ foretrekke]</em></p>
</blockquote>

<h2>ðŸ“ Notater</h2>
<p><em>Tilleggshensyn, edge cases, eller viktige pÃ¥minnelser om denne sammenligningen.</em></p>"""},
        "meeting": {"name": "Meeting Notes", "icon": "event_note", "content": """<h1 style="color: #1a1a1a; border-bottom: 3px solid #4A90D9; padding-bottom: 12px;">MÃ¸tenotater</h1>

<h2 style="color: #2c3e50; border-left: 4px solid #4A90D9; padding-left: 12px; margin-top: 30px;">ðŸ“… MÃ¸teinformasjon</h2>
<blockquote style="border-left: 4px solid #4A90D9; background: #f8f9fa; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<ul style="line-height: 1.8;">
<li><strong style="color: #2c3e50;">Dato:</strong> [Dato]</li>
<li><strong style="color: #2c3e50;">Tid:</strong> [Starttid] - [Sluttid]</li>
<li><strong style="color: #2c3e50;">Sted/Plattform:</strong> [Fysisk sted eller virtuell plattform]</li>
<li><strong style="color: #2c3e50;">MÃ¸tetype:</strong> [Regelmessig, Engangs, Prosjektgjennomgang, etc.]</li>
</ul>
</blockquote>

<h2>ðŸ‘¥ Deltakere</h2>
<ul>
<li><strong>Tilstede:</strong> [Navn 1], [Navn 2], [Navn 3]</li>
<li><strong>FravÃ¦rende:</strong> [Navn 1], [Navn 2]</li>
<li><strong>Fasilitator:</strong> [Navn]</li>
</ul>

<h2>ðŸŽ¯ MÃ¸temÃ¥l</h2>
<ol>
<li>[MÃ¥l 1]</li>
<li>[MÃ¥l 2]</li>
<li>[MÃ¥l 3]</li>
</ol>

<h2>ðŸ“‹ Agenda</h2>
<ol>
<li><strong>[Punkt 1]:</strong> [Beskrivelse]</li>
<li><strong>[Punkt 2]:</strong> [Beskrivelse]</li>
<li><strong>[Punkt 3]:</strong> [Beskrivelse]</li>
</ol>

<h2>ðŸ’¬ NÃ¸kkel Diskusjonspunkter</h2>
<ol>
<li><strong>Tema 1:</strong> [Sammendrag av diskusjonspunkt]
    <ul>
        <li>NÃ¸kkelpoeng nevnt</li>
        <li>Beslutning eller konklusjon nÃ¥dd</li>
    </ul>
</li>
<li><strong>Tema 2:</strong> [Sammendrag av diskusjonspunkt]
    <ul>
        <li>NÃ¸kkelpoeng nevnt</li>
        <li>Beslutning eller konklusjon nÃ¥dd</li>
    </ul>
</li>
<li><strong>Tema 3:</strong> [Sammendrag av diskusjonspunkt]
    <ul>
        <li>NÃ¸kkelpoeng nevnt</li>
        <li>Beslutning eller konklusjon nÃ¥dd</li>
    </ul>
</li>
</ol>

<h2 style="color: #2c3e50; border-left: 4px solid #28a745; padding-left: 12px; margin-top: 30px;">âœ… Beslutninger Tatt</h2>
<blockquote style="border-left: 4px solid #28a745; background: #f0fff4; padding: 15px 20px; margin: 15px 0; border-radius: 4px;">
<ul style="line-height: 1.8;">
<li><strong style="color: #28a745;">Beslutning 1:</strong> [Hva ble bestemt] - <em style="color: #6c757d;">Begrunnelse: [Hvorfor]</em></li>
<li><strong style="color: #28a745;">Beslutning 2:</strong> [Hva ble bestemt] - <em style="color: #6c757d;">Begrunnelse: [Hvorfor]</em></li>
<li><strong style="color: #28a745;">Beslutning 3:</strong> [Hva ble bestemt] - <em style="color: #6c757d;">Begrunnelse: [Hvorfor]</em></li>
</ul>
</blockquote>

<h2 style="color: #2c3e50; border-left: 4px solid #ffc107; padding-left: 12px; margin-top: 30px;">ðŸ“‹ Handlingspunkter</h2>
<ul style="line-height: 1.8;">
<li>â˜ <strong style="color: #2c3e50;">Oppgave 1:</strong> [Beskrivelse]
    <ul>
        <li><strong style="color: #6c757d;">Ansvarlig:</strong> [Navn]</li>
        <li><strong style="color: #6c757d;">Forfallsdato:</strong> [Dato]</li>
        <li><strong style="color: #6c757d;">Status:</strong> [Ikke Startet / PÃ¥gÃ¥r / FullfÃ¸rt]</li>
    </ul>
</li>
<li>â˜ <strong style="color: #2c3e50;">Oppgave 2:</strong> [Beskrivelse]
    <ul>
        <li><strong style="color: #6c757d;">Ansvarlig:</strong> [Navn]</li>
        <li><strong style="color: #6c757d;">Forfallsdato:</strong> [Dato]</li>
        <li><strong style="color: #6c757d;">Status:</strong> [Ikke Startet / PÃ¥gÃ¥r / FullfÃ¸rt]</li>
    </ul>
</li>
<li>â˜ <strong style="color: #2c3e50;">Oppgave 3:</strong> [Beskrivelse]
    <ul>
        <li><strong style="color: #6c757d;">Ansvarlig:</strong> [Navn]</li>
        <li><strong style="color: #6c757d;">Forfallsdato:</strong> [Dato]</li>
        <li><strong style="color: #6c757d;">Status:</strong> [Ikke Startet / PÃ¥gÃ¥r / FullfÃ¸rt]</li>
    </ul>
</li>
</ul>

<h2>ðŸš€ Neste Steg</h2>
<ol>
<li>[Umiddelbar neste handling]</li>
<li>[OppfÃ¸lgingshandling]</li>
<li>[Langtids handling]</li>
</ol>

<h2>ðŸ“… OppfÃ¸lging</h2>
<ul>
<li><strong>Neste MÃ¸te:</strong> [Dato og tid]</li>
<li><strong>Agendapunkter:</strong> [Temaer Ã¥ diskutere neste gang]</li>
</ul>

<h2>ðŸ“Ž Vedlegg & Ressurser</h2>
<ul>
<li>[Lenke til delt dokument eller ressurs]</li>
<li>[Lenke til delt dokument eller ressurs]</li>
</ul>"""},
        "exam_prep": {"name": "Exam Prep", "icon": "quiz", "content": """<h1>Eksamen Forberedelse: [Fag/Kursnavn]</h1>

<h2>ðŸ“š Eksamen Informasjon</h2>
<ul>
<li><strong>Fag:</strong> [Kurs/Fagnavn]</li>
<li><strong>Eksamen Dato:</strong> [Dato]</li>
<li><strong>Eksamen Tid:</strong> [Tidspunkt]</li>
<li><strong>Varighet:</strong> [Lengde pÃ¥ eksamen]</li>
<li><strong>Format:</strong> [Flervalg, Essay, Praktisk, etc.]</li>
<li><strong>Vekt:</strong> [Prosent av sluttkarakter]</li>
</ul>

<h2>âœ… NÃ¸kkelemner Ã¥ GjennomgÃ¥</h2>
<ul>
<li>â˜ <strong>Emne 1:</strong> [Kort beskrivelse] - <em>Prioritet: HÃ¸y/Middels/Lav</em>
    <ul>
        <li><em>Fokus:</em> [Hva spesifikt skal gjennomgÃ¥s?]</li>
    </ul>
</li>
<li>â˜ <strong>Emne 2:</strong> [Kort beskrivelse] - <em>Prioritet: HÃ¸y/Middels/Lav</em>
    <ul>
        <li><em>Fokus:</em> [Hva spesifikt skal gjennomgÃ¥s?]</li>
    </ul>
</li>
<li>â˜ <strong>Emne 3:</strong> [Kort beskrivelse] - <em>Prioritet: HÃ¸y/Middels/Lav</em>
    <ul>
        <li><em>Fokus:</em> [Hva spesifikt skal gjennomgÃ¥s?]</li>
    </ul>
</li>
<li>â˜ <strong>Emne 4:</strong> [Kort beskrivelse] - <em>Prioritet: HÃ¸y/Middels/Lav</em>
    <ul>
        <li><em>Fokus:</em> [Hva spesifikt skal gjennomgÃ¥s?]</li>
    </ul>
</li>
<li>â˜ <strong>Emne 5:</strong> [Kort beskrivelse] - <em>Prioritet: HÃ¸y/Middels/Lav</em>
    <ul>
        <li><em>Fokus:</em> [Hva spesifikt skal gjennomgÃ¥s?]</li>
    </ul>
</li>
</ul>

<h2>ðŸ“– Viktige Definisjoner</h2>
<ul>
<li><strong>Term 1:</strong> [Tydelig, konsis definisjon]
    <ul>
        <li><em>Eksempel:</em> [Eksempel pÃ¥ bruk eller kontekst]</li>
    </ul>
</li>
<li><strong>Term 2:</strong> [Tydelig, konsis definisjon]
    <ul>
        <li><em>Eksempel:</em> [Eksempel pÃ¥ bruk eller kontekst]</li>
    </ul>
</li>
<li><strong>Term 3:</strong> [Tydelig, konsis definisjon]
    <ul>
        <li><em>Eksempel:</em> [Eksempel pÃ¥ bruk eller kontekst]</li>
    </ul>
</li>
<li><strong>Term 4:</strong> [Tydelig, konsis definisjon]
    <ul>
        <li><em>Eksempel:</em> [Eksempel pÃ¥ bruk eller kontekst]</li>
    </ul>
</li>
</ul>

<h2>ðŸ§® Formler Ã¥ Pugge</h2>
<ul>
<li><strong>Formel 1:</strong> <code>[Formel]</code> - <em>BruksomrÃ¥de: [NÃ¥r brukes den?]</em></li>
<li><strong>Formel 2:</strong> <code>[Formel]</code> - <em>BruksomrÃ¥de: [NÃ¥r brukes den?]</em></li>
<li><strong>Formel 3:</strong> <code>[Formel]</code> - <em>BruksomrÃ¥de: [NÃ¥r brukes den?]</em></li>
</ul>

<h2>â“ Ã˜vingsspÃ¸rsmÃ¥l</h2>
<ol>
<li><strong>SpÃ¸rsmÃ¥l 1:</strong> [SpÃ¸rsmÃ¥lstekst]
    <ul>
        <li><em>Svar:</em> [Ditt svar]</li>
        <li><em>NÃ¸kkelpoeng:</em> [Viktige konsepter dette spÃ¸rsmÃ¥let tester]</li>
        <li><em>LÃ¦ring:</em> [Hva lÃ¦rte du fra dette spÃ¸rsmÃ¥let?]</li>
    </ul>
</li>
<li><strong>SpÃ¸rsmÃ¥l 2:</strong> [SpÃ¸rsmÃ¥lstekst]
    <ul>
        <li><em>Svar:</em> [Ditt svar]</li>
        <li><em>NÃ¸kkelpoeng:</em> [Viktige konsepter dette spÃ¸rsmÃ¥let tester]</li>
        <li><em>LÃ¦ring:</em> [Hva lÃ¦rte du fra dette spÃ¸rsmÃ¥let?]</li>
    </ul>
</li>
<li><strong>SpÃ¸rsmÃ¥l 3:</strong> [SpÃ¸rsmÃ¥lstekst]
    <ul>
        <li><em>Svar:</em> [Ditt svar]</li>
        <li><em>NÃ¸kkelpoeng:</em> [Viktige konsepter dette spÃ¸rsmÃ¥let tester]</li>
        <li><em>LÃ¦ring:</em> [Hva lÃ¦rte du fra dette spÃ¸rsmÃ¥let?]</li>
    </ul>
</li>
<li><strong>SpÃ¸rsmÃ¥l 4:</strong> [SpÃ¸rsmÃ¥lstekst]
    <ul>
        <li><em>Svar:</em> [Ditt svar]</li>
        <li><em>NÃ¸kkelpoeng:</em> [Viktige konsepter dette spÃ¸rsmÃ¥let tester]</li>
        <li><em>LÃ¦ring:</em> [Hva lÃ¦rte du fra dette spÃ¸rsmÃ¥let?]</li>
    </ul>
</li>
</ol>

<h2>ðŸ§  Rask Hukommelseshjelp</h2>
<ul>
<li><strong>Huskeregel 1:</strong> [Huskeregel eller akronym] - <em>For: [Hva det hjelper Ã¥ huske]</em></li>
<li><strong>Huskeregel 2:</strong> [Huskeregel eller akronym] - <em>For: [Hva det hjelper Ã¥ huske]</em></li>
<li><strong>Huskeregel 3:</strong> [Huskeregel eller akronym] - <em>For: [Hva det hjelper Ã¥ huske]</em></li>
</ul>
<blockquote>
<p><strong>Viktig PÃ¥minnelse:</strong> [Viktig konsept eller regel Ã¥ huske]</p>
</blockquote>

<h2>âš ï¸ Vanlige Eksamenfeil</h2>
<ul>
<li><strong>Feil 1:</strong> [Beskrivelse av vanlig feil]
    <ul>
        <li><em>Hvorfor skjer det:</em> [Ã…rsak]</li>
        <li><em>Hvordan unngÃ¥:</em> [Forebyggingsstrategi]</li>
    </ul>
</li>
<li><strong>Feil 2:</strong> [Beskrivelse av vanlig feil]
    <ul>
        <li><em>Hvorfor skjer det:</em> [Ã…rsak]</li>
        <li><em>Hvordan unngÃ¥:</em> [Forebyggingsstrategi]</li>
    </ul>
</li>
<li><strong>Feil 3:</strong> [Beskrivelse av vanlig feil]
    <ul>
        <li><em>Hvorfor skjer det:</em> [Ã…rsak]</li>
        <li><em>Hvordan unngÃ¥:</em> [Forebyggingsstrategi]</li>
    </ul>
</li>
</ul>

<h2>ðŸ“ Studieplan</h2>
<ul>
<li><strong>Uke 1:</strong> [Emner Ã¥ dekke]
    <ul>
        <li><em>Fokus:</em> [Hva skal fokuseres pÃ¥ denne uken?]</li>
    </ul>
</li>
<li><strong>Uke 2:</strong> [Emner Ã¥ dekke]
    <ul>
        <li><em>Fokus:</em> [Hva skal fokuseres pÃ¥ denne uken?]</li>
    </ul>
</li>
<li><strong>Uke 3:</strong> [Gjennomgang og praksis]
    <ul>
        <li><em>Fokus:</em> [Hva skal fokuseres pÃ¥ denne uken?]</li>
    </ul>
</li>
</ul>

<h2>âœ… Studie Sjekkliste</h2>
<ul>
<li>â˜ GjennomgÃ¥tt alle hovedtemaer</li>
<li>â˜ LÃ¸st Ã¸vingsoppgaver</li>
<li>â˜ GjennomgÃ¥tt formler og definisjoner</li>
<li>â˜ GjennomgÃ¥tt tidligere eksamener</li>
<li>â˜ FullfÃ¸rt selvtester</li>
<li>â˜ GjennomgÃ¥tt vanlige feil</li>
</ul>

<h2>ðŸ“Š Selvtest</h2>
<ul>
<li>â˜ [Selvtest 1: Beskriv hva du skal teste deg selv pÃ¥]</li>
<li>â˜ [Selvtest 2: Beskriv hva du skal teste deg selv pÃ¥]</li>
<li>â˜ [Selvtest 3: Beskriv hva du skal teste deg selv pÃ¥]</li>
</ul>

<h2>ðŸ”— Ressurser</h2>
<ul>
<li><strong>LÃ¦rebok:</strong> [Lenke til lÃ¦rebok kapittel eller seksjon]</li>
<li><strong>Ã˜vingsproblemer:</strong> [Lenke til praksisproblemer]</li>
<li><strong>Forelesningsnotater:</strong> [Lenke til forelesningsnotater eller slides]</li>
<li><strong>Videoer:</strong> [Lenke til relevante videoer]</li>
</ul>"""}
    }
    
    course_options = {f"{c['code']} - {c['name']}": c['code'] for c in courses_data}
    
    if 'current_note_content' not in st.session_state:
        st.session_state.current_note_content = ""
    if 'word_view_mode' not in st.session_state:
        st.session_state.word_view_mode = "edit"
    
    header_col1, header_col2 = st.columns([3, 1])
    with header_col1:
        st.markdown(f"## {render_mui_icon('note', 28)} Study Notes", unsafe_allow_html=True)
    with header_col2:
        view_mode = st.radio("View:", ["Edit", "Preview", "Split"], horizontal=True, key="word_view", label_visibility="collapsed")
    
    menu_col1, menu_col2, menu_col3, menu_col4, menu_col5 = st.columns([1, 1, 1, 1, 1])
    with menu_col1:
        st.markdown(render_mui_icon('note_add', 18), unsafe_allow_html=True)
        if st.button("New", key="word_new", icon=":material/note_add:", use_container_width=True, help="Create new note"):
            st.session_state.current_note_content = ""
            st.session_state.pop('editing_note_idx', None)
            st.session_state.pop('current_note_title', None)
            st.session_state.pop('current_note_category', None)
            st.session_state.pop('current_note_importance', None)
            st.session_state.pop('current_note_tags', None)
            st.session_state.pop('current_note_outcome', None)
            st.session_state.pop('last_applied_template', None)
            st.session_state.quill_key_counter = st.session_state.get('quill_key_counter', 0) + 1
            st.rerun()
    with menu_col2:
        st.markdown(render_mui_icon('save', 18), unsafe_allow_html=True)
        if st.button("Save", key="word_save_top", icon=":material/save:", use_container_width=True, help="Save note"):
            st.session_state.trigger_save = True
    with menu_col3:
        st.markdown(render_mui_icon('download', 18), unsafe_allow_html=True)
        if st.button("Export", key="word_export", icon=":material/download:", use_container_width=True, help="Export notes"):
            st.session_state.show_export = True
    with menu_col4:
        st.markdown(render_mui_icon('smart_toy', 18), unsafe_allow_html=True)
        if st.button("AI Help", key="word_ai", icon=":material/smart_toy:", use_container_width=True, help="AI Assistant"):
            st.session_state.show_ai_panel = not st.session_state.get('show_ai_panel', False)
    with menu_col5:
        st.markdown(render_mui_icon('analytics', 18), unsafe_allow_html=True)
        if st.button("Stats", key="word_stats", icon=":material/analytics:", use_container_width=True, help="Statistics"):
            st.session_state.show_stats = not st.session_state.get('show_stats', False)
    
    st.markdown("---")
    
    sidebar_col, main_col = st.columns([1, 3])
    
    with sidebar_col:
        st.markdown("##### ðŸ“ Documents")
        
        selected_course_label = st.selectbox(
            "Course:",
            options=list(course_options.keys()),
            key="word_course_select",
            label_visibility="collapsed"
        )
        selected_course_code = course_options[selected_course_label]
        
        if selected_course_code not in st.session_state.study_notes:
            st.session_state.study_notes[selected_course_code] = []
        
        course_notes = st.session_state.study_notes[selected_course_code]
        
        cat_filter = st.selectbox(
            "Filter:",
            ["All"] + [f"{v['icon']} {v['label']}" for v in NOTE_CATEGORIES.values()],
            key="word_cat_filter",
            label_visibility="collapsed"
        )
        
        search_term = st.text_input(render_mui_icon('search', 18), placeholder="Search...", key="word_search", label_visibility="collapsed")
        
        filtered_notes = course_notes.copy()
        if cat_filter != "All":
            # Extract category key from filter
            for k, v in NOTE_CATEGORIES.items():
                if v['label'] == cat_filter:
                    filtered_notes = [n for n in filtered_notes if n.get('category') == k]
                    break
        if search_term:
            filtered_notes = [n for n in filtered_notes if search_term.lower() in n.get('title', '').lower() or search_term.lower() in n.get('content', '').lower()]
        
        st.markdown(f"**{len(filtered_notes)} notes**")
        
        for idx, note in enumerate(filtered_notes):
            # Find actual index in course_notes by iterating to avoid duplicate index issues
            orig_idx = None
            for i, cn in enumerate(course_notes):
                if cn is note:  # Check by identity, not equality
                    orig_idx = i
                    break
            if orig_idx is None:
                orig_idx = idx
            
            cat = note.get('category', 'lecture')
            cat_info = NOTE_CATEGORIES.get(cat, NOTE_CATEGORIES['lecture'])
            imp = note.get('importance', 'normal')
            imp_info = IMPORTANCE_LEVELS.get(imp, IMPORTANCE_LEVELS['normal'])
            
            # Display icon and button separately
            icon_col, btn_col = st.columns([1, 9])
            with icon_col:
                st.markdown(f"{render_mui_icon(cat_info['icon'], 16)}{render_mui_icon(imp_info['icon'], 16)}", unsafe_allow_html=True)
            with btn_col:
                # Use idx from filtered list to ensure unique keys
                if st.button(note.get('title', 'Untitled')[:30], key=f"open_{selected_course_code}_{idx}_{orig_idx}", use_container_width=True):
                    st.session_state.current_note_content = note.get('content', '')
                    st.session_state.current_note_title = note.get('title', '')
                    st.session_state.current_note_category = note.get('category', 'lecture')
                    st.session_state.current_note_importance = note.get('importance', 'normal')
                    st.session_state.current_note_tags = ', '.join(note.get('tags', []))
                    st.session_state.current_note_outcome = note.get('learning_outcome', '')
                    st.session_state.editing_note_idx = orig_idx
                    st.session_state.quill_key_counter = st.session_state.get('quill_key_counter', 0) + 1
                    st.rerun()
    
    with main_col:
        # Note properties
        prop_col1, prop_col2, prop_col3, prop_col4 = st.columns(4)
        with prop_col1:
            note_title = st.text_input("Title:", value=st.session_state.get('current_note_title', ''), key="word_title", placeholder="Document title...")
        with prop_col2:
            note_category = st.selectbox(
                "Category:",
                options=list(NOTE_CATEGORIES.keys()),
                format_func=lambda x: NOTE_CATEGORIES[x]['label'],
                index=list(NOTE_CATEGORIES.keys()).index(st.session_state.get('current_note_category', 'lecture')),
                key="word_category"
            )
        with prop_col3:
            note_importance = st.selectbox(
                "Importance:",
                options=list(IMPORTANCE_LEVELS.keys()),
                format_func=lambda x: IMPORTANCE_LEVELS[x]['label'],
                index=list(IMPORTANCE_LEVELS.keys()).index(st.session_state.get('current_note_importance', 'normal')),
                key="word_importance"
            )
        with prop_col4:
            template_choice = st.selectbox(
                "Template:",
                options=["(None)"] + [t['name'] for t in NOTE_TEMPLATES.values()],
                key="word_template"
            )
            if template_choice != "(None)":
                for k, v in NOTE_TEMPLATES.items():
                    if v['name'] == template_choice:
                        template_key = k
                        if not st.session_state.get('editing_note_idx'):
                            if st.session_state.get('last_applied_template') != template_choice:
                                st.session_state.current_note_content = NOTE_TEMPLATES[template_key]['content']
                                st.session_state.last_applied_template = template_choice
                                st.session_state.quill_key_counter = st.session_state.get('quill_key_counter', 0) + 1
                                st.rerun()
                        break
        
        st.markdown("---")
        
        # Dynamic key to force Quill refresh when template changes
        quill_key = f"quill_editor_{st.session_state.get('quill_key_counter', 0)}"
        
        if view_mode == "Edit":
            from streamlit_quill import st_quill
            current_content = st.session_state.get('current_note_content', '')
            
            # Quill Rich Text Editor
            quill_content = st_quill(
                value=current_content,
                html=True,
                toolbar=[
                    [{'header': [1, 2, 3, 4, 5, 6, False]}],
                    ['bold', 'italic', 'underline', 'strike'],
                    [{'list': 'ordered'}, {'list': 'bullet'}],
                    [{'indent': '-1'}, {'indent': '+1'}],
                    [{'color': []}, {'background': []}],
                    [{'align': []}],
                    ['link', 'image'],
                    ['blockquote', 'code-block'],
                    ['clean']
                ],
                key=quill_key
            )
            
            if quill_content:
                st.session_state.current_note_content = quill_content
        
        elif view_mode == "Preview":
            st.markdown("**Preview:**")
            content = st.session_state.get('current_note_content', '')
            if content:
                # Render HTML content from Quill
                st.markdown(content, unsafe_allow_html=True)
            else:
                st.info("Nothing to preview. Start writing in Edit mode.")
        
        else:  # Split view
            from streamlit_quill import st_quill
            edit_col, preview_col = st.columns(2)
            with edit_col:
                current_content = st.session_state.get('current_note_content', '')
                quill_content_split = st_quill(
                    value=current_content,
                    html=True,
                    toolbar=[
                        [{'header': [1, 2, 3, False]}],
                        ['bold', 'italic', 'underline'],
                        [{'list': 'ordered'}, {'list': 'bullet'}],
                        ['link'],
                        ['clean']
                    ],
                    key=f"{quill_key}_split"
                )
                if quill_content_split:
                    st.session_state.current_note_content = quill_content_split
            with preview_col:
                st.markdown("**Preview:**")
                content = st.session_state.get('current_note_content', '')
                if content:
                    st.markdown(content, unsafe_allow_html=True)
                else:
                    st.caption("Preview appears here...")
        
        tags_col, outcome_col = st.columns(2)
        with tags_col:
            note_tags = st.text_input("Tags (comma-separated):", value=st.session_state.get('current_note_tags', ''), key="word_tags")
        with outcome_col:
            selected_course_data = next((c for c in courses_data if c['code'] == selected_course_code), None)
            outcomes_list = []
            if selected_course_data:
                for lo in selected_course_data.get('learning_outcomes', []):
                    outcomes_list.extend(lo.get('items', []))
            default_outcome = st.session_state.get('current_note_outcome', '')
            note_outcome = st.selectbox(
                "Learning Outcome:",
                options=["(None)"] + outcomes_list[:10],
                index=(outcomes_list.index(default_outcome) + 1) if default_outcome in outcomes_list else 0,
                key="word_outcome"
            )
        
        save_col1, save_col2, save_col3 = st.columns([1, 1, 2])
        with save_col1:
            st.markdown(render_mui_icon('save', 18), unsafe_allow_html=True)
            if st.button("Save Note", type="primary", icon=":material/save:", key="word_save_main", use_container_width=True) or st.session_state.get('trigger_save'):
                st.session_state.pop('trigger_save', None)
                if note_title and st.session_state.get('current_note_content', ''):
                    tags_list = [t.strip() for t in note_tags.split(',') if t.strip()] if note_tags else []
                    
                    note_data = {
                        'title': note_title,
                        'content': st.session_state.current_note_content,
                        'date': datetime.now().strftime("%Y-%m-%d %H:%M"),
                        'tags': tags_list,
                        'category': note_category,
                        'importance': note_importance,
                        'learning_outcome': note_outcome if note_outcome != "(None)" else "",
                        'version_history': []
                    }
                    
                    editing_idx = st.session_state.get('editing_note_idx')
                    if editing_idx is not None and editing_idx < len(course_notes):
                        old_note = course_notes[editing_idx]
                        history = old_note.get('version_history', [])
                        if old_note.get('content') != note_data['content']:
                            history.append({
                                'date': datetime.now().strftime("%Y-%m-%d %H:%M"),
                                'summary': f"Edited: {old_note.get('title', '')[:30]}"
                            })
                        note_data['version_history'] = history[-10:]
                        st.session_state.study_notes[selected_course_code][editing_idx] = note_data
                        st.success("Note updated!")
                    else:
                        st.session_state.study_notes[selected_course_code].append(note_data)
                        st.session_state.current_note_content = ""
                        st.session_state.pop('current_note_title', None)
                        st.session_state.pop('current_note_category', None)
                        st.session_state.pop('current_note_importance', None)
                        st.session_state.pop('current_note_tags', None)
                        st.session_state.pop('current_note_outcome', None)
                        st.session_state.pop('last_applied_template', None)
                        st.session_state.quill_key_counter = st.session_state.get('quill_key_counter', 0) + 1
                        st.success("New note created! Editor cleared for next note.")
                    
                    st.rerun()
                else:
                    st.warning("Please add a title and content.")
        
        with save_col2:
            if st.session_state.get('editing_note_idx') is not None:
                st.markdown(render_mui_icon('delete', 18), unsafe_allow_html=True)
                if st.button("Delete", key="word_delete", icon=":material/delete:", use_container_width=True):
                    idx = st.session_state.editing_note_idx
                    if idx < len(course_notes):
                        st.session_state.study_notes[selected_course_code].pop(idx)
                        st.session_state.current_note_content = ""
                        st.session_state.pop('editing_note_idx', None)
                        st.session_state.pop('current_note_title', None)
                        st.success("Deleted!")
                        st.rerun()
    
    if st.session_state.get('show_ai_panel'):
        st.markdown("---")
        st.markdown(f"### {render_mui_icon('smart_toy', 24)} AI Study Assistant", unsafe_allow_html=True)
        
        # Get comprehensive course context
        course_info = next((c for c in courses_data if c['code'] == selected_course_code), None)
        content = st.session_state.get('current_note_content', '')
        
        # Build rich context
        course_context_parts = []
        if course_info:
            course_context_parts.append(f"Course: {course_info['name']} ({course_info['code']})")
            course_context_parts.append(f"Description: {course_info.get('description', '')}")
            if course_info.get('knowledge'):
                course_context_parts.append(f"Knowledge Topics: {', '.join(course_info['knowledge'][:5])}")
            if course_info.get('skills'):
                course_context_parts.append(f"Skills: {', '.join(course_info['skills'][:5])}")
            if course_info.get('learning_outcomes'):
                outcomes_text = []
                for lo in course_info['learning_outcomes'][:3]:
                    outcomes_text.extend(lo.get('items', [])[:3])
                if outcomes_text:
                    course_context_parts.append(f"Learning Outcomes: {', '.join(outcomes_text[:5])}")
        
        course_context = "\n".join(course_context_parts)
        
        # Get related notes for context
        related_notes_context = ""
        if selected_course_code in st.session_state.study_notes:
            course_notes = st.session_state.study_notes[selected_course_code]
            if course_notes:
                related_titles = [n.get('title', '') for n in course_notes[:5]]
                if related_titles:
                    related_notes_context = f"\nRelated notes in this course: {', '.join(related_titles)}"
        
        full_context = course_context + related_notes_context
        
        ai_tabs = st.tabs(["Quick Actions", "Generate Content", "Analyze & Improve", "Custom Prompt"])
        
        with ai_tabs[0]:
            st.markdown("**Transform your notes:**")
            action_col1, action_col2, action_col3, action_col4 = st.columns(4)
            
            with action_col1:
                st.markdown("**Content**")
                if st.button("Summarize", key="ai_summarize", use_container_width=True):
                    st.session_state.ai_pending_action = "summarize"
                if st.button("Expand", key="ai_expand", use_container_width=True):
                    st.session_state.ai_pending_action = "expand"
                if st.button("Simplify", key="ai_simplify", use_container_width=True):
                    st.session_state.ai_pending_action = "simplify"
                if st.button("Fix Grammar", key="ai_grammar", use_container_width=True):
                    st.session_state.ai_pending_action = "grammar"
            
            with action_col2:
                st.markdown("**Enhance**")
                if st.button("Add Examples", key="ai_examples", use_container_width=True):
                    st.session_state.ai_pending_action = "examples"
                if st.button("Create Outline", key="ai_outline", use_container_width=True):
                    st.session_state.ai_pending_action = "outline"
                if st.button("Explain Concept", key="ai_explain", use_container_width=True):
                    st.session_state.ai_pending_action = "explain"
                if st.button("Add Context", key="ai_context", use_container_width=True):
                    st.session_state.ai_pending_action = "context"
            
            with action_col3:
                st.markdown("**Study Tools**")
                if st.button("Study Questions", key="ai_questions", use_container_width=True):
                    st.session_state.ai_pending_action = "questions"
                if st.button("Flashcards", key="ai_flashcards", use_container_width=True):
                    st.session_state.ai_pending_action = "flashcards"
                if st.button("Key Points", key="ai_keypoints", use_container_width=True):
                    st.session_state.ai_pending_action = "keypoints"
                if st.button("Exam Prep", key="ai_examprep", use_container_width=True):
                    st.session_state.ai_pending_action = "examprep"
            
            with action_col4:
                st.markdown("**ðŸŒ Language**")
                if st.button("Translate to NO", key="ai_translate_no", use_container_width=True):
                    st.session_state.ai_pending_action = "translate_no"
                if st.button("Translate to EN", key="ai_translate_en", use_container_width=True):
                    st.session_state.ai_pending_action = "translate_en"
                if st.button("Improve Style", key="ai_style", use_container_width=True):
                    st.session_state.ai_pending_action = "style"
                if st.button("Format Text", key="ai_format", use_container_width=True):
                    st.session_state.ai_pending_action = "format"
            
            # Process pending action
            if st.session_state.get('ai_pending_action') and content:
                action = st.session_state.ai_pending_action
                prompts = {
                    "summarize": f"Summarize this study note content concisely in bullet points. Focus on key concepts and main takeaways. {full_context}\n\nContent:\n{content}",
                    "expand": f"Expand on this content with more details, context, and practical applications relevant to data analysis. {full_context}\n\nContent:\n{content}",
                    "grammar": f"Fix any grammar, spelling, and clarity issues in this text. Return only the corrected text in the same format:\n\n{content}",
                    "simplify": f"Rewrite this content in simpler terms that a beginner could understand. Use analogies and real-world examples where helpful. {full_context}\n\nContent:\n{content}",
                    "examples": f"Add 3-5 practical, real-world data analysis examples to illustrate the concepts in this content. Make examples relevant to the course context. {full_context}\n\nContent:\n{content}",
                    "outline": f"Create a structured outline/table of contents from this content with main topics and subtopics. Format as HTML with proper headings:\n\n{content}",
                    "questions": f"Generate 5-7 study questions (mix of multiple choice, short answer, and critical thinking) based on this content. Include detailed answers. {full_context}\n\nContent:\n{content}",
                    "flashcards": f"Create 5-8 flashcards in the format 'Term | Definition' from the key concepts in this content. Make them suitable for memorization. {full_context}\n\nContent:\n{content}",
                    "explain": f"Explain the main concept(s) in this content as if teaching to someone new to data analysis. Include why it matters, how it connects to real-world applications, and relate it to the course context. {full_context}\n\nContent:\n{content}",
                    "context": f"Add relevant context, background information, and connections to course material. Link concepts to the course learning outcomes and knowledge/skills. {full_context}\n\nContent:\n{content}",
                    "keypoints": f"Extract and list the key points, main concepts, and important takeaways from this content. Format as a clear bullet list. {full_context}\n\nContent:\n{content}",
                    "examprep": f"Transform this content into exam preparation material. Include key definitions, important formulas, common exam questions, and things to remember. {full_context}\n\nContent:\n{content}",
                    "translate_no": f"Translate this content to Norwegian (bokmÃ¥l). Maintain the same structure and formatting. Keep technical terms in English if commonly used in Norwegian data analysis context:\n\n{content}",
                    "translate_en": f"Translate this content to English. Maintain the same structure and formatting:\n\n{content}",
                    "style": f"Improve the writing style, clarity, and flow of this content. Make it more engaging and easier to read while keeping all the information. {full_context}\n\nContent:\n{content}",
                    "format": f"Format this content properly with clear headings, bullet points, and structure. Improve readability while keeping all information. Return in HTML format:\n\n{content}"
                }
                
                with st.spinner(f"AI is processing..."):
                    try:
                        system_prompt = f"""You are an expert study assistant for a Data Analyst vocational program at Noroff. 
You help students learn data analysis concepts, tools, and techniques.

Context about the program:
- This is a vocational program focused on practical data analysis skills
- Students learn about business intelligence, statistics, data visualization, and decision-making
- The program emphasizes real-world applications and industry-relevant tools
- Students need clear, practical explanations with examples

Guidelines:
- Format responses in clean HTML (use <h2>, <h3>, <h4>, <p>, <ul>, <ol>, <li>, <strong>, <em>, <blockquote>, <code> tags)
- Always respond in English
- Focus on practical, actionable learning
- Connect concepts to real-world data analysis scenarios
- Be clear, concise, and educational
- When relevant, reference course learning outcomes and skills"""
                        
                        response = client.chat.completions.create(
                            model="gpt-4o-mini",
                            messages=[
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": prompts[action]}
                            ],
                            max_tokens=2000,
                            temperature=0.7
                        )
                        st.session_state.ai_result = response.choices[0].message.content
                        st.session_state.pop('ai_pending_action', None)
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
                        st.session_state.pop('ai_pending_action', None)
            elif st.session_state.get('ai_pending_action') and not content:
                st.warning("Write some content first.")
                st.session_state.pop('ai_pending_action', None)
        
        with ai_tabs[1]:
            st.markdown("**Generate new content for your notes:**")
            
            gen_topic = st.text_input("Topic or concept:", placeholder="e.g., VLOOKUP function, regression analysis, data cleaning, KPIs...", key="ai_gen_topic")
            gen_type = st.selectbox("Content type:", [
                "Concept Explanation",
                "Step-by-Step Guide", 
                "Key Points Summary",
                "Comparison Table",
                "Common Mistakes & Tips",
                "Practice Exercises",
                "Exam Review Notes",
                "Case Study Template",
                "Formula Reference",
                "Quick Reference Guide"
            ], key="ai_gen_type")
            
            include_course_context = st.checkbox("Include course context in generation", value=True, key="ai_include_course")
            
            if st.button("Generate", type="primary", icon=":material/auto_awesome:", key="ai_generate", use_container_width=True):
                if gen_topic:
                    context_to_use = full_context if include_course_context else ""
                    
                    type_prompts = {
                        "Concept Explanation": f"Explain the concept of '{gen_topic}' for a data analyst student. Include definition, importance, practical applications, and real-world examples. {context_to_use}",
                        "Step-by-Step Guide": f"Create a detailed step-by-step guide for '{gen_topic}'. Number each step clearly, include tips, warnings, and expected outcomes. {context_to_use}",
                        "Key Points Summary": f"Summarize the key points about '{gen_topic}' in bullet points. Focus on what a student needs to know for exams and practical work. {context_to_use}",
                        "Comparison Table": f"Create a comparison for '{gen_topic}' showing pros/cons, use cases, or comparing with similar concepts. Use HTML table format with clear headers. {context_to_use}",
                        "Common Mistakes & Tips": f"List common mistakes students make with '{gen_topic}' and how to avoid them. Include pro tips and best practices. {context_to_use}",
                        "Practice Exercises": f"Create 3 practice exercises about '{gen_topic}' with detailed solutions. Range from basic to intermediate difficulty. Include explanations. {context_to_use}",
                        "Exam Review Notes": f"Create exam-focused review notes for '{gen_topic}'. Include key definitions, formulas, important concepts, and things to remember. {context_to_use}",
                        "Case Study Template": f"Create a case study template for '{gen_topic}'. Include sections for background, problem, data, analysis, findings, and recommendations. {context_to_use}",
                        "Formula Reference": f"Create a formula reference sheet for '{gen_topic}'. Include formulas, variable definitions, use cases, and worked examples. {context_to_use}",
                        "Quick Reference Guide": f"Create a quick reference guide for '{gen_topic}'. Include essential information, shortcuts, key concepts, and common use cases in a concise format. {context_to_use}"
                    }
                    
                    with st.spinner("Generating content..."):
                        try:
                            system_prompt = f"""You are an expert study assistant for a Data Analyst vocational program at Noroff. 
Format your response in clean HTML (use <h2>, <h3>, <h4>, <p>, <ul>, <ol>, <li>, <table>, <tr>, <td>, <th>, <strong>, <em>, <blockquote>, <code> tags). 
Always respond in English.
Be practical and focused on real-world data analysis. {full_context if include_course_context else ''}"""
                            
                            response = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[
                                    {"role": "system", "content": system_prompt},
                                    {"role": "user", "content": type_prompts[gen_type]}
                                ],
                                max_tokens=2000,
                                temperature=0.7
                            )
                            st.session_state.ai_result = response.choices[0].message.content
                            st.rerun()
                        except Exception as e:
                            st.error(f"Error: {str(e)}")
                else:
                    st.warning("Enter a topic first.")
        
        with ai_tabs[2]:
            st.markdown("**Analyze and improve your notes:**")
            
            analyze_type = st.selectbox("Analysis type:", [
                "Check Completeness",
                "Identify Gaps",
                "Suggest Improvements",
                "Compare with Course Material",
                "Extract Key Concepts",
                "Find Related Topics",
                "Assess Exam Readiness"
            ], key="ai_analyze_type")
            
            if st.button("Analyze", type="primary", icon=":material/analytics:", key="ai_analyze", use_container_width=True):
                if content:
                    analyze_prompts = {
                        "Check Completeness": f"Analyze this study note and check if it's complete. Identify what might be missing (definitions, examples, applications, etc.). {full_context}\n\nContent:\n{content}",
                        "Identify Gaps": f"Identify knowledge gaps in this study note. What important concepts, details, or connections are missing? {full_context}\n\nContent:\n{content}",
                        "Suggest Improvements": f"Review this study note and suggest specific improvements for clarity, completeness, and learning effectiveness. {full_context}\n\nContent:\n{content}",
                        "Compare with Course Material": f"Compare this study note with the course material and learning outcomes. Does it cover the required topics? What should be added? {full_context}\n\nContent:\n{content}",
                        "Extract Key Concepts": f"Extract and list all key concepts, terms, and important information from this study note. Organize them clearly. {full_context}\n\nContent:\n{content}",
                        "Find Related Topics": f"Identify related topics, concepts, and connections that should be linked or referenced in this study note. {full_context}\n\nContent:\n{content}",
                        "Assess Exam Readiness": f"Assess if this study note is ready for exam preparation. What's missing? What should be emphasized? Provide specific recommendations. {full_context}\n\nContent:\n{content}"
                    }
                    
                    with st.spinner("Analyzing..."):
                        try:
                            system_prompt = f"""You are an expert study assistant for a Data Analyst vocational program. 
Provide detailed, constructive analysis. Format responses in clean HTML with clear sections. 
Always respond in English.
Be specific and actionable in your recommendations. {full_context}"""
                            
                            response = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[
                                    {"role": "system", "content": system_prompt},
                                    {"role": "user", "content": analyze_prompts[analyze_type]}
                                ],
                                max_tokens=2000,
                                temperature=0.7
                            )
                            st.session_state.ai_result = response.choices[0].message.content
                            st.rerun()
                        except Exception as e:
                            st.error(f"Error: {str(e)}")
                else:
                    st.warning("Write some content first to analyze.")
        
        with ai_tabs[3]:
            st.markdown("**Ask anything or give custom instructions:**")
            custom_prompt = st.text_area("Your prompt:", placeholder="e.g., 'Translate to Norwegian', 'Make this more formal', 'Add more statistics examples', 'Create a mind map structure'...", key="ai_custom_prompt", height=120)
            
            include_content = st.checkbox("Include current note content", value=True, key="ai_include_content")
            include_course_info = st.checkbox("Include course context", value=True, key="ai_include_course_info")
            
            if st.button("Send to AI", type="primary", icon=":material/send:", key="ai_custom_send", use_container_width=True):
                if custom_prompt:
                    full_prompt = custom_prompt
                    if include_content and content:
                        full_prompt = f"{custom_prompt}\n\nContent:\n{content}"
                    if include_course_info:
                        full_prompt = f"{full_prompt}\n\n{course_context}"
                    
                    with st.spinner("Processing..."):
                        try:
                            system_prompt = f"""You are a helpful study assistant for a Data Analyst program at Noroff. 
Format responses in HTML when appropriate. Always respond in English.
Be helpful, educational, and practical. 
Focus on data analysis concepts, tools, and real-world applications."""
                            
                            response = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[
                                    {"role": "system", "content": system_prompt},
                                    {"role": "user", "content": full_prompt}
                                ],
                                max_tokens=2000,
                                temperature=0.7
                            )
                            st.session_state.ai_result = response.choices[0].message.content
                            st.rerun()
                        except Exception as e:
                            st.error(f"Error: {str(e)}")
                else:
                    st.warning("Enter a prompt first.")
    
    # Display AI result - outside of AI panel so it's always visible
    if st.session_state.get('ai_result'):
        st.markdown("---")
        st.markdown(f"### {render_mui_icon('auto_awesome', 24)} AI Generated Result", unsafe_allow_html=True)
        
        # Get the result content
        ai_result_content = st.session_state.ai_result
        
        # Display the result with proper HTML rendering
        st.markdown(ai_result_content, unsafe_allow_html=True)
        
        st.markdown("---")
        st.markdown("**Actions:**")
        result_col1, result_col2, result_col3, result_col4 = st.columns(4)
        
        with result_col1:
            if st.button("Replace Content", key="ai_replace", use_container_width=True, type="primary", icon=":material/publish:"):
                st.session_state.current_note_content = ai_result_content
                st.session_state.quill_key_counter = st.session_state.get('quill_key_counter', 0) + 1
                st.session_state.pop('ai_result', None)
                st.rerun()
        
        with result_col2:
            if st.button("Append to Note", key="ai_append", icon=":material/note_add:", use_container_width=True):
                current = st.session_state.get('current_note_content', '')
                separator = "<hr>" if current else ""
                st.session_state.current_note_content = current + f"{separator}<h3>AI Generated Content</h3>{ai_result_content}"
                st.session_state.quill_key_counter = st.session_state.get('quill_key_counter', 0) + 1
                st.session_state.pop('ai_result', None)
                st.rerun()
        
        with result_col3:
            if st.button("View HTML", key="ai_view_html", icon=":material/code:", use_container_width=True):
                st.session_state.show_raw_html = not st.session_state.get('show_raw_html', False)
                st.rerun()
        
        with result_col4:
            if st.button("Dismiss", key="ai_dismiss", icon=":material/close:", use_container_width=True):
                st.session_state.pop('ai_result', None)
                st.session_state.pop('show_raw_html', None)
                st.rerun()
        
        # Show raw HTML if requested
        if st.session_state.get('show_raw_html'):
            st.markdown("---")
            st.markdown("**Raw HTML Code:**")
            st.code(ai_result_content, language='html')
            st.info("Copy this HTML code and paste it into your note editor if needed.")
    
    if st.session_state.get('show_stats'):
        st.markdown("---")
        st.markdown(f"### {render_mui_icon('analytics', 24)} Statistics", unsafe_allow_html=True)
        
        total_notes = sum(len(notes) for notes in st.session_state.study_notes.values())
        
        stat_cols = st.columns(4)
        with stat_cols[0]:
            st.metric("Total Notes", total_notes)
        with stat_cols[1]:
            critical_count = sum(1 for notes in st.session_state.study_notes.values() for n in notes if n.get('importance') == 'critical')
            st.metric("Critical", critical_count)
        with stat_cols[2]:
            important_count = sum(1 for notes in st.session_state.study_notes.values() for n in notes if n.get('importance') == 'important')
            st.metric("Important", important_count)
        with stat_cols[3]:
            courses_with_notes = sum(1 for notes in st.session_state.study_notes.values() if notes)
            st.metric("Courses", courses_with_notes)
        
        if total_notes > 0:
            cat_col, course_col = st.columns(2)
            with cat_col:
                st.markdown("**By Category:**")
                cat_counts = {}
                for notes in st.session_state.study_notes.values():
                    for note in notes:
                        cat = note.get('category', 'lecture')
                        cat_counts[cat] = cat_counts.get(cat, 0) + 1
                for cat, count in sorted(cat_counts.items(), key=lambda x: x[1], reverse=True):
                    cat_info = NOTE_CATEGORIES.get(cat, NOTE_CATEGORIES['lecture'])
                    st.markdown(f"{render_mui_icon(cat_info['icon'], 18)} {cat_info['label']}: {count}", unsafe_allow_html=True)
            
            with course_col:
                st.markdown("**By Course:**")
                course_counts = {code: len(notes) for code, notes in st.session_state.study_notes.items() if notes}
                for code, count in sorted(course_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                    st.write(f"{code}: {count}")
    
    if st.session_state.get('show_export'):
        st.markdown("---")
        st.markdown(f"### {render_mui_icon('download', 24)} Export Notes", unsafe_allow_html=True)
        
        if course_notes:
            export_format = st.radio("Format:", ["JSON", "Markdown"], horizontal=True, key="export_format")
            
            if export_format == "JSON":
                export_data = {selected_course_code: course_notes}
                export_json = json.dumps(export_data, indent=2, ensure_ascii=False)
                st.markdown(render_mui_icon('download', 18), unsafe_allow_html=True)
                st.download_button(
                    "Download JSON",
                    data=export_json,
                    file_name=f"notes_{selected_course_code}_{datetime.now().strftime('%Y%m%d')}.json",
                    mime="application/json",
                    icon=":material/download:"
                )
            else:
                md_content = f"# Study Notes - {selected_course_code}\n\n"
                for note in course_notes:
                    md_content += f"## {note.get('title', 'Untitled')}\n"
                    md_content += f"*{note.get('date', '')}* | {NOTE_CATEGORIES.get(note.get('category', 'lecture'), {}).get('label', '')}\n\n"
                    md_content += note.get('content', '') + "\n\n---\n\n"
                st.markdown(render_mui_icon('download', 18), unsafe_allow_html=True)
                st.download_button(
                    "Download Markdown",
                    data=md_content,
                    file_name=f"notes_{selected_course_code}_{datetime.now().strftime('%Y%m%d')}.md",
                    mime="text/markdown",
                    icon=":material/download:"
                )
        else:
            st.info("No notes to export.")
        
        st.session_state.show_export = False
    
    word_count = len(st.session_state.get('current_note_content', '').split())
    char_count = len(st.session_state.get('current_note_content', ''))
    st.markdown(f"{render_mui_icon('description', 16)} {word_count} words | {char_count} characters | Course: {selected_course_code}", unsafe_allow_html=True)
elif page == "Flashcards":
    mui_title("style", "Flashcards")
    st.markdown("*Learn with spaced repetition*")
    st.markdown("---")
    
    # Initialize flashcard counter if not exists
    if 'flashcard_counter' not in st.session_state:
        st.session_state.flashcard_counter = 0
    
    tab1, tab2, tab3 = st.tabs(["Study", "Create Cards", "Statistics"])
    
    with tab1:
        mui_subheader("school", "Study Mode")
        
        # Get cards that need review
        now = datetime.now()
        cards_to_review = []
        for card_id, card in st.session_state.flashcards.items():
            next_review = card.get('next_review')
            if next_review:
                try:
                    next_review_dt = datetime.fromisoformat(next_review)
                    if next_review_dt <= now:
                        cards_to_review.append((card_id, card))
                except:
                    cards_to_review.append((card_id, card))
            else:
                cards_to_review.append((card_id, card))
        
        if not cards_to_review:
            st.info("No cards need review right now. Create some cards or check back later.")
        else:
            # Filter by course
            course_filter = st.selectbox(
                "Filter by course:",
                ["All Courses"] + [c['code'] for c in courses_data]
            )
            
            if course_filter != "All Courses":
                cards_to_review = [(cid, c) for cid, c in cards_to_review if c.get('course') == course_filter]
            
            if cards_to_review:
                # Get current card
                if 'current_card_idx' not in st.session_state:
                    st.session_state.current_card_idx = 0
                
                current_idx = st.session_state.current_card_idx % len(cards_to_review)
                card_id, current_card = cards_to_review[current_idx]
                
                st.markdown(f"**Card {current_idx + 1} of {len(cards_to_review)}**")
                st.progress((current_idx + 1) / len(cards_to_review))
                
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("### Front")
                    st.info(current_card.get('front', 'No front text'))
                
                show_answer = st.session_state.get(f"show_answer_{card_id}", False)
                
                if show_answer:
                    with col2:
                        st.markdown("### Back")
                        st.success(current_card.get('back', 'No back text'))
                    
                    st.markdown("---")
                    st.markdown("### How well did you know this?")
                    
                    col_a, col_b, col_c, col_d = st.columns(4)
                    
                    def update_card_difficulty(difficulty):
                        # SM-2 algorithm parameters
                        card = st.session_state.flashcards[card_id]
                        ease_factor = card.get('ease_factor', 2.5)
                        interval = card.get('interval', 1)
                        repetitions = card.get('repetitions', 0)
                        
                        if difficulty == "Again":
                            interval = 1
                            repetitions = 0
                            ease_factor = max(1.3, ease_factor - 0.2)
                        elif difficulty == "Hard":
                            interval = max(1, interval * 1.2)
                            ease_factor = max(1.3, ease_factor - 0.15)
                        elif difficulty == "Good":
                            if repetitions == 0:
                                interval = 1
                            elif repetitions == 1:
                                interval = 6
                            else:
                                interval = int(interval * ease_factor)
                            repetitions += 1
                            ease_factor = ease_factor
                        elif difficulty == "Easy":
                            interval = int(interval * ease_factor * 1.3)
                            repetitions += 1
                            ease_factor = ease_factor + 0.15
                        
                        # Update card
                        next_review = datetime.now() + timedelta(days=interval)
                        st.session_state.flashcards[card_id].update({
                            'interval': interval,
                            'repetitions': repetitions,
                            'ease_factor': ease_factor,
                            'next_review': next_review.isoformat(),
                            'last_reviewed': datetime.now().isoformat()
                        })
                        
                        # Update stats
                        st.session_state.flashcard_stats['cards_reviewed'] = st.session_state.flashcard_stats.get('cards_reviewed', 0) + 1
                        if difficulty in ["Good", "Easy"]:
                            st.session_state.flashcard_stats['cards_mastered'] = st.session_state.flashcard_stats.get('cards_mastered', 0) + 1
                        
                        # Move to next card
                        st.session_state.current_card_idx += 1
                        del st.session_state[f"show_answer_{card_id}"]
                        st.rerun()
                    
                    with col_a:
                        st.button("Again", on_click=lambda: update_card_difficulty("Again"), use_container_width=True)
                    with col_b:
                        st.button("Hard", on_click=lambda: update_card_difficulty("Hard"), use_container_width=True)
                    with col_c:
                        st.button("Good", on_click=lambda: update_card_difficulty("Good"), use_container_width=True, type="primary", icon=":material/thumb_up:")
                    with col_d:
                        st.button("Easy", on_click=lambda: update_card_difficulty("Easy"), use_container_width=True)
                else:
                    if st.button("Show Answer", type="primary", icon=":material/visibility:", use_container_width=True):
                        st.session_state[f"show_answer_{card_id}"] = True
                        st.rerun()
            else:
                st.info("No cards to review for this course.")
    
    with tab2:
        mui_subheader("add_box", "Create New Flashcard")
        
        col1, col2 = st.columns(2)
        with col1:
            card_course = st.selectbox(
                "Course:",
                [c['code'] for c in courses_data]
            )
            card_front = st.text_area("Front:", height=150, placeholder="Question or term...")
        
        with col2:
            card_back = st.text_area("Back:", height=150, placeholder="Answer or definition...")
            card_tags = st.text_input("Tags (optional):", placeholder="comma-separated")
        
        if st.button("Create Card", type="primary", icon=":material/add_card:"):
            if card_front and card_back:
                card_id = f"card_{st.session_state.flashcard_counter}"
                st.session_state.flashcard_counter += 1
                
                st.session_state.flashcards[card_id] = {
                    'front': card_front,
                    'back': card_back,
                    'course': card_course,
                    'tags': [t.strip() for t in card_tags.split(',') if t.strip()] if card_tags else [],
                    'interval': 1,
                    'repetitions': 0,
                    'ease_factor': 2.5,
                    'next_review': datetime.now().isoformat(),
                    'created': datetime.now().isoformat()
                }
                
                st.session_state.flashcard_stats['total_cards'] = len(st.session_state.flashcards)
                st.success("Card created!")
                st.rerun()
            else:
                st.warning("Please fill in both front and back.")
        
        # Show existing cards
        st.markdown("---")
        mui_subheader("view_carousel", "Your Cards")
        
        if st.session_state.flashcards:
            course_filter_create = st.selectbox(
                "Filter by course:",
                ["All Courses"] + [c['code'] for c in courses_data],
                key="filter_create"
            )
            
            filtered_cards = st.session_state.flashcards
            if course_filter_create != "All Courses":
                filtered_cards = {k: v for k, v in filtered_cards.items() if v.get('course') == course_filter_create}
            
            for card_id, card in filtered_cards.items():
                with st.expander(f"Card: {card.get('front', 'No front')[:50]}..."):
                    col_a, col_b = st.columns(2)
                    with col_a:
                        st.markdown(f"**Front:** {card.get('front')}")
                    with col_b:
                        st.markdown(f"**Back:** {card.get('back')}")
                    st.caption(f"Course: {card.get('course')} | Next review: {card.get('next_review', 'Not set')}")
                    if st.button(f"Delete", key=f"delete_card_{card_id}"):
                        del st.session_state.flashcards[card_id]
                        st.session_state.flashcard_stats['total_cards'] = len(st.session_state.flashcards)
                        st.rerun()
        else:
            st.info("No cards created yet.")
    
    with tab3:
        mui_subheader("analytics", "Statistics")
        
        stats = st.session_state.flashcard_stats
        total = stats.get('total_cards', 0)
        reviewed = stats.get('cards_reviewed', 0)
        mastered = stats.get('cards_mastered', 0)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Cards", total)
        with col2:
            st.metric("Cards Reviewed", reviewed)
        with col3:
            st.metric("Cards Mastered", mastered)
        
        if total > 0:
            st.markdown("---")
            st.markdown(f"**Mastery Rate:** {mastered / reviewed * 100:.1f}%" if reviewed > 0 else "**Mastery Rate:** N/A")
            
            # Cards by course
            course_counts = {}
            for card in st.session_state.flashcards.values():
                course = card.get('course', 'Unknown')
                course_counts[course] = course_counts.get(course, 0) + 1
            
            if course_counts:
                st.markdown("---")
                st.markdown("### Cards by Course")
                for course, count in course_counts.items():
                    st.markdown(f"- **{course}:** {count} cards")

elif page == "Exam Simulator":
    mui_title("fact_check", "Exam Simulator")
    st.markdown("*Practice for your exams with timed simulations*")
    st.markdown("---")
    
    if not st.session_state.exam_mode:
        mui_subheader("settings", "Configure Your Exam")
        
        col1, col2 = st.columns(2)
        with col1:
            exam_course = st.selectbox(
                "Select Course:",
                [c['code'] for c in courses_data]
            )
            num_questions = st.number_input("Number of Questions:", min_value=5, max_value=50, value=10, step=5)
        
        with col2:
            exam_duration = st.number_input("Duration (minutes):", min_value=10, max_value=180, value=60, step=10)
            question_types = st.multiselect(
                "Question Types:",
                ["General", "Knowledge-based", "Skills-based", "Case Study"],
                default=["General", "Knowledge-based"]
            )
        
        if st.button("Start Exam", type="primary", icon=":material/play_arrow:"):
            # Generate exam questions
            selected_course = next(c for c in courses_data if c['code'] == exam_course)
            exam_questions_list = []
            
            type_map = {
                "General": "general",
                "Knowledge-based": "knowledge",
                "Skills-based": "skills",
                "Case Study": "case_study"
            }
            
            with st.spinner("Generating exam questions..."):
                for i in range(num_questions):
                    q_type = type_map.get(question_types[i % len(question_types)], "general")
                    question = generate_practice_question(selected_course, q_type)
                    if question and "Error" not in question:
                        exam_questions_list.append({
                            'id': i,
                            'question': question,
                            'type': q_type,
                            'user_answer': ''
                        })
            
            if exam_questions_list:
                st.session_state.exam_questions = exam_questions_list
                st.session_state.exam_answers = {}
                st.session_state.exam_start_time = time.time()
                st.session_state.exam_duration = exam_duration * 60  # Convert to seconds
                st.session_state.exam_mode = True
                st.session_state.exam_course = exam_course
                st.rerun()
            else:
                st.error("Failed to generate exam questions. Please try again.")
    
    else:
        # Exam in progress
        elapsed_time = time.time() - st.session_state.exam_start_time
        remaining_time = max(0, st.session_state.exam_duration - elapsed_time)
        remaining_minutes = int(remaining_time // 60)
        remaining_seconds = int(remaining_time % 60)
        
        # Timer display
        if remaining_time > 0:
            st.warning(f"Time Remaining: {remaining_minutes:02d}:{remaining_seconds:02d}")
            progress = elapsed_time / st.session_state.exam_duration
            st.progress(progress)
        else:
            st.error("Time's up! Your exam will be submitted automatically.")
            if 'exam_submitted' not in st.session_state:
                st.session_state.exam_submitted = True
                st.rerun()
        
        st.markdown(f"**Course:** {st.session_state.exam_course}")
        st.markdown(f"**Questions:** {len(st.session_state.exam_questions)}")
        
        # Display questions
        st.markdown("---")
        
        view_mode = st.radio("View Mode:", ["One at a time", "All questions"], horizontal=True)
        
        if view_mode == "One at a time":
            if 'current_question_idx' not in st.session_state:
                st.session_state.current_question_idx = 0
            
            current_idx = st.session_state.current_question_idx
            current_q = st.session_state.exam_questions[current_idx]
            
            st.markdown(f"### Question {current_idx + 1} of {len(st.session_state.exam_questions)}")
            
            # Parse question
            if "ANSWER:" in current_q['question']:
                parts = current_q['question'].split("ANSWER:")
                question_text = parts[0].strip()
            else:
                question_text = current_q['question']
            
            st.markdown(f"**{question_text}**")
            
            # Answer input
            answer_key = f"exam_answer_{current_idx}"
            if answer_key not in st.session_state.exam_answers:
                st.session_state.exam_answers[answer_key] = ""
            
            user_answer = st.text_area(
                "Your Answer:",
                value=st.session_state.exam_answers[answer_key],
                height=150,
                key=f"answer_input_{current_idx}"
            )
            st.session_state.exam_answers[answer_key] = user_answer
            
            # Navigation
            col_nav1, col_nav2, col_nav3 = st.columns(3)
            with col_nav1:
                if st.button("Previous", icon=":material/arrow_back:", disabled=current_idx == 0):
                    st.session_state.current_question_idx = max(0, current_idx - 1)
                    st.rerun()
            with col_nav2:
                st.markdown(f"**{current_idx + 1}/{len(st.session_state.exam_questions)}**")
            with col_nav3:
                if st.button("Next", icon=":material/arrow_forward:", disabled=current_idx == len(st.session_state.exam_questions) - 1):
                    st.session_state.current_question_idx = min(len(st.session_state.exam_questions) - 1, current_idx + 1)
                    st.rerun()
        
        else:
            # All questions view
            for idx, q in enumerate(st.session_state.exam_questions):
                with st.expander(f"Question {idx + 1}"):
                    if "ANSWER:" in q['question']:
                        parts = q['question'].split("ANSWER:")
                        question_text = parts[0].strip()
                    else:
                        question_text = q['question']
                    
                    st.markdown(f"**{question_text}**")
                    
                    answer_key = f"exam_answer_{idx}"
                    if answer_key not in st.session_state.exam_answers:
                        st.session_state.exam_answers[answer_key] = ""
                    
                    user_answer = st.text_area(
                        "Your Answer:",
                        value=st.session_state.exam_answers[answer_key],
                        height=100,
                        key=f"answer_all_{idx}"
                    )
                    st.session_state.exam_answers[answer_key] = user_answer
        
        st.markdown("---")
        
        # Submit exam
        col_sub1, col_sub2 = st.columns([1, 1])
        with col_sub1:
            if st.button("Submit Exam", type="primary", icon=":material/send:"):
                st.session_state.exam_submitted = True
                st.rerun()
        with col_sub2:
            if st.button("Cancel Exam", icon=":material/cancel:"):
                if st.checkbox("Are you sure? This will discard your progress."):
                    st.session_state.exam_mode = False
                    st.session_state.exam_questions = []
                    st.session_state.exam_answers = {}
                    st.session_state.exam_start_time = None
                    st.rerun()
        
        # Auto-submit if time is up
        if remaining_time <= 0 and 'exam_submitted' not in st.session_state:
            st.session_state.exam_submitted = True
            st.rerun()
    
    # Show results if exam submitted
    if st.session_state.get('exam_submitted', False) and st.session_state.exam_mode:
        st.markdown("---")
        mui_title("assessment", "Exam Results")
        
        # Evaluate answers
        results = []
        total_score = 0
        
        for idx, q in enumerate(st.session_state.exam_questions):
            answer_key = f"exam_answer_{idx}"
            user_answer = st.session_state.exam_answers.get(answer_key, "")
            
            # Parse question for correct answer
            if "ANSWER:" in q['question']:
                parts = q['question'].split("ANSWER:")
                question_text = parts[0].strip()
                correct_answer = parts[1].strip() if len(parts) > 1 else ""
            else:
                question_text = q['question']
                correct_answer = "Answer not available"
            
            # Evaluate using AI
            if user_answer.strip():
                feedback = evaluate_answer(question_text, correct_answer, user_answer)
                # Simple scoring: if feedback is positive, give points
                score = 1 if "correct" in feedback.lower() or "good" in feedback.lower() else 0.5
            else:
                feedback = "No answer provided"
                score = 0
            
            total_score += score
            results.append({
                'question': question_text,
                'user_answer': user_answer,
                'correct_answer': correct_answer,
                'feedback': feedback,
                'score': score
            })
        
        # Display results
        percentage = (total_score / len(st.session_state.exam_questions)) * 100
        
        st.metric("Your Score", f"{total_score:.1f}/{len(st.session_state.exam_questions)} ({percentage:.1f}%)")
        
        if percentage >= 80:
            st.success("Excellent work!")
        elif percentage >= 60:
            st.info("Good job!")
        else:
            st.warning("Keep studying!")
        
        st.markdown("---")
        mui_subheader("feedback", "Detailed Feedback")
        
        for idx, result in enumerate(results):
            with st.expander(f"Question {idx + 1} - Score: {result['score']}/1"):
                st.markdown(f"**Question:** {result['question']}")
                st.markdown(f"**Your Answer:** {result['user_answer'] if result['user_answer'] else 'No answer'}")
                st.markdown(f"**Correct Answer:** {result['correct_answer']}")
                st.markdown(f"**Feedback:** {result['feedback']}")
        
        if st.button("Take Another Exam"):
            st.session_state.exam_mode = False
            st.session_state.exam_submitted = False
            st.session_state.exam_questions = []
            st.session_state.exam_answers = {}
            st.session_state.exam_start_time = None
            st.rerun()

elif page == "Code Library":
    mui_title("terminal", "Code Library")
    st.markdown("*Useful code snippets and examples*")
    st.markdown("---")
    
    # Initialize default snippets if empty
    if not st.session_state.code_snippets:
        default_snippets = {
            "pandas_basic": {
                "title": "Pandas DataFrame Basics",
                "code": """import pandas as pd

# Create DataFrame
df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'salary': [50000, 60000, 70000]
})

# Basic operations
print(df.head())
print(df.describe())
print(df.info())""",
                "language": "python",
                "category": "Pandas",
                "description": "Basic DataFrame creation and operations"
            },
            "pandas_filter": {
                "title": "Filtering DataFrames",
                "code": """# Filter rows
filtered = df[df['age'] > 25]

# Multiple conditions
filtered = df[(df['age'] > 25) & (df['salary'] > 55000)]

# Filter by string contains
filtered = df[df['name'].str.contains('A')]

# Filter by isin
filtered = df[df['name'].isin(['Alice', 'Bob'])]""",
                "language": "python",
                "category": "Pandas",
                "description": "Common filtering operations"
            },
            "pandas_groupby": {
                "title": "GroupBy Operations",
                "code": """# Group by column
grouped = df.groupby('department')

# Aggregations
summary = df.groupby('department').agg({
    'salary': ['mean', 'sum', 'count'],
    'age': 'mean'
})

# Multiple groupby columns
grouped = df.groupby(['department', 'role']).sum()""",
                "language": "python",
                "category": "Pandas",
                "description": "GroupBy and aggregation examples"
            },
            "sql_select": {
                "title": "SQL SELECT Basics",
                "code": """-- Basic SELECT
SELECT * FROM customers;

-- SELECT with WHERE
SELECT name, email 
FROM customers 
WHERE age > 25;

-- SELECT with JOIN
SELECT c.name, o.order_date, o.amount
FROM customers c
JOIN orders o ON c.id = o.customer_id;""",
                "language": "sql",
                "category": "SQL",
                "description": "Basic SQL SELECT queries"
            },
            "sql_aggregate": {
                "title": "SQL Aggregations",
                "code": """-- COUNT, SUM, AVG
SELECT 
    COUNT(*) as total_orders,
    SUM(amount) as total_revenue,
    AVG(amount) as avg_order_value
FROM orders;

-- GROUP BY
SELECT 
    customer_id,
    COUNT(*) as order_count,
    SUM(amount) as total_spent
FROM orders
GROUP BY customer_id
HAVING COUNT(*) > 5;""",
                "language": "sql",
                "category": "SQL",
                "description": "SQL aggregation functions"
            },
            "excel_sumif": {
                "title": "Excel SUMIF Function",
                "code": """=SUMIF(range, criteria, sum_range)

-- Examples:
=SUMIF(A2:A10, ">100", B2:B10)
=SUMIF(C2:C10, "North", D2:D10)
=SUMIF(E2:E10, ">=2024-01-01", F2:F10)""",
                "language": "excel",
                "category": "Excel",
                "description": "SUMIF function examples"
            },
            "excel_vlookup": {
                "title": "Excel VLOOKUP",
                "code": """=VLOOKUP(lookup_value, table_array, col_index_num, [range_lookup])

-- Examples:
=VLOOKUP(A2, Sheet2!A:B, 2, FALSE)
=VLOOKUP("Product1", Products!A:D, 4, FALSE)""",
                "language": "excel",
                "category": "Excel",
                "description": "VLOOKUP function examples"
            },
            "python_stats": {
                "title": "Python Statistical Functions",
                "code": """import numpy as np
from scipy import stats

# Descriptive statistics
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
mean = np.mean(data)
median = np.median(data)
std = np.std(data)

# Correlation
correlation = np.corrcoef(x, y)[0, 1]

# T-test
t_stat, p_value = stats.ttest_ind(group1, group2)""",
                "language": "python",
                "category": "Statistics",
                "description": "Statistical calculations in Python"
            },
            "pandas_time_series": {
                "title": "Time Series Resampling & Rolling",
                "code": """import pandas as pd

# Ensure datetime index
df['date'] = pd.to_datetime(df['date'])
df = df.set_index('date')

# Resample to monthly totals
monthly = df['sales'].resample('M').sum()

# Rolling 7-day average
df['sales_7d_avg'] = df['sales'].rolling(window=7, min_periods=1).mean()

# Year-over-year growth
df['yoy_growth'] = df['sales'].pct_change(periods=365)""",
                "language": "python",
                "category": "Time Series",
                "description": "Common pandas patterns for date indexed data"
            },
            "python_visualization": {
                "title": "Quick Matplotlib/Seaborn Plots",
                "code": """import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('sales.csv')

# Line plot with trend
plt.figure(figsize=(8, 4))
sns.lineplot(data=df, x='date', y='revenue')
plt.title('Revenue over time')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Bar chart by category
plt.figure(figsize=(6, 4))
sns.barplot(data=df, x='category', y='revenue', estimator=sum)
plt.title('Revenue by category')
plt.xticks(rotation=20)
plt.tight_layout()
plt.show()""",
                "language": "python",
                "category": "Visualization",
                "description": "Two fast plotting patterns with seaborn/matplotlib"
            },
            "sql_window_functions": {
                "title": "SQL Window Functions",
                "code": """-- Running totals by date
SELECT
    order_date,
    amount,
    SUM(amount) OVER (ORDER BY order_date) AS running_revenue
FROM orders;

-- Ranking within groups
SELECT
    customer_id,
    order_date,
    amount,
    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) AS order_rank
FROM orders;

-- Previous value comparison
SELECT
    order_date,
    amount,
    LAG(amount) OVER (ORDER BY order_date) AS prev_amount,
    amount - LAG(amount) OVER (ORDER BY order_date) AS delta_amount
FROM orders;""",
                "language": "sql",
                "category": "SQL",
                "description": "Running totals, ranking, and lag/lead examples"
            },
            "python_api_requests": {
                "title": "API Requests with Error Handling",
                "code": """import requests

BASE_URL = \"https://api.example.com/data\"

def fetch_data(resource_id: str) -> dict:
    try:
        response = requests.get(
            f\"{BASE_URL}/{resource_id}\",
            timeout=10
        )
        response.raise_for_status()
        return response.json()
    except requests.exceptions.Timeout:
        return {\"error\": \"Request timed out\"}
    except requests.exceptions.HTTPError as exc:
        return {\"error\": f\"HTTP error: {exc.response.status_code}\"}
    except Exception as exc:
        return {\"error\": f\"Unexpected error: {exc}\"}

payload = fetch_data(\"customers\")
print(payload)""",
                "language": "python",
                "category": "Python",
                "description": "Requests pattern with timeouts and basic error handling"
            },
            "excel_index_match": {
                "title": "Excel INDEX/MATCH",
                "code": """=INDEX(return_range, MATCH(lookup_value, lookup_range, 0))

-- Examples:
=INDEX(D:D, MATCH(A2, A:A, 0))                 -- Get value from column D by ID in A2
=INDEX(B2:B100, MATCH(\"ProductA\", A2:A100, 0)) -- Lookup product name in column A""",
                "language": "excel",
                "category": "Excel",
                "description": "Flexible lookup pattern that can look left or right"
            },
            "python_data_quality": {
                "title": "Data Quality Checks",
                "code": """import pandas as pd

def validate_orders(df: pd.DataFrame) -> pd.DataFrame:
    issues = []
    
    if not df['order_id'].is_unique:
        issues.append('Duplicate order_id values detected')
    if (df['amount'] < 0).any():
        issues.append('Negative amounts present')
    if not df['customer_id'].notna().all():
        issues.append('Missing customer_id values')
    if (df['order_date'] > pd.Timestamp.today()).any():
        issues.append('Orders dated in the future')
    
    return pd.DataFrame({'issue': issues})

orders = pd.read_csv('orders.csv')
print(validate_orders(orders))""",
                "language": "python",
                "category": "Data Quality",
                "description": "Small checklist for catching common data issues"
            },
            "stats_ab_test": {
                "title": "A/B Test Significance (Two Proportions)",
                "code": """import numpy as np
from scipy.stats import norm

def ab_z_test(success_a, total_a, success_b, total_b):
    p_a = success_a / total_a
    p_b = success_b / total_b
    p_pool = (success_a + success_b) / (total_a + total_b)
    
    se = np.sqrt(p_pool * (1 - p_pool) * (1/total_a + 1/total_b))
    z = (p_a - p_b) / se
    p_value = 2 * (1 - norm.cdf(abs(z)))
    return z, p_value

z_score, p_val = ab_z_test(520, 10000, 570, 10050)
print(f\"z={z_score:.2f}, p-value={p_val:.4f}\")""",
                "language": "python",
                "category": "Statistics",
                "description": "Lightweight two-proportion z-test without extra deps"
            },
            "excel_data_validation_activity_141": {
                "title": "Excel Activity 1.4.1 - Data Validation Dataset Script",
                "code": """import pandas as pd

# Activity 1.4.1 dataset template
item_names = ["Pen", "Notebook", "Eraser", "Ruler", "Marker"]
status_options = ["In Progress", "Completed", "Cancelled"]

df = pd.DataFrame({
    "Item Name": ["Pen", "Notebook", "Eraser", "Ruler", "Marker", "Pen", "Notebook", "Eraser", "Ruler", "Marker"],
    "Quantity": [5, 3, 8, 2, 6, 4, 1, 7, 9, 2],
    "Unit Price": [1.5, 3.2, 0.99, 2.75, 4.6, 1.5, 3.2, 0.99, 2.75, 4.6],
    "Date": ["01/10/2026", "01/11/2026", "01/12/2026", "01/13/2026", "01/14/2026", "01/15/2026", "01/16/2026", "01/17/2026", "01/18/2026", "01/19/2026"],
    "Status": ["In Progress", "Completed", "Cancelled", "In Progress", "Completed", "Cancelled", "In Progress", "Completed", "Cancelled", "In Progress"]
})

# Validation checks
issues = []
for idx, row in df.iterrows():
    row_no = idx + 1
    if row["Item Name"] not in item_names:
        issues.append((row_no, "Item Name not in list"))
    if len(str(row["Item Name"])) < 3 or len(str(row["Item Name"])) > 10:
        issues.append((row_no, "Item Name length not in 3..10"))
    if not float(row["Quantity"]).is_integer() or row["Quantity"] <= 0:
        issues.append((row_no, "Quantity must be positive whole number"))
    if row["Unit Price"] < 0.01 or row["Unit Price"] > 1000.00:
        issues.append((row_no, "Unit Price must be in 0.01..1000.00"))
    parsed_date = pd.to_datetime(row["Date"], format="%m/%d/%Y", errors="coerce")
    if pd.isna(parsed_date):
        issues.append((row_no, "Date must match mm/dd/yyyy"))
    if row["Status"] not in status_options:
        issues.append((row_no, "Status must be In Progress/Completed/Cancelled"))

if issues:
    print("Validation issues found:")
    for issue in issues:
        print(issue)
else:
    print("All rows valid for Activity 1.4.1")
    df["Date"] = pd.to_datetime(df["Date"], format="%m/%d/%Y")
    df["Line Total"] = df["Quantity"] * df["Unit Price"]
    print(df.head())

    # Save for Excel testing
    df.to_csv("activity_141_data.csv", index=False)
    print("Saved: activity_141_data.csv")""",
                "language": "python",
                "category": "Excel",
                "description": "Creates and validates Activity 1.4.1 table data for Excel Data Validation practice"
            }
        }
        st.session_state.code_snippets = default_snippets
    
    # Search and filter
    categories_available = sorted(
        set(s.get('category', 'Other') for s in st.session_state.code_snippets.values()),
        key=str.lower
    )
    languages_available = sorted(
        set(s.get('language', 'python') for s in st.session_state.code_snippets.values())
    )

    if st.session_state.get("code_library_search_prefill"):
        st.session_state["code_library_search_input"] = st.session_state["code_library_search_prefill"]
        st.session_state.pop("code_library_search_prefill", None)

    category_options = ["All"] + categories_available
    if st.session_state.get("code_library_category_prefill"):
        preferred_category = st.session_state["code_library_category_prefill"]
        st.session_state["code_library_category_filter"] = preferred_category if preferred_category in category_options else "All"
        st.session_state.pop("code_library_category_prefill", None)

    language_options = ["All"] + languages_available
    if st.session_state.get("code_library_language_prefill"):
        preferred_language = st.session_state["code_library_language_prefill"]
        st.session_state["code_library_language_filter"] = preferred_language if preferred_language in language_options else "All"
        st.session_state.pop("code_library_language_prefill", None)

    col_search, col_filter, col_lang = st.columns([2, 1, 1])
    with col_search:
        search_query = st.text_input(
            "Search snippets:",
            placeholder="Search by title, description, or code...",
            key="code_library_search_input"
        )
    with col_filter:
        category_filter = st.selectbox(
            "Category:",
            category_options,
            key="code_library_category_filter"
        )
    with col_lang:
        language_filter = st.selectbox(
            "Language:",
            language_options,
            key="code_library_language_filter"
        )
    
    col_meta1, col_meta2 = st.columns([1, 1])
    with col_meta1:
        favorites_only = st.checkbox("â­ Favorites only", value=False)
    with col_meta2:
        sort_by = st.selectbox("Sort by:", ["Title", "Category"])
    
    # Filter snippets
    filtered_snippets = st.session_state.code_snippets
    if search_query:
        filtered_snippets = {
            k: v for k, v in filtered_snippets.items()
            if search_query.lower() in v.get('title', '').lower() or
               search_query.lower() in v.get('description', '').lower() or
               search_query.lower() in v.get('code', '').lower()
        }
    if category_filter != "All":
        filtered_snippets = {
            k: v for k, v in filtered_snippets.items()
            if v.get('category') == category_filter
        }
    if language_filter != "All":
        filtered_snippets = {
            k: v for k, v in filtered_snippets.items()
            if v.get('language') == language_filter
        }
    if favorites_only:
        filtered_snippets = {
            k: v for k, v in filtered_snippets.items()
            if k in st.session_state.code_snippet_favorites
        }
    
    # Display snippets
    if filtered_snippets:
        sort_key = (lambda item: item[1].get('category', '').lower()) if sort_by == "Category" else (lambda item: item[1].get('title', '').lower())
        for snippet_id, snippet in sorted(filtered_snippets.items(), key=sort_key):
            with st.expander(f"ðŸ“„ {snippet.get('title', 'Untitled')} - {snippet.get('category', 'Other')}"):
                st.markdown(f"**Description:** {snippet.get('description', 'No description')}")
                st.caption(f"Language: {snippet.get('language', 'python')} â€¢ Category: {snippet.get('category', 'Other')}")
                st.code(snippet.get('code', ''), language=snippet.get('language', 'python'))
                
                # Copy button (using download button as workaround)
                code_text = snippet.get('code', '')
                st.download_button(
                    "Copy Code",
                    data=code_text,
                    file_name=f"{snippet.get('title', 'code')}.txt",
                    mime="text/plain",
                    icon=":material/content_copy:"
                )
                
                # Favorite toggle
                favorites = set(st.session_state.code_snippet_favorites)
                fav_toggle = st.checkbox("â­ Favorite", value=snippet_id in favorites, key=f"favorite_{snippet_id}")
                if fav_toggle:
                    favorites.add(snippet_id)
                else:
                    favorites.discard(snippet_id)
                st.session_state.code_snippet_favorites = list(favorites)
                
                if st.button(f"Delete", key=f"delete_snippet_{snippet_id}"):
                    del st.session_state.code_snippets[snippet_id]
                    st.rerun()
    else:
        st.info("No snippets found. Create one below!")
    
    st.markdown("---")
    mui_subheader("folder", "Import / Export")
    export_payload = json.dumps(st.session_state.code_snippets, indent=2)
    col_export, col_import = st.columns(2)
    with col_export:
        st.download_button(
            "Download Library (.json)",
            data=export_payload,
            file_name="code_snippets.json",
            mime="application/json",
            icon=":material/download:"
        )
    with col_import:
        uploaded = st.file_uploader("Upload snippets JSON", type=["json"])
        if uploaded:
            try:
                uploaded_snippets = json.loads(uploaded.read().decode("utf-8"))
                if isinstance(uploaded_snippets, dict):
                    st.session_state.code_snippets.update(uploaded_snippets)
                    st.success(f"Imported {len(uploaded_snippets)} snippets")
                    st.rerun()
                else:
                    st.error("Uploaded file must be a JSON object keyed by snippet id.")
            except Exception as exc:
                st.error(f"Could not import snippets: {exc}")
    
    st.markdown("---")
    st.subheader("Add Custom Snippet")
    
    col1, col2 = st.columns(2)
    with col1:
        new_title = st.text_input("Title:")
        new_category = st.selectbox(
            "Category:",
            ["Pandas", "SQL", "Excel", "Statistics", "Python", "Visualization", "Time Series", "Data Quality", "Other"]
        )
        new_language = st.selectbox(
            "Language:",
            ["python", "sql", "excel", "javascript", "bash", "r", "other"]
        )
    
    with col2:
        new_description = st.text_area("Description:", height=80)
    
    new_code = st.text_area("Code:", height=200, placeholder="Paste your code here...")
    
    if st.button("Add Snippet", type="primary", icon=":material/add:"):
        if new_title and new_code:
            snippet_id = f"custom_{int(time.time())}"
            st.session_state.code_snippets[snippet_id] = {
                'title': new_title,
                'code': new_code,
                'language': new_language,
                'category': new_category,
                'description': new_description
            }
            st.success("Snippet added!")
            st.rerun()
        else:
            st.warning("Please fill in title and code.")

elif page == "Formula Reference":
    mui_title("functions", "Formula Reference")
    st.markdown("*Quick reference for statistical formulas*")
    st.markdown("---")
    
    # Category selection
    category = st.selectbox(
        "Select Category:",
        ["Descriptive Statistics", "Inferential Statistics", "Regression", "Correlation", "Probability", "All"]
    )
    
    # Search
    search_query = st.text_input("Search formulas:", placeholder="Search by name or description...")
    
    # Formula database
    formulas = {
        "Descriptive Statistics": [
            {
                "name": "Mean (Average)",
                "formula": r"\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i",
                "description": "The average of all values in a dataset",
                "example": "For [1, 2, 3, 4, 5]: mean = (1+2+3+4+5)/5 = 3"
            },
            {
                "name": "Median",
                "formula": r"\text{Median} = \begin{cases} x_{\frac{n+1}{2}} & \text{if } n \text{ is odd} \\ \frac{x_{\frac{n}{2}} + x_{\frac{n}{2}+1}}{2} & \text{if } n \text{ is even} \end{cases}",
                "description": "The middle value when data is sorted",
                "example": "For [1, 2, 3, 4, 5]: median = 3\nFor [1, 2, 3, 4]: median = (2+3)/2 = 2.5"
            },
            {
                "name": "Standard Deviation",
                "formula": r"s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2}",
                "description": "Measures the spread of data around the mean",
                "example": "For [1, 2, 3, 4, 5] with mean=3:\ns = âˆš[((1-3)Â²+(2-3)Â²+(3-3)Â²+(4-3)Â²+(5-3)Â²)/4] = âˆš2.5 â‰ˆ 1.58"
            },
            {
                "name": "Variance",
                "formula": r"s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2",
                "description": "The square of standard deviation",
                "example": "Variance = (Standard Deviation)Â²"
            },
            {
                "name": "Range",
                "formula": r"\text{Range} = \max(x_i) - \min(x_i)",
                "description": "Difference between maximum and minimum values",
                "example": "For [1, 2, 3, 4, 5]: range = 5 - 1 = 4"
            }
        ],
        "Inferential Statistics": [
            {
                "name": "Z-Score",
                "formula": r"z = \frac{x - \mu}{\sigma}",
                "description": "Number of standard deviations a value is from the mean",
                "example": "If x=110, Î¼=100, Ïƒ=10: z = (110-100)/10 = 1.0"
            },
            {
                "name": "T-Test (One Sample)",
                "formula": r"t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}",
                "description": "Tests if sample mean differs from population mean",
                "example": "Compare sample mean to hypothesized population mean"
            },
            {
                "name": "Confidence Interval (Mean)",
                "formula": r"\bar{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}",
                "description": "Range likely to contain population mean",
                "example": "95% CI: xÌ„ Â± 1.96 Ã— (Ïƒ/âˆšn)"
            },
            {
                "name": "Sample Size",
                "formula": r"n = \left(\frac{z_{\alpha/2} \cdot \sigma}{E}\right)^2",
                "description": "Required sample size for desired margin of error",
                "example": "For E=2, Ïƒ=10, 95% confidence: n = (1.96Ã—10/2)Â² â‰ˆ 96"
            }
        ],
        "Regression": [
            {
                "name": "Simple Linear Regression",
                "formula": r"y = \beta_0 + \beta_1 x + \epsilon",
                "description": "Predicts y from x using a linear relationship",
                "example": "y = 2 + 3x means for each unit increase in x, y increases by 3"
            },
            {
                "name": "Slope (Î²â‚)",
                "formula": r"\beta_1 = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}",
                "description": "Rate of change in y per unit change in x",
                "example": "Positive slope = positive correlation"
            },
            {
                "name": "Intercept (Î²â‚€)",
                "formula": r"\beta_0 = \bar{y} - \beta_1 \bar{x}",
                "description": "Value of y when x = 0",
                "example": "Starting point of the regression line"
            },
            {
                "name": "RÂ² (Coefficient of Determination)",
                "formula": r"R^2 = 1 - \frac{SS_{res}}{SS_{tot}}",
                "description": "Proportion of variance explained by the model",
                "example": "RÂ² = 0.85 means 85% of variance is explained"
            }
        ],
        "Correlation": [
            {
                "name": "Pearson Correlation",
                "formula": r"r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \sum(y_i - \bar{y})^2}}",
                "description": "Measures linear relationship between two variables",
                "example": "r ranges from -1 (perfect negative) to +1 (perfect positive)"
            },
            {
                "name": "Covariance",
                "formula": r"\text{Cov}(X,Y) = \frac{1}{n-1}\sum(x_i - \bar{x})(y_i - \bar{y})",
                "description": "Measures how two variables vary together",
                "example": "Positive = variables increase together"
            }
        ],
        "Probability": [
            {
                "name": "Probability",
                "formula": r"P(A) = \frac{\text{Number of favorable outcomes}}{\text{Total outcomes}}",
                "description": "Likelihood of an event occurring",
                "example": "P(rolling 6 on die) = 1/6 â‰ˆ 0.167"
            },
            {
                "name": "Conditional Probability",
                "formula": r"P(A|B) = \frac{P(A \cap B)}{P(B)}",
                "description": "Probability of A given B has occurred",
                "example": "P(rain|cloudy) = probability of rain when it's cloudy"
            },
            {
                "name": "Bayes' Theorem",
                "formula": r"P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}",
                "description": "Updates probability based on new evidence",
                "example": "Used in medical diagnosis and machine learning"
            }
        ]
    }
    
    # Filter formulas
    display_formulas = {}
    if category == "All":
        display_formulas = formulas
    else:
        if category in formulas:
            display_formulas[category] = formulas[category]
    
    # Apply search filter
    if search_query:
        filtered_formulas = {}
        for cat, form_list in display_formulas.items():
            filtered = [
                f for f in form_list
                if search_query.lower() in f['name'].lower() or
                   search_query.lower() in f['description'].lower()
            ]
            if filtered:
                filtered_formulas[cat] = filtered
        display_formulas = filtered_formulas
    
    # Display formulas
    if display_formulas:
        for cat, form_list in display_formulas.items():
            st.markdown(f"### {cat}")
            for formula in form_list:
                with st.expander(f"**{formula['name']}**"):
                    st.markdown(f"**Description:** {formula['description']}")
                    st.latex(formula['formula'])
                    st.markdown(f"**Example:** {formula['example']}")
            st.markdown("---")
    else:
        st.info("No formulas found. Try a different search or category.")

elif page == "Study Timer":
    mui_title("timer", "Study Timer")
    st.markdown("*Track your study time with Pomodoro technique*")
    st.markdown("---")
    
    # Initialize timer state
    if 'timer_paused' not in st.session_state:
        st.session_state.timer_paused = False
    if 'pomodoro_count' not in st.session_state:
        st.session_state.pomodoro_count = 0
    if 'current_timer_course' not in st.session_state:
        st.session_state.current_timer_course = None
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Timer display
        if st.session_state.study_timer_active:
            elapsed = time.time() - st.session_state.study_timer_start
            remaining = max(0, st.session_state.study_timer_duration - elapsed)
            
            # Convert to minutes and seconds
            minutes = int(remaining // 60)
            seconds = int(remaining % 60)
            
            # Large timer display
            st.markdown(f"<h1 style='text-align: center; font-size: 72px;'>{minutes:02d}:{seconds:02d}</h1>", unsafe_allow_html=True)
            
            # Progress bar
            progress = 1 - (remaining / st.session_state.study_timer_duration) if st.session_state.study_timer_duration > 0 else 0
            st.progress(progress)
            
            # Course being studied
            if st.session_state.current_timer_course:
                course_name = next((c['name'] for c in courses_data if c['code'] == st.session_state.current_timer_course), "Unknown")
                st.markdown(f"**Studying:** {st.session_state.current_timer_course} - {course_name}")
            
            # Control buttons
            col_btn1, col_btn2, col_btn3 = st.columns(3)
            
            with col_btn1:
                if st.button("Pause", icon=":material/pause:", use_container_width=True):
                    if not st.session_state.timer_paused:
                        st.session_state.timer_paused = True
                        st.session_state.timer_pause_time = time.time()
                        st.session_state.timer_elapsed_before_pause = elapsed
                    else:
                        st.session_state.timer_paused = False
                        st.session_state.study_timer_start = time.time() - st.session_state.timer_elapsed_before_pause
                    st.rerun()
            
            with col_btn2:
                if st.button("Stop", use_container_width=True, type="primary", icon=":material/stop:"):
                    # Save session
                    if st.session_state.current_timer_course:
                        course_code = st.session_state.current_timer_course
                        elapsed_total = elapsed if not st.session_state.timer_paused else st.session_state.timer_elapsed_before_pause
                        
                        session_data = {
                            'course': course_code,
                            'duration': int(elapsed_total),
                            'date': datetime.now().isoformat(),
                            'pomodoro': st.session_state.pomodoro_count > 0
                        }
                        st.session_state.study_sessions.append(session_data)
                        
                        # Update course time
                        if course_code not in st.session_state.study_time_by_course:
                            st.session_state.study_time_by_course[course_code] = 0
                        st.session_state.study_time_by_course[course_code] += int(elapsed_total)
                    
                    # Reset timer
                    st.session_state.study_timer_active = False
                    st.session_state.study_timer_start = None
                    st.session_state.timer_paused = False
                    st.session_state.pomodoro_count = 0
                    st.rerun()
            
            with col_btn3:
                if st.button("Reset", icon=":material/restart_alt:", use_container_width=True):
                    st.session_state.study_timer_start = time.time()
                    st.session_state.timer_paused = False
                    st.rerun()
            
            # Auto-stop when timer reaches 0
            if remaining <= 0:
                st.balloons()
                st.success("Time's up! Great work!")
                
                # Save session
                if st.session_state.current_timer_course:
                    course_code = st.session_state.current_timer_course
                    session_data = {
                        'course': course_code,
                        'duration': st.session_state.study_timer_duration,
                        'date': datetime.now().isoformat(),
                        'pomodoro': True
                    }
                    st.session_state.study_sessions.append(session_data)
                    
                    if course_code not in st.session_state.study_time_by_course:
                        st.session_state.study_time_by_course[course_code] = 0
                    st.session_state.study_time_by_course[course_code] += st.session_state.study_timer_duration
                
                st.session_state.study_timer_active = False
                st.session_state.pomodoro_count += 1
                st.rerun()
            
            # Auto-refresh for timer updates (Streamlit will rerun on interaction)
            # Use a placeholder to trigger updates
            placeholder = st.empty()
            placeholder.markdown("")  # Empty placeholder to allow reruns
        
        else:
            # Timer setup
            st.subheader("Configure Timer")
            
            timer_mode = st.radio(
                "Timer Mode:",
                ["Pomodoro (25 min)", "Short Break (5 min)", "Long Break (15 min)", "Custom"]
            )
            
            if timer_mode == "Pomodoro (25 min)":
                duration_minutes = 25
            elif timer_mode == "Short Break (5 min)":
                duration_minutes = 5
            elif timer_mode == "Long Break (15 min)":
                duration_minutes = 15
            else:
                duration_minutes = st.number_input("Custom Duration (minutes):", min_value=1, max_value=180, value=25)
            
            selected_course = st.selectbox(
                "Select Course (optional):",
                ["None"] + [f"{c['code']} - {c['name']}" for c in courses_data]
            )
            
            if st.button("Start Timer", type="primary", icon=":material/play_arrow:", use_container_width=True):
                st.session_state.study_timer_active = True
                st.session_state.study_timer_start = time.time()
                st.session_state.study_timer_duration = duration_minutes * 60
                st.session_state.timer_paused = False
                
                if selected_course != "None":
                    course_code = selected_course.split(" - ")[0]
                    st.session_state.current_timer_course = course_code
                else:
                    st.session_state.current_timer_course = None
                
                st.rerun()
    
    with col2:
        mui_subheader("insights", "Statistics")
        
        # Total study time
        total_seconds = sum(s.get('duration', 0) for s in st.session_state.study_sessions)
        total_hours = total_seconds // 3600
        total_minutes = (total_seconds % 3600) // 60
        
        st.metric("Total Study Time", f"{total_hours}h {total_minutes}m")
        st.metric("Total Sessions", len(st.session_state.study_sessions))
        st.metric("Pomodoros Completed", st.session_state.pomodoro_count)
        
        # Study time by course
        if st.session_state.study_time_by_course:
            st.markdown("---")
            st.markdown("### Time by Course")
            for course_code, seconds in sorted(st.session_state.study_time_by_course.items(), key=lambda x: x[1], reverse=True):
                course_name = next((c['name'] for c in courses_data if c['code'] == course_code), course_code)
                hours = seconds // 3600
                minutes = (seconds % 3600) // 60
                st.markdown(f"**{course_code}**\n- {hours}h {minutes}m")
        
        # Recent sessions
        if st.session_state.study_sessions:
            st.markdown("---")
            st.markdown("### Recent Sessions")
            recent = st.session_state.study_sessions[-5:]
            for session in reversed(recent):
                course_code = session.get('course', 'Unknown')
                duration_min = session.get('duration', 0) // 60
                date_str = session.get('date', '')[:10]
                st.caption(f"{date_str}: {duration_min} min - {course_code}")
        
        # Clear data button
        if st.button("Clear All Data", icon=":material/delete_sweep:"):
            if st.checkbox("Are you sure? This cannot be undone."):
                st.session_state.study_sessions = []
                st.session_state.study_time_by_course = {}
                st.session_state.pomodoro_count = 0
                st.success("Data cleared!")
                st.rerun()

elif page == "Progress":
    mui_title("trending_up", "My Progress")
    st.markdown("---")
    
    st.subheader("Mark Completed Courses")
    
    semesters = ["2025 Spring", "2025 Fall", "2026 Spring", "2026 Fall"]
    
    for sem in semesters:
        st.markdown(f"**{sem}**")
        sem_courses = [c for c in courses_data if c["semester"] == sem]
        
        cols = st.columns(2)
        for i, course in enumerate(sem_courses):
            with cols[i % 2]:
                is_checked = st.checkbox(
                    f"{course['name']} ({course['credits']} cr)",
                    value=course["code"] in st.session_state.completed_courses,
                    key=f"course_{course['code']}"
                )
                if is_checked and course["code"] not in st.session_state.completed_courses:
                    st.session_state.completed_courses.append(course["code"])
                elif not is_checked and course["code"] in st.session_state.completed_courses:
                    st.session_state.completed_courses.remove(course["code"])
        
        st.markdown("---")
    
    total_credits = sum(c["credits"] for c in courses_data)
    completed_credits = sum(c["credits"] for c in courses_data if c["code"] in st.session_state.completed_courses)
    progress = completed_credits / total_credits if total_credits > 0 else 0
    
    st.subheader("Summary")
    st.progress(progress)
    st.write(f"**{completed_credits:.1f} / {total_credits:.0f} credits completed ({progress*100:.0f}%)**")

elif page == "Learning Outcomes":
    mui_title("workspace_premium", "Program Learning Outcomes")
    st.markdown("*Based on the Norwegian Qualifications Framework (NQF)*")
    st.markdown("---")
    
    tab1, tab2, tab3 = st.tabs(["Knowledge", "Skills", "General Competence"])
    
    with tab1:
        st.subheader("Knowledge")
        st.markdown("*After graduation, the candidate has knowledge of:*")
        
        for i, outcome in enumerate(knowledge_outcomes):
            checked = st.checkbox(
                outcome,
                value=st.session_state.knowledge_progress[i],
                key=f"knowledge_{i}"
            )
            st.session_state.knowledge_progress[i] = checked
        
        completed = sum(st.session_state.knowledge_progress)
        st.progress(completed / len(knowledge_outcomes))
        st.caption(f"{completed} / {len(knowledge_outcomes)} learning goals achieved")
    
    with tab2:
        st.subheader("Skills")
        st.markdown("*After graduation, the candidate can:*")
        
        for i, outcome in enumerate(skills_outcomes):
            checked = st.checkbox(
                outcome,
                value=st.session_state.skills_progress[i],
                key=f"skills_{i}"
            )
            st.session_state.skills_progress[i] = checked
        
        completed = sum(st.session_state.skills_progress)
        st.progress(completed / len(skills_outcomes))
        st.caption(f"{completed} / {len(skills_outcomes)} learning goals achieved")
    
    with tab3:
        st.subheader("General Competence")
        st.markdown("*After graduation, the candidate:*")
        
        for i, outcome in enumerate(competence_outcomes):
            checked = st.checkbox(
                outcome,
                value=st.session_state.competence_progress[i],
                key=f"competence_{i}"
            )
            st.session_state.competence_progress[i] = checked
        
        completed = sum(st.session_state.competence_progress)
        st.progress(completed / len(competence_outcomes))
        st.caption(f"{completed} / {len(competence_outcomes)} learning goals achieved")

elif page == "Playground":
    mui_title("construction", "Interactive Playground")
    st.markdown("*Practice with real tools - enter data, run code, and see results instantly*")
    st.markdown("---")
    
    # Organize playground tools by course/category
    playground_tools = {
        "Semester 1 - Tool Skills": {
            "FI1BBDF05 - Data Analysis Fundamentals": [
                "BI & Big Data Explorer"
            ],
            "FI1BBSF05 - Spreadsheet Fundamentals": [
                "Excel Formula Simulator",
                "Power Query Simulator",
                "Excel Workbook Profiler"
            ],
            "FI1BBST05 - Statistical Tools": [
                "Statistical Analysis",
                "Z-Score & Outlier Tool"
            ],
            "FI1BBPF20 - Programming Fundamentals": [
                "Python Code Runner",
                "PDF Viewer (PDF.js)"
            ]
        },
        "Semester 2 - Data & Visualization": {
            "FI1BBDC20 - Databases and Cloud Services": [
                "SQL Query Tester"
            ],
            "FI1BBDV15 - Data Visualisation": [
                "Chart Builder",
                "Data Visualization Studio"
            ],
            "FI1BBDD75 - Data Driven Decision-Making": [
                "KPI Dashboard Builder",
                "Decision Analysis Tool"
            ],
            "FI1BBP175 - Semester Project 1": [
                "Project Planning Workshop"
            ],
            "FI1BBAR05 - Analysis Reporting": [
                "Report Writing Workshop"
            ],
            "FI1BBP275 - Exam Project 1": [
                "Exam Project Toolkit"
            ]
        },
        "Semester 3 - Competence Skills": {
            "FI1BBEO10 - Evaluation of Outcomes": [
                "Ethical Analysis Critique",
                "Error Detection Workshop",
                "Confidence Level Planner"
            ]
        }
    }
    
    st.markdown("### Choose a Practice Tool")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        # Category filter
        available_categories = list(playground_tools.keys())
        selected_category = st.selectbox(
            "Semester/Category:",
            options=["All Categories"] + available_categories
        )
    
    with col2:
        # Course filter
        if selected_category == "All Categories":
            all_courses = []
            for cat in available_categories:
                for course in playground_tools[cat].keys():
                    if course not in all_courses:
                        all_courses.append(course)
            available_pg_courses = all_courses
        else:
            available_pg_courses = list(playground_tools[selected_category].keys())
        
        selected_pg_course = st.selectbox(
            "Course:",
            options=["All Courses"] + available_pg_courses,
            key="pg_course"
        )
    
    # Get filtered tools
    filtered_tools = []
    for category, courses in playground_tools.items():
        if selected_category != "All Categories" and category != selected_category:
            continue
        for course, tools in courses.items():
            if selected_pg_course != "All Courses" and course != selected_pg_course:
                continue
            for tool in tools:
                filtered_tools.append((tool, course, category))
    
    if not filtered_tools:
        st.warning("No tools found for the selected filters.")
        st.stop()
    
    # Show tool count
    st.caption(f"{len(filtered_tools)} tools available")
    
    # Tool selector
    tool_options = [t[0] for t in filtered_tools]
    playground_tab = st.selectbox(
        "Select Tool:",
        options=tool_options
    )
    
    # Find course info for selected tool
    tool_course = None
    tool_category = None
    for tool, course, category in filtered_tools:
        if tool == playground_tab:
            tool_course = course
            tool_category = category
            break
    
    # Show context
    st.markdown(f"**{tool_category}** | **{tool_course}**")
    
    if playground_tab == "Python Code Runner":
        mui_subheader("code", "Python Code Playground")
        st.markdown("Practice pandas operations on your own data! Edit the tables below, then run exercises.")
        
        # Initialize session state for editable data
        if 'python_sales_data' not in st.session_state:
            st.session_state.python_sales_data = pd.DataFrame({
                'Product': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones'],
                'Price': [999, 699, 449, 299, 149],
                'Units_Sold': [150, 300, 200, 400, 500],
                'Region': ['North', 'South', 'North', 'East', 'West']
            })
        
        if 'python_employee_data' not in st.session_state:
            st.session_state.python_employee_data = pd.DataFrame({
                'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
                'Department': ['Sales', 'IT', 'Sales', 'HR', 'IT'],
                'Salary': [55000, 72000, 48000, 61000, 68000],
                'Years': [3, 5, 2, 7, 4]
            })
        
        st.markdown("### Your Datasets (Edit to add your own data!)")
        data_tab1, data_tab2 = st.tabs(["sales_data", "employee_data"])
        with data_tab1:
            st.caption("Click any cell to edit. Use the + button to add rows.")
            st.session_state.python_sales_data = st.data_editor(
                st.session_state.python_sales_data,
                num_rows="dynamic",
                use_container_width=True,
                key="python_sales_editor"
            )
        with data_tab2:
            st.caption("Click any cell to edit. Use the + button to add rows.")
            st.session_state.python_employee_data = st.data_editor(
                st.session_state.python_employee_data,
                num_rows="dynamic",
                use_container_width=True,
                key="python_employee_editor"
            )
        
        # Use the session state data
        sales_data = st.session_state.python_sales_data
        employee_data = st.session_state.python_employee_data
        
        st.markdown("### Select an Exercise")
        
        exercises = {
            "View DataFrame": {
                "code": "# View the sales data\nprint(sales_data)",
                "description": "Display the entire DataFrame"
            },
            "Add calculated column": {
                "code": "# Add a Revenue column\nsales_data['Revenue'] = sales_data['Price'] * sales_data['Units_Sold']\nprint(sales_data)",
                "description": "Create new columns from calculations"
            },
            "Basic statistics": {
                "code": "# Get summary statistics\nprint(sales_data.describe())\n\n# Specific calculations\nprint(f\"Total Revenue: ${(sales_data['Price'] * sales_data['Units_Sold']).sum():,}\")\nprint(f\"Average Price: ${sales_data['Price'].mean():.2f}\")",
                "description": "Calculate mean, sum, and other statistics"
            },
            "Filter data": {
                "code": "# Filter products with price over $300\nexpensive = sales_data[sales_data['Price'] > 300]\nprint(expensive)",
                "description": "Select rows based on conditions"
            },
            "Group By aggregation": {
                "code": "# Total units sold by region\nby_region = sales_data.groupby('Region')['Units_Sold'].sum()\nprint(by_region)",
                "description": "Aggregate data by categories"
            },
            "Sort data": {
                "code": "# Sort by price (highest first)\nsorted_data = sales_data.sort_values('Price', ascending=False)\nprint(sorted_data)",
                "description": "Order rows by column values"
            },
            "Employee analysis": {
                "code": "# Average salary by department\navg_salary = employee_data.groupby('Department')['Salary'].mean()\nprint(avg_salary)\n\n# Highest paid employee\ntop_earner = employee_data.loc[employee_data['Salary'].idxmax()]\nprint(f\"\\nHighest paid: {top_earner['Name']} (${top_earner['Salary']:,})\")",
                "description": "Practice with employee dataset"
            }
        }
        
        selected_exercise = st.selectbox("Choose exercise:", list(exercises.keys()))
        
        st.markdown(f"*{exercises[selected_exercise]['description']}*")
        
        st.markdown("### Code")
        st.code(exercises[selected_exercise]['code'], language="python")
        
        if st.button("Run Code", type="primary", icon=":material/play_arrow:"):
            st.markdown("### Output:")
            
            # Execute the selected exercise safely with pre-defined data
            import io
            import sys as _sys
            
            old_stdout = _sys.stdout
            _sys.stdout = buffer = io.StringIO()
            
            try:
                # Create a copy of the data for this execution
                local_sales = sales_data.copy()
                local_employee = employee_data.copy()
                
                # Execute based on selected exercise
                if selected_exercise == "View DataFrame":
                    print(local_sales)
                elif selected_exercise == "Add calculated column":
                    local_sales['Revenue'] = local_sales['Price'] * local_sales['Units_Sold']
                    print(local_sales)
                elif selected_exercise == "Basic statistics":
                    print(local_sales.describe())
                    print(f"\nTotal Revenue: ${(local_sales['Price'] * local_sales['Units_Sold']).sum():,}")
                    print(f"Average Price: ${local_sales['Price'].mean():.2f}")
                elif selected_exercise == "Filter data":
                    expensive = local_sales[local_sales['Price'] > 300]
                    print(expensive)
                elif selected_exercise == "Group By aggregation":
                    by_region = local_sales.groupby('Region')['Units_Sold'].sum()
                    print(by_region)
                elif selected_exercise == "Sort data":
                    sorted_data = local_sales.sort_values('Price', ascending=False)
                    print(sorted_data)
                elif selected_exercise == "Employee analysis":
                    avg_salary = local_employee.groupby('Department')['Salary'].mean()
                    print(avg_salary)
                    top_earner = local_employee.loc[local_employee['Salary'].idxmax()]
                    print(f"\nHighest paid: {top_earner['Name']} (${top_earner['Salary']:,})")
                
                output = buffer.getvalue()
                _sys.stdout = old_stdout
                
                st.code(output, language="text")
                
                # Show resulting DataFrame if applicable
                if selected_exercise == "Add calculated column":
                    st.markdown("**Result DataFrame:**")
                    st.dataframe(local_sales, use_container_width=True)
                elif selected_exercise == "Filter data":
                    expensive = local_sales[local_sales['Price'] > 300]
                    st.dataframe(expensive, use_container_width=True)
                elif selected_exercise == "Sort data":
                    sorted_data = local_sales.sort_values('Price', ascending=False)
                    st.dataframe(sorted_data, use_container_width=True)
                    
            except Exception as e:
                _sys.stdout = old_stdout
                st.error(f"Error: {str(e)}")
        
        st.markdown("---")
        st.markdown("**Key pandas methods demonstrated:**")
        st.markdown("- `df.describe()` - Summary statistics")
        st.markdown("- `df.groupby()` - Group and aggregate")
        st.markdown("- `df.sort_values()` - Sort rows")
        st.markdown("- `df[condition]` - Filter rows")

    elif playground_tab == "PDF Viewer (PDF.js)":
        mui_subheader("picture_as_pdf", "PDF Viewer (PDF.js)")
        st.markdown("Upload a PDF and render it with PDF.js in the app.")

        uploaded_pdf = st.file_uploader(
            "Upload PDF",
            type=["pdf"],
            key="pdfjs_viewer_upload"
        )

        if uploaded_pdf is not None:
            pdf_bytes = uploaded_pdf.read()
            if not pdf_bytes:
                st.warning("The uploaded file is empty.")
            else:
                st.caption(f"Loaded: {uploaded_pdf.name} ({len(pdf_bytes):,} bytes)")

                col1, col2 = st.columns(2)
                with col1:
                    initial_page = st.number_input("Start page", min_value=1, value=1, step=1)
                with col2:
                    initial_zoom = st.slider("Initial zoom (%)", min_value=50, max_value=250, value=100, step=10)

                pdf_base64 = base64.b64encode(pdf_bytes).decode("utf-8")
                viewer_html = f"""
                <!doctype html>
                <html>
                <head>
                  <meta charset=\"utf-8\" />
                  <style>
                    body {{ margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; }}
                    .toolbar {{ display: flex; gap: 8px; align-items: center; padding: 10px; border-bottom: 1px solid #e5e7eb; background: #f8fafc; }}
                    .toolbar button {{ border: 1px solid #cbd5e1; background: #fff; border-radius: 6px; padding: 6px 10px; cursor: pointer; }}
                    .meta {{ margin-left: auto; color: #334155; font-size: 13px; }}
                    #canvas-wrap {{ height: 690px; overflow: auto; background: #f1f5f9; display: flex; justify-content: center; align-items: flex-start; padding: 16px 0; }}
                    #pdf-canvas {{ border: 1px solid #cbd5e1; box-shadow: 0 2px 10px rgba(0,0,0,0.08); background: #fff; }}
                    #error {{ color: #b91c1c; padding: 10px; font-size: 13px; }}
                  </style>
                </head>
                <body>
                  <div class=\"toolbar\">
                    <button id=\"prev\">Prev</button>
                    <button id=\"next\">Next</button>
                    <button id=\"zoom-out\">-</button>
                    <button id=\"zoom-in\">+</button>
                    <button id=\"reset\">Reset</button>
                    <div class=\"meta\" id=\"meta\">Loading PDF...</div>
                  </div>
                  <div id=\"canvas-wrap\"><canvas id=\"pdf-canvas\"></canvas></div>
                  <div id=\"error\"></div>

                  <script type=\"module\">
                    import * as pdfjsLib from "https://cdn.jsdelivr.net/npm/pdfjs-dist@5.4.624/build/pdf.min.mjs";
                    pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdn.jsdelivr.net/npm/pdfjs-dist@5.4.624/build/pdf.worker.min.mjs";

                    const base64 = "{pdf_base64}";
                    const binary = atob(base64);
                    const bytes = new Uint8Array(binary.length);
                    for (let i = 0; i < binary.length; i++) bytes[i] = binary.charCodeAt(i);

                    const canvas = document.getElementById("pdf-canvas");
                    const ctx = canvas.getContext("2d");
                    const meta = document.getElementById("meta");
                    const error = document.getElementById("error");

                    let pdfDoc = null;
                    let currentPage = {int(initial_page)};
                    let scale = {float(initial_zoom) / 100.0};
                    const baseScale = scale;

                    async function renderPage(pageNum) {{
                      if (!pdfDoc) return;
                      const page = await pdfDoc.getPage(pageNum);
                      const viewport = page.getViewport({{ scale }});
                      canvas.width = viewport.width;
                      canvas.height = viewport.height;
                      await page.render({{ canvasContext: ctx, viewport }}).promise;
                      meta.textContent = `Page ${{currentPage}} / ${{pdfDoc.numPages}} â€¢ Zoom ${{Math.round(scale * 100)}}%`;
                    }}

                    async function loadPdf() {{
                      try {{
                        const loadingTask = pdfjsLib.getDocument({{ data: bytes }});
                        pdfDoc = await loadingTask.promise;
                        currentPage = Math.max(1, Math.min(currentPage, pdfDoc.numPages));
                        await renderPage(currentPage);
                      }} catch (e) {{
                        error.textContent = `Failed to load PDF: ${{e?.message || e}}`;
                      }}
                    }}

                    document.getElementById("prev").addEventListener("click", async () => {{
                      if (!pdfDoc || currentPage <= 1) return;
                      currentPage -= 1;
                      await renderPage(currentPage);
                    }});

                    document.getElementById("next").addEventListener("click", async () => {{
                      if (!pdfDoc || currentPage >= pdfDoc.numPages) return;
                      currentPage += 1;
                      await renderPage(currentPage);
                    }});

                    document.getElementById("zoom-in").addEventListener("click", async () => {{
                      scale = Math.min(4, scale + 0.1);
                      await renderPage(currentPage);
                    }});

                    document.getElementById("zoom-out").addEventListener("click", async () => {{
                      scale = Math.max(0.3, scale - 0.1);
                      await renderPage(currentPage);
                    }});

                    document.getElementById("reset").addEventListener("click", async () => {{
                      scale = baseScale;
                      await renderPage(currentPage);
                    }});

                    loadPdf();
                  </script>
                </body>
                </html>
                """

                html(viewer_html, height=760, scrolling=False)
        else:
            st.caption("Upload a PDF to start viewing pages with PDF.js.")

    elif playground_tab == "Excel Formula Simulator":
        mui_subheader("table_view", "Excel Formula Simulator")
        st.markdown("Practice Excel formulas including VLOOKUP and INDEX/MATCH!")
        
        # Main data table
        st.markdown("### Main Data Table")
        st.caption("Click cells to edit. Use + to add rows.")
        
        if 'excel_data' not in st.session_state:
            st.session_state.excel_data = pd.DataFrame({
                'EmployeeID': ['E001', 'E002', 'E003', 'E004', 'E005'],
                'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
                'Department': ['Sales', 'IT', 'Sales', 'HR', 'Marketing'],
                'Sales': [15000, 22000, 18000, 12000, 25000],
                'Target': [14000, 20000, 17000, 15000, 22000]
            })
        
        edited_data = st.data_editor(
            st.session_state.excel_data,
            num_rows="dynamic",
            use_container_width=True,
            key="excel_editor"
        )
        st.session_state.excel_data = edited_data
        
        # Lookup reference table for VLOOKUP/INDEX-MATCH
        st.markdown("### Lookup Reference Table")
        st.caption("This table is used for VLOOKUP and INDEX/MATCH lookups.")
        
        if 'excel_lookup' not in st.session_state:
            st.session_state.excel_lookup = pd.DataFrame({
                'Department': ['Sales', 'IT', 'HR', 'Marketing', 'Finance'],
                'Manager': ['John Smith', 'Sarah Lee', 'Mike Brown', 'Lisa Chen', 'Tom Wilson'],
                'Budget': [50000, 75000, 40000, 60000, 80000],
                'Bonus_Rate': [0.10, 0.08, 0.05, 0.12, 0.07]
            })
        
        lookup_data = st.data_editor(
            st.session_state.excel_lookup,
            num_rows="dynamic",
            use_container_width=True,
            key="excel_lookup_editor"
        )
        st.session_state.excel_lookup = lookup_data
        
        st.markdown("### Apply Formulas")
        
        formula_categories = {
            "Basic Functions": ["SUM", "AVERAGE", "COUNT", "MAX", "MIN"],
            "Conditional Functions": ["SUMIF", "COUNTIF", "AVERAGEIF"],
            "Lookup Functions": ["VLOOKUP", "INDEX/MATCH", "XLOOKUP"],
            "Formatting & Tables": ["Conditional Formatting", "Pivot Table"],
            "Comparison": ["Excel vs Google Sheets"],
            "Other": ["IF Statement", "Custom Calculation"]
        }
        
        col1, col2 = st.columns(2)
        with col1:
            category = st.selectbox("Category:", list(formula_categories.keys()))
        with col2:
            formula_type = st.selectbox("Formula:", formula_categories[category])
        
        st.markdown("---")
        
        # Basic Functions
        if formula_type in ["SUM", "AVERAGE", "COUNT", "MAX", "MIN"]:
            numeric_cols = edited_data.select_dtypes(include=['number']).columns.tolist()
            if numeric_cols:
                column = st.selectbox("Select column:", numeric_cols)
                
                if st.button("Calculate", type="primary", icon=":material/calculate:"):
                    try:
                        if formula_type == "SUM":
                            result = edited_data[column].sum()
                        elif formula_type == "AVERAGE":
                            result = edited_data[column].mean()
                        elif formula_type == "COUNT":
                            result = edited_data[column].count()
                        elif formula_type == "MAX":
                            result = edited_data[column].max()
                        elif formula_type == "MIN":
                            result = edited_data[column].min()
                        
                        st.success(f"={formula_type}({column}) = **{result:,.2f}**")
                        st.code(f"Excel: ={formula_type}(B2:B{len(edited_data)+1})")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
            else:
                st.warning("No numeric columns available.")
        
        # Conditional Functions
        elif formula_type in ["SUMIF", "COUNTIF", "AVERAGEIF"]:
            crit_col = st.selectbox("Criteria column:", edited_data.columns.tolist())
            criteria = st.text_input("Criteria value:", edited_data[crit_col].iloc[0] if len(edited_data) > 0 else "")
            
            if formula_type != "COUNTIF":
                numeric_cols = edited_data.select_dtypes(include=['number']).columns.tolist()
                if numeric_cols:
                    sum_col = st.selectbox("Value column:", numeric_cols)
            
            if st.button("Calculate", type="primary", icon=":material/calculate:"):
                try:
                    mask = edited_data[crit_col].astype(str).str.lower() == str(criteria).lower()
                    
                    if formula_type == "SUMIF":
                        result = edited_data.loc[mask, sum_col].sum()
                        st.success(f"=SUMIF({crit_col}, \"{criteria}\", {sum_col}) = **{result:,.2f}**")
                        st.code(f"Excel: =SUMIF(A:A, \"{criteria}\", B:B)")
                    elif formula_type == "COUNTIF":
                        result = mask.sum()
                        st.success(f"=COUNTIF({crit_col}, \"{criteria}\") = **{result}**")
                        st.code(f"Excel: =COUNTIF(A:A, \"{criteria}\")")
                    elif formula_type == "AVERAGEIF":
                        result = edited_data.loc[mask, sum_col].mean()
                        st.success(f"=AVERAGEIF({crit_col}, \"{criteria}\", {sum_col}) = **{result:,.2f}**")
                        st.code(f"Excel: =AVERAGEIF(A:A, \"{criteria}\", B:B)")
                        
                    st.info(f"Found {mask.sum()} matching row(s)")
                except Exception as e:
                    st.error(f"Error: {str(e)}")
        
        # VLOOKUP
        elif formula_type == "VLOOKUP":
            st.markdown("**VLOOKUP** finds a value in the first column of the lookup table and returns a value from another column.")
            
            col1, col2 = st.columns(2)
            with col1:
                lookup_value = st.selectbox("Lookup value from main table:", edited_data.columns.tolist())
                row_to_lookup = st.selectbox("Which row to lookup:", list(range(len(edited_data))))
                actual_lookup = edited_data.iloc[row_to_lookup][lookup_value]
                st.info(f"Looking up: **{actual_lookup}**")
            
            with col2:
                return_col = st.selectbox("Return column from lookup table:", lookup_data.columns.tolist()[1:])
            
            if st.button("Run VLOOKUP", type="primary", icon=":material/search:"):
                try:
                    lookup_key_col = lookup_data.columns[0]
                    match = lookup_data[lookup_data[lookup_key_col].astype(str).str.lower() == str(actual_lookup).lower()]
                    
                    if len(match) > 0:
                        result = match.iloc[0][return_col]
                        st.success(f"=VLOOKUP(\"{actual_lookup}\", LookupTable, {list(lookup_data.columns).index(return_col)+1}, FALSE)")
                        st.success(f"Result: **{result}**")
                        st.code(f"Excel: =VLOOKUP(A2, LookupTable!A:D, {list(lookup_data.columns).index(return_col)+1}, FALSE)")
                        
                        st.markdown("**How it works:**")
                        st.markdown(f"1. Search for '{actual_lookup}' in first column of lookup table")
                        st.markdown(f"2. Find matching row")
                        st.markdown(f"3. Return value from '{return_col}' column: **{result}**")
                    else:
                        st.warning(f"No match found for '{actual_lookup}' in lookup table.")
                        st.info("This would return #N/A in Excel")
                except Exception as e:
                    st.error(f"Error: {str(e)}")
        
        # INDEX/MATCH
        elif formula_type == "INDEX/MATCH":
            st.markdown("**INDEX/MATCH** is more flexible than VLOOKUP - it can look up values in any column!")
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**MATCH: Find the row**")
                match_value = st.selectbox("Value to find:", edited_data.columns.tolist())
                row_idx = st.selectbox("From row:", list(range(len(edited_data))))
                actual_match = edited_data.iloc[row_idx][match_value]
                st.info(f"Finding: **{actual_match}**")
                
                match_in_col = st.selectbox("Search in column:", lookup_data.columns.tolist())
            
            with col2:
                st.markdown("**INDEX: Return the value**")
                return_col = st.selectbox("Return from column:", lookup_data.columns.tolist())
            
            if st.button("Run INDEX/MATCH", type="primary", icon=":material/search:"):
                try:
                    match_result = lookup_data[lookup_data[match_in_col].astype(str).str.lower() == str(actual_match).lower()]
                    
                    if len(match_result) > 0:
                        row_num = match_result.index[0]
                        result = lookup_data.loc[row_num, return_col]
                        
                        st.success(f"=INDEX({return_col}, MATCH(\"{actual_match}\", {match_in_col}, 0))")
                        st.success(f"Result: **{result}**")
                        st.code(f"Excel: =INDEX(LookupTable!{return_col}:{return_col}, MATCH(A2, LookupTable!{match_in_col}:{match_in_col}, 0))")
                        
                        st.markdown("**How it works:**")
                        st.markdown(f"1. MATCH finds '{actual_match}' in '{match_in_col}' column")
                        st.markdown(f"2. Returns row position: **{list(lookup_data.index).index(row_num) + 1}**")
                        st.markdown(f"3. INDEX uses that position to get value from '{return_col}': **{result}**")
                        
                        st.info("INDEX/MATCH advantage: Can lookup left (VLOOKUP can only look right)")
                    else:
                        st.warning(f"No match found for '{actual_match}'")
                except Exception as e:
                    st.error(f"Error: {str(e)}")
        
        # XLOOKUP (modern alternative)
        elif formula_type == "XLOOKUP":
            st.markdown("**XLOOKUP** is the modern replacement for VLOOKUP (Excel 365/2021+)")
            
            col1, col2 = st.columns(2)
            with col1:
                lookup_col = st.selectbox("Lookup value column:", edited_data.columns.tolist())
                row_idx = st.selectbox("From row:", list(range(len(edited_data))))
                lookup_val = edited_data.iloc[row_idx][lookup_col]
                st.info(f"Looking up: **{lookup_val}**")
            
            with col2:
                search_col = st.selectbox("Search in:", lookup_data.columns.tolist())
                return_col = st.selectbox("Return from:", lookup_data.columns.tolist())
            
            if st.button("Run XLOOKUP", type="primary", icon=":material/search:"):
                try:
                    match = lookup_data[lookup_data[search_col].astype(str).str.lower() == str(lookup_val).lower()]
                    
                    if len(match) > 0:
                        result = match.iloc[0][return_col]
                        st.success(f"=XLOOKUP(\"{lookup_val}\", {search_col}, {return_col})")
                        st.success(f"Result: **{result}**")
                        st.code(f"Excel: =XLOOKUP(A2, LookupTable!{search_col}:{search_col}, LookupTable!{return_col}:{return_col})")
                        
                        st.markdown("**XLOOKUP advantages over VLOOKUP:**")
                        st.markdown("- Can search in any direction (left or right)")
                        st.markdown("- Simpler syntax (no column numbers)")
                        st.markdown("- Built-in error handling")
                    else:
                        st.warning(f"No match found for '{lookup_val}'")
                except Exception as e:
                    st.error(f"Error: {str(e)}")
        
        # IF Statement
        elif formula_type == "IF Statement":
            st.markdown("**IF** returns different values based on a condition.")
            
            numeric_cols = edited_data.select_dtypes(include=['number']).columns.tolist()
            if len(numeric_cols) >= 2:
                col1, col2 = st.columns(2)
                with col1:
                    check_col = st.selectbox("Check column:", numeric_cols)
                    operator = st.selectbox("Operator:", [">", "<", ">=", "<=", "="])
                with col2:
                    compare_to = st.selectbox("Compare to:", ["Value", "Another Column"])
                    if compare_to == "Value":
                        compare_val = st.number_input("Value:", value=0)
                    else:
                        compare_col = st.selectbox("Column:", [c for c in numeric_cols if c != check_col])
                
                if st.button("Apply IF Formula", type="primary", icon=":material/functions:"):
                    try:
                        if compare_to == "Value":
                            if operator == ">":
                                result = edited_data[check_col] > compare_val
                            elif operator == "<":
                                result = edited_data[check_col] < compare_val
                            elif operator == ">=":
                                result = edited_data[check_col] >= compare_val
                            elif operator == "<=":
                                result = edited_data[check_col] <= compare_val
                            else:
                                result = edited_data[check_col] == compare_val
                            formula_str = f"=IF({check_col}{operator}{compare_val}, \"Yes\", \"No\")"
                        else:
                            if operator == ">":
                                result = edited_data[check_col] > edited_data[compare_col]
                            elif operator == "<":
                                result = edited_data[check_col] < edited_data[compare_col]
                            elif operator == ">=":
                                result = edited_data[check_col] >= edited_data[compare_col]
                            elif operator == "<=":
                                result = edited_data[check_col] <= edited_data[compare_col]
                            else:
                                result = edited_data[check_col] == edited_data[compare_col]
                            formula_str = f"=IF({check_col}{operator}{compare_col}, \"Yes\", \"No\")"
                        
                        result_df = edited_data.copy()
                        result_df['IF_Result'] = result.map({True: 'Yes', False: 'No'})
                        
                        st.success(formula_str)
                        st.dataframe(result_df, use_container_width=True)
                        st.code(f"Excel: =IF(A2{operator}B2, \"Yes\", \"No\")")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
        
        # Conditional Formatting
        elif formula_type == "Conditional Formatting":
            st.markdown("**Conditional Formatting** applies visual styles based on cell values.")
            st.markdown("Common uses: highlight high/low values, show trends, identify outliers")
            
            numeric_cols = edited_data.select_dtypes(include=['number']).columns.tolist()
            
            if numeric_cols:
                format_col = st.selectbox("Column to format:", numeric_cols)
                
                format_type = st.selectbox(
                    "Formatting rule:",
                    ["Highlight cells > value", "Highlight cells < value", "Color Scale (low to high)", 
                     "Data Bars", "Top/Bottom N values", "Highlight duplicates"]
                )
                
                if format_type == "Highlight cells > value":
                    threshold = st.number_input("Highlight values greater than:", value=float(edited_data[format_col].mean()))
                    highlight_color = st.color_picker("Highlight color:", "#90EE90")
                    
                    if st.button("Apply Formatting", type="primary", icon=":material/format_paint:"):
                        result_df = edited_data.copy()
                        
                        def highlight_above(row):
                            return [f'background-color: {highlight_color}' if col == format_col and pd.notna(row[col]) and row[col] > threshold else '' for col in row.index]
                        
                        styled = result_df.style.apply(highlight_above, axis=1)
                        st.markdown("### Result (cells highlighted in green):")
                        st.dataframe(styled, use_container_width=True)
                        
                        matches = (result_df[format_col] > threshold).sum()
                        st.success(f"Highlighted {matches} cells where {format_col} > {threshold}")
                        st.code(f"Excel: Home > Conditional Formatting > Highlight Cells Rules > Greater Than > {threshold}")
                
                elif format_type == "Highlight cells < value":
                    threshold = st.number_input("Highlight values less than:", value=float(edited_data[format_col].mean()))
                    highlight_color = st.color_picker("Highlight color:", "#FFB6C1")
                    
                    if st.button("Apply Formatting", type="primary", icon=":material/format_paint:"):
                        result_df = edited_data.copy()
                        
                        def highlight_below(row):
                            return [f'background-color: {highlight_color}' if col == format_col and pd.notna(row[col]) and row[col] < threshold else '' for col in row.index]
                        
                        styled = result_df.style.apply(highlight_below, axis=1)
                        st.markdown("### Result (cells highlighted):")
                        st.dataframe(styled, use_container_width=True)
                        
                        matches = (result_df[format_col] < threshold).sum()
                        st.success(f"Highlighted {matches} cells where {format_col} < {threshold}")
                        st.code(f"Excel: Home > Conditional Formatting > Highlight Cells Rules > Less Than > {threshold}")
                
                elif format_type == "Color Scale (low to high)":
                    st.markdown("Color scales show values on a gradient from low (red) to high (green).")
                    
                    if st.button("Apply Color Scale", type="primary", icon=":material/format_color_fill:"):
                        result_df = edited_data.copy()
                        
                        styled = result_df.style.background_gradient(
                            subset=[format_col],
                            cmap='RdYlGn'
                        )
                        
                        st.markdown("### Result (color gradient applied):")
                        st.dataframe(styled, use_container_width=True)
                        
                        st.success(f"Applied color scale: Red (low) â†’ Yellow (mid) â†’ Green (high)")
                        st.code("Excel: Home > Conditional Formatting > Color Scales > Green-Yellow-Red")
                
                elif format_type == "Data Bars":
                    st.markdown("Data bars show relative values as bars within cells.")
                    
                    if st.button("Apply Data Bars", type="primary", icon=":material/bar_chart:"):
                        result_df = edited_data.copy()
                        
                        styled = result_df.style.bar(
                            subset=[format_col],
                            color='#5fba7d',
                            vmin=0
                        )
                        
                        st.markdown("### Result (data bars in cells):")
                        st.dataframe(styled, use_container_width=True)
                        
                        st.success("Applied data bars showing relative values")
                        st.code("Excel: Home > Conditional Formatting > Data Bars > Solid Fill")
                
                elif format_type == "Top/Bottom N values":
                    n_values = st.slider("Number of values:", 1, min(10, len(edited_data)), 3)
                    top_or_bottom = st.radio("Highlight:", ["Top N", "Bottom N"], horizontal=True)
                    
                    if st.button("Apply Formatting", type="primary", icon=":material/format_paint:"):
                        result_df = edited_data.copy()
                        
                        if top_or_bottom == "Top N":
                            threshold = result_df[format_col].nlargest(n_values).min()
                            mask = result_df[format_col] >= threshold
                            color = '#90EE90'
                        else:
                            threshold = result_df[format_col].nsmallest(n_values).max()
                            mask = result_df[format_col] <= threshold
                            color = '#FFB6C1'
                        
                        def highlight_topbottom(row):
                            if mask[row.name]:
                                return [f'background-color: {color}' if col == format_col else '' for col in row.index]
                            return ['' for _ in row.index]
                        
                        styled = result_df.style.apply(highlight_topbottom, axis=1)
                        st.markdown(f"### Result ({top_or_bottom} {n_values} highlighted):")
                        st.dataframe(styled, use_container_width=True)
                        
                        st.success(f"Highlighted {mask.sum()} cells ({top_or_bottom} {n_values})")
                        st.code(f"Excel: Home > Conditional Formatting > Top/Bottom Rules > Top {n_values} Items")
                
                elif format_type == "Highlight duplicates":
                    dup_col = st.selectbox("Check for duplicates in:", edited_data.columns.tolist())
                    
                    if st.button("Find Duplicates", type="primary", icon=":material/content_copy:"):
                        result_df = edited_data.copy()
                        mask = result_df.duplicated(subset=[dup_col], keep=False)
                        
                        def highlight_dups(row):
                            if mask[row.name]:
                                return ['background-color: #FFFF99' if col == dup_col else '' for col in row.index]
                            return ['' for _ in row.index]
                        
                        styled = result_df.style.apply(highlight_dups, axis=1)
                        st.markdown("### Result (duplicates highlighted in yellow):")
                        st.dataframe(styled, use_container_width=True)
                        
                        dup_count = mask.sum()
                        if dup_count > 0:
                            st.warning(f"Found {dup_count} duplicate values in '{dup_col}'")
                        else:
                            st.success("No duplicates found!")
                        
                        st.code("Excel: Home > Conditional Formatting > Highlight Cells Rules > Duplicate Values")
            else:
                st.warning("Need numeric columns for conditional formatting.")
        
        # Pivot Table
        elif formula_type == "Pivot Table":
            st.markdown("**Pivot Tables** summarize and analyze data by grouping and aggregating.")
            st.markdown("Drag-and-drop style: choose rows, columns, and values to create summaries.")
            
            all_cols = edited_data.columns.tolist()
            numeric_cols = edited_data.select_dtypes(include=['number']).columns.tolist()
            categorical_cols = [c for c in all_cols if c not in numeric_cols]
            
            if categorical_cols and numeric_cols:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Rows (Group By):**")
                    row_field = st.selectbox("Row field:", categorical_cols + ["None"])
                    
                    st.markdown("**Columns (Optional):**")
                    col_field = st.selectbox("Column field:", ["None"] + [c for c in categorical_cols if c != row_field])
                
                with col2:
                    st.markdown("**Values:**")
                    value_field = st.selectbox("Value field:", numeric_cols)
                    
                    st.markdown("**Aggregation:**")
                    agg_func = st.selectbox("Function:", ["Sum", "Average", "Count", "Min", "Max"])
                
                if st.button("Create Pivot Table", type="primary", icon=":material/pivot_table_chart:"):
                    try:
                        agg_map = {"Sum": "sum", "Average": "mean", "Count": "count", "Min": "min", "Max": "max"}
                        
                        if row_field == "None":
                            st.warning("Please select at least a Row field.")
                        else:
                            if col_field != "None":
                                pivot = pd.pivot_table(
                                    edited_data,
                                    values=value_field,
                                    index=row_field,
                                    columns=col_field,
                                    aggfunc=agg_map[agg_func],
                                    fill_value=0
                                )
                            else:
                                pivot = edited_data.groupby(row_field)[value_field].agg(agg_map[agg_func]).reset_index()
                                pivot.columns = [row_field, f"{agg_func} of {value_field}"]
                            
                            st.markdown("### Pivot Table Result:")
                            st.dataframe(pivot, use_container_width=True)
                            
                            # Add grand totals
                            if col_field != "None":
                                st.markdown(f"**Grand Total ({agg_func}):** {pivot.values.sum():,.2f}")
                            
                            st.code("Excel: Insert > PivotTable > Select data range")
                            st.markdown("**Steps in Excel:**")
                            st.markdown(f"1. Drag '{row_field}' to Rows area")
                            if col_field != "None":
                                st.markdown(f"2. Drag '{col_field}' to Columns area")
                            st.markdown(f"3. Drag '{value_field}' to Values area")
                            st.markdown(f"4. Change aggregation to '{agg_func}'")
                    except Exception as e:
                        st.error(f"Error creating pivot table: {str(e)}")
            else:
                st.warning("Need at least one categorical column and one numeric column for pivot tables.")
                st.info("Try adding a text column (like 'Department' or 'Region') to your data.")
        
        # Excel vs Google Sheets Comparison
        elif formula_type == "Excel vs Google Sheets":
            st.markdown("## Excel vs Google Sheets Comparison")
            st.markdown("Both are powerful spreadsheet tools. Here's when to use each:")
            
            st.markdown("### Feature Comparison")
            
            comparison_data = pd.DataFrame({
                'Feature': [
                    'Best For', 'Collaboration', 'Offline Access', 'Data Size', 
                    'Advanced Features', 'Cost', 'Learning Curve', 'Integration',
                    'Macros/Automation', 'Mobile App'
                ],
                'Microsoft Excel': [
                    'Complex analysis, large datasets', 'SharePoint/OneDrive (paid)', 
                    'Full offline support', 'Millions of rows', 'Power Query, Power Pivot, VBA',
                    'Paid (Office 365)', 'Steeper', 'Microsoft ecosystem',
                    'VBA macros (powerful)', 'Good'
                ],
                'Google Sheets': [
                    'Collaboration, simple tasks', 'Real-time built-in (free)',
                    'Limited (Chrome extension)', '10 million cells max',
                    'Apps Script, Explore feature', 'Free', 'Easier', 'Google Workspace',
                    'Apps Script (web-based)', 'Excellent'
                ]
            })
            
            st.dataframe(comparison_data, use_container_width=True, hide_index=True)
            
            st.markdown("### Formula Differences")
            
            formula_diff = pd.DataFrame({
                'Function': ['VLOOKUP', 'Array Formulas', 'Unique Values', 'Filter Data', 'Import Data'],
                'Excel': ['=VLOOKUP()', 'Ctrl+Shift+Enter (legacy)', '=UNIQUE()', '=FILTER()', '=WEBSERVICE()'],
                'Google Sheets': ['=VLOOKUP()', 'Native (no special entry)', '=UNIQUE()', '=FILTER()', '=IMPORTDATA()']
            })
            
            st.dataframe(formula_diff, use_container_width=True, hide_index=True)
            
            st.markdown("### When to Use Each")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### Use Excel When:")
                st.markdown("- Working with very large datasets (100k+ rows)")
                st.markdown("- Need Power Query for data transformation")
                st.markdown("- Require VBA macros for complex automation")
                st.markdown("- Working offline frequently")
                st.markdown("- Need Power Pivot for data modeling")
                st.markdown("- Using advanced statistical tools")
            
            with col2:
                st.markdown("#### Use Google Sheets When:")
                st.markdown("- Real-time collaboration is essential")
                st.markdown("- Team needs free access")
                st.markdown("- Data needs to be always accessible online")
                st.markdown("- Integrating with other Google tools")
                st.markdown("- Simple to medium complexity tasks")
                st.markdown("- Quick sharing without file attachments")
            
            st.markdown("### Key Shortcuts Comparison")
            
            shortcuts = pd.DataFrame({
                'Action': ['Copy', 'Paste', 'Fill Down', 'Select All', 'Find', 'Insert Row', 'Format Cells'],
                'Excel (Windows)': ['Ctrl+C', 'Ctrl+V', 'Ctrl+D', 'Ctrl+A', 'Ctrl+F', 'Ctrl+Shift++', 'Ctrl+1'],
                'Google Sheets': ['Ctrl+C', 'Ctrl+V', 'Ctrl+D', 'Ctrl+A', 'Ctrl+F', 'Ctrl+Shift++', 'Ctrl+\\']
            })
            
            st.dataframe(shortcuts, use_container_width=True, hide_index=True)
            
            st.info("**Tip:** Most basic formulas work the same in both! Learn one, and you'll know 80% of the other.")
        
        # Custom Calculation
        elif formula_type == "Custom Calculation":
            st.markdown("### Add Calculated Column")
            new_col_name = st.text_input("New column name:", "Performance")
            
            numeric_cols = edited_data.select_dtypes(include=['number']).columns.tolist()
            if len(numeric_cols) >= 2:
                col_a = st.selectbox("First column:", numeric_cols)
                operation = st.selectbox("Operation:", ["+", "-", "*", "/"])
                col_b = st.selectbox("Second column:", [c for c in numeric_cols if c != col_a])
                
                if st.button("Add Column"):
                    try:
                        if operation == "+":
                            edited_data[new_col_name] = edited_data[col_a] + edited_data[col_b]
                        elif operation == "-":
                            edited_data[new_col_name] = edited_data[col_a] - edited_data[col_b]
                        elif operation == "*":
                            edited_data[new_col_name] = edited_data[col_a] * edited_data[col_b]
                        elif operation == "/":
                            edited_data[new_col_name] = (edited_data[col_a] / edited_data[col_b]).round(2)
                        
                        st.session_state.excel_data = edited_data
                        st.success(f"Added column: {new_col_name} = {col_a} {operation} {col_b}")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
    
    elif playground_tab == "SQL Query Tester":
        mui_subheader("database", "SQL Query Tester")
        st.markdown("Write SQL queries against your data - edit the tables below to add your own!")
        
        # Initialize editable tables in session state
        if 'sql_customers' not in st.session_state:
            st.session_state.sql_customers = pd.DataFrame({
                'id': [1, 2, 3, 4, 5],
                'name': ['Alice Smith', 'Bob Johnson', 'Charlie Brown', 'Diana Ross', 'Eve Wilson'],
                'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],
                'joined_date': ['2023-01-15', '2023-03-22', '2023-02-10', '2023-04-05', '2023-01-30']
            })
        
        if 'sql_orders' not in st.session_state:
            st.session_state.sql_orders = pd.DataFrame({
                'order_id': [101, 102, 103, 104, 105, 106, 107, 108],
                'customer_id': [1, 2, 1, 3, 4, 2, 5, 1],
                'product': ['Laptop', 'Phone', 'Tablet', 'Laptop', 'Phone', 'Watch', 'Headphones', 'Phone'],
                'amount': [999, 699, 449, 999, 699, 299, 149, 699],
                'order_date': ['2024-01-10', '2024-01-12', '2024-01-15', '2024-02-01', '2024-02-05', '2024-02-10', '2024-02-15', '2024-03-01']
            })
        
        st.markdown("### Your Tables (Edit to add your own data!)")
        
        tab1, tab2 = st.tabs(["customers", "orders"])
        with tab1:
            st.caption("Click cells to edit. Use + to add rows.")
            st.session_state.sql_customers = st.data_editor(
                st.session_state.sql_customers,
                num_rows="dynamic",
                use_container_width=True,
                key="sql_customers_editor"
            )
        with tab2:
            st.caption("Click cells to edit. Use + to add rows.")
            st.session_state.sql_orders = st.data_editor(
                st.session_state.sql_orders,
                num_rows="dynamic",
                use_container_width=True,
                key="sql_orders_editor"
            )
        
        customers = st.session_state.sql_customers
        orders = st.session_state.sql_orders
        
        st.markdown("### Write Your Query")
        
        example_queries = {
            "Select all customers": "SELECT * FROM customers",
            "Orders over $500": "SELECT * FROM orders WHERE amount > 500",
            "Total sales by product": "SELECT product, SUM(amount) as total FROM orders GROUP BY product",
            "Join customers and orders": "SELECT c.name, o.product, o.amount FROM orders o JOIN customers c ON o.customer_id = c.id",
            "Top customers by spend": "SELECT c.name, SUM(o.amount) as total_spent FROM orders o JOIN customers c ON o.customer_id = c.id GROUP BY c.name ORDER BY total_spent DESC"
        }
        
        selected_example = st.selectbox("Try an example:", ["Custom Query"] + list(example_queries.keys()))
        
        if selected_example == "Custom Query":
            default_query = "SELECT * FROM customers"
        else:
            default_query = example_queries[selected_example]
        
        query = st.text_area("SQL Query:", value=default_query, height=100)
        
        if st.button("Run Query", type="primary", icon=":material/play_arrow:"):
            st.markdown("### Results:")
            
            try:
                import sqlite3
                
                # Create in-memory database
                conn = sqlite3.connect(':memory:')
                customers.to_sql('customers', conn, index=False, if_exists='replace')
                orders.to_sql('orders', conn, index=False, if_exists='replace')
                
                # Execute query
                result = pd.read_sql_query(query, conn)
                
                st.dataframe(result, use_container_width=True)
                st.caption(f"Returned {len(result)} row(s)")
                
                conn.close()
                
            except Exception as e:
                st.error(f"Query Error: {str(e)}")
        
        st.markdown("---")
        st.markdown("**Practice Exercises:**")
        st.markdown("1. Find all orders from customer_id 1")
        st.markdown("2. Calculate average order amount")
        st.markdown("3. List customers who have placed more than 1 order")
    
    elif playground_tab == "Chart Builder":
        mui_subheader("insert_chart", "Chart Builder")
        st.markdown("Create visualizations from your own data! Edit the table below.")
        
        # Initialize editable chart data in session state
        if 'chart_data' not in st.session_state:
            st.session_state.chart_data = pd.DataFrame({
                'Category': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],
                'Value1': [45000, 52000, 48000, 61000, 55000, 67000],
                'Value2': [32000, 35000, 33000, 38000, 36000, 40000]
            })
        
        st.markdown("### Your Data (Edit to add your own!)")
        st.caption("Click cells to edit values. Use + to add rows.")
        st.session_state.chart_data = st.data_editor(
            st.session_state.chart_data,
            num_rows="dynamic",
            use_container_width=True,
            key="chart_data_editor"
        )
        
        chart_data = st.session_state.chart_data
        
        st.markdown("### Choose Chart Type")
        
        col1, col2 = st.columns(2)
        
        # Get numeric columns for Y-axis options
        numeric_cols = chart_data.select_dtypes(include=['number']).columns.tolist()
        all_cols = chart_data.columns.tolist()
        
        with col1:
            chart_type = st.selectbox(
                "Chart type:",
                ["Bar Chart", "Line Chart", "Area Chart", "Scatter Plot"]
            )
        
        with col2:
            x_column = st.selectbox("X-axis (categories):", all_cols, index=0)
            y_column = st.selectbox("Y-axis (values):", numeric_cols if numeric_cols else all_cols)
        
        st.markdown("### Your Chart")
        
        try:
            if chart_type == "Bar Chart":
                st.bar_chart(chart_data.set_index(x_column)[y_column])
            elif chart_type == "Line Chart":
                st.line_chart(chart_data.set_index(x_column)[y_column])
            elif chart_type == "Area Chart":
                st.area_chart(chart_data.set_index(x_column)[y_column])
            elif chart_type == "Scatter Plot":
                if len(numeric_cols) >= 2:
                    import altair as alt
                    x_scatter = st.selectbox("Scatter X:", numeric_cols, key="scatter_x")
                    y_scatter = st.selectbox("Scatter Y:", [c for c in numeric_cols if c != x_scatter], key="scatter_y")
                    scatter = alt.Chart(chart_data).mark_circle(size=100).encode(
                        x=x_scatter,
                        y=y_scatter,
                        tooltip=all_cols
                    ).properties(width=600, height=400)
                    st.altair_chart(scatter, use_container_width=True)
                else:
                    st.info("Scatter plots need at least 2 numeric columns. Add more data!")
        except Exception as e:
            st.warning(f"Could not create chart: {str(e)}")
        
        st.markdown("---")
        st.markdown("**Chart Best Practices:**")
        st.markdown("- **Bar charts**: Best for comparing categories")
        st.markdown("- **Line charts**: Best for showing trends over time")
        st.markdown("- **Area charts**: Good for showing volume/cumulative data")
        st.markdown("- **Scatter plots**: Best for showing relationships between two variables")
    
    elif playground_tab == "Data Visualization Studio":
        mui_subheader("palette", "Data Visualization Studio")
        st.markdown("*Practice choosing charts, designing for accessibility, and identifying visualization problems*")
        st.markdown("---")
        
        viz_mode = st.radio(
            "Choose practice mode:",
            ["Chart Selection Advisor", "Accessibility Checker", "Visualization Critique", "Story Builder"],
            horizontal=True
        )
        
        if viz_mode == "Chart Selection Advisor":
            st.markdown("### Chart Selection Advisor")
            st.markdown("*Answer questions about your data to get chart recommendations*")
            
            col1, col2 = st.columns(2)
            with col1:
                data_goal = st.selectbox(
                    "What do you want to show?",
                    ["Compare categories", "Show trend over time", "Show parts of a whole", 
                     "Show relationship between variables", "Show distribution", "Show geographic patterns"]
                )
                num_categories = st.slider("How many categories/data points?", 2, 50, 6)
            
            with col2:
                audience = st.selectbox(
                    "Who is your audience?",
                    ["Executives (quick glance)", "Technical team (detailed analysis)", 
                     "General public", "Mixed audience"]
                )
                interactive = st.checkbox("Does it need to be interactive?")
            
            if st.button("Get Recommendations", type="primary", icon=":material/lightbulb:"):
                st.markdown("---")
                st.markdown("### Recommended Charts")
                
                recommendations = []
                warnings = []
                
                if data_goal == "Compare categories":
                    if num_categories <= 7:
                        recommendations.append(("Bar Chart (horizontal)", "Best for comparing values across categories", "â­ Top Pick"))
                        recommendations.append(("Column Chart (vertical)", "Good when categories have short labels", ""))
                    else:
                        recommendations.append(("Horizontal Bar Chart", "Essential when you have many categories", "â­ Top Pick"))
                        warnings.append("With 7+ categories, avoid pie charts - they become unreadable")
                
                elif data_goal == "Show trend over time":
                    recommendations.append(("Line Chart", "Perfect for continuous time series", "â­ Top Pick"))
                    recommendations.append(("Area Chart", "Good for showing volume/magnitude over time", ""))
                    if num_categories <= 4:
                        recommendations.append(("Multiple Line Chart", "Compare trends across 2-4 series", ""))
                
                elif data_goal == "Show parts of a whole":
                    if num_categories <= 5:
                        recommendations.append(("Pie Chart", "Works well with few categories", "â­ Top Pick"))
                        recommendations.append(("Donut Chart", "Modern alternative to pie", ""))
                    else:
                        recommendations.append(("Stacked Bar Chart", "Better than pie for many categories", "â­ Top Pick"))
                        recommendations.append(("Treemap", "Good for hierarchical data", ""))
                        warnings.append("Avoid pie charts with more than 5 slices")
                
                elif data_goal == "Show relationship between variables":
                    recommendations.append(("Scatter Plot", "Shows correlation between variables", "â­ Top Pick"))
                    recommendations.append(("Bubble Chart", "Adds a third dimension (size)", ""))
                
                elif data_goal == "Show distribution":
                    recommendations.append(("Histogram", "Shows frequency distribution", "â­ Top Pick"))
                    recommendations.append(("Box Plot", "Shows median, quartiles, outliers", ""))
                
                elif data_goal == "Show geographic patterns":
                    recommendations.append(("Choropleth Map", "Color-coded regions", "â­ Top Pick"))
                    recommendations.append(("Bubble Map", "Points with size encoding", ""))
                
                if audience == "Executives (quick glance)":
                    recommendations.append(("Single Number + Sparkline", "Maximum simplicity for busy executives", "ðŸ’¼"))
                    warnings.append("Keep it simple - one key insight per visual")
                
                for chart, desc, badge in recommendations:
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.markdown(f"**{chart}**")
                        st.caption(desc)
                    with col2:
                        if badge:
                            st.markdown(badge)
                
                if warnings:
                    st.markdown("### Watch Out For:")
                    for w in warnings:
                        st.warning(w)
        
        elif viz_mode == "Accessibility Checker":
            st.markdown("### Accessibility Checker")
            st.markdown("*Evaluate your visualization design for accessibility*")
            
            st.markdown("**Rate your visualization on these criteria:**")
            
            checks = {
                "Color contrast": st.select_slider(
                    "Text and data have sufficient contrast against background",
                    options=["No", "Partially", "Yes"],
                    value="Partially"
                ),
                "Color independence": st.select_slider(
                    "Information is conveyed by more than just color (shapes, patterns, labels)",
                    options=["No", "Partially", "Yes"],
                    value="Partially"
                ),
                "Clear labels": st.select_slider(
                    "All axes, legends, and data points are clearly labeled",
                    options=["No", "Partially", "Yes"],
                    value="Partially"
                ),
                "Font readability": st.select_slider(
                    "Font size is large enough and font is sans-serif",
                    options=["No", "Partially", "Yes"],
                    value="Partially"
                ),
                "Title and context": st.select_slider(
                    "Chart has a descriptive title and necessary context",
                    options=["No", "Partially", "Yes"],
                    value="Partially"
                ),
                "Data source": st.select_slider(
                    "Data source is cited",
                    options=["No", "Partially", "Yes"],
                    value="Partially"
                )
            }
            
            if st.button("Generate Accessibility Report", type="primary", icon=":material/summarize:"):
                st.markdown("---")
                st.markdown("### Accessibility Report")
                
                score = 0
                max_score = len(checks) * 2
                
                for check, value in checks.items():
                    if value == "Yes":
                        st.success(f"**{check}**: Good")
                        score += 2
                    elif value == "Partially":
                        st.warning(f"**{check}**: Needs improvement")
                        score += 1
                    else:
                        st.error(f"âŒ **{check}**: Missing")
                
                st.markdown("---")
                pct = (score / max_score) * 100
                st.metric("Accessibility Score", f"{pct:.0f}%")
                
                if pct >= 80:
                    st.success("Your visualization meets basic accessibility standards!")
                elif pct >= 50:
                    st.warning("Your visualization needs some improvements for full accessibility.")
                else:
                    st.error("Significant accessibility improvements needed.")
                
                st.markdown("**Tips for improvement:**")
                if checks["Color independence"] != "Yes":
                    st.markdown("- Add patterns, shapes, or direct labels so color isn't the only differentiator")
                if checks["Color contrast"] != "Yes":
                    st.markdown("- Use a contrast checker tool (aim for 4.5:1 ratio)")
                if checks["Clear labels"] != "Yes":
                    st.markdown("- Add axis labels, legend, and data labels where helpful")
        
        elif viz_mode == "Visualization Critique":
            st.markdown("### Visualization Critique")
            st.markdown("*Identify problems in common visualization mistakes*")
            
            critique_scenarios = {
                "Truncated Y-Axis": {
                    "description": "A bar chart shows company revenue: Year 1 = $98M, Year 2 = $100M, Year 3 = $102M. The y-axis starts at $95M.",
                    "problems": ["Y-axis starting at $95M makes 2% differences look like 50%+ differences", "Viewer perceives dramatic growth that doesn't exist"],
                    "fixes": ["Start y-axis at $0 for bar charts", "If small differences matter, use a line chart or table with % change"]
                },
                "Pie Chart Overload": {
                    "description": "A pie chart shows market share with 15 different slices, many smaller than 3%.",
                    "problems": ["Too many slices to compare", "Small slices are indistinguishable", "Colors become repetitive"],
                    "fixes": ["Group small categories into 'Other'", "Use horizontal bar chart instead", "Limit pie charts to 5-7 slices"]
                },
                "3D Chart Distortion": {
                    "description": "A 3D pie chart is used to show budget allocation. The slice at the front appears larger than equal slices at the back.",
                    "problems": ["3D perspective distorts slice sizes", "Front slices appear larger due to perspective", "Harder to compare values accurately"],
                    "fixes": ["Use 2D charts - they're more accurate", "If you must use 3D, provide data labels", "Consider a bar chart instead"]
                },
                "Cherry-Picked Timeframe": {
                    "description": "A chart shows 'Record Growth!' by comparing March (lowest month) to December (highest month), ignoring the full year context.",
                    "problems": ["Misleading comparison (worst to best)", "Hides overall trend", "Reader can't see seasonal patterns"],
                    "fixes": ["Show complete time period", "Compare same periods (year-over-year)", "Include context about seasonality"]
                }
            }
            
            selected_scenario = st.selectbox("Choose a scenario to analyze:", list(critique_scenarios.keys()))
            scenario = critique_scenarios[selected_scenario]
            
            st.markdown(f"**Scenario:** {scenario['description']}")
            
            user_problems = st.text_area(
                "What problems do you see? (List them)",
                placeholder="Enter the issues you identify...",
                height=100
            )
            
            if st.button("Show Expert Analysis", type="primary", icon=":material/psychology:"):
                st.markdown("---")
                st.markdown("### Problems Identified:")
                for p in scenario["problems"]:
                    st.error(f"âŒ {p}")
                
                st.markdown("### Recommended Fixes:")
                for f in scenario["fixes"]:
                    st.success(f"{f}")
        
        elif viz_mode == "Story Builder":
            st.markdown("### Data Story Builder")
            st.markdown("*Practice structuring a data presentation*")
            
            st.markdown("**Fill in your data story framework:**")
            
            context = st.text_area(
                "1. CONTEXT: Why should the audience care?",
                placeholder="e.g., 'Customer churn has increased 15% this quarter, threatening our revenue targets...'",
                height=80
            )
            
            insight = st.text_area(
                "2. INSIGHT: What does the data reveal?",
                placeholder="e.g., 'Analysis of 5,000 churned customers shows 70% cited 'slow support response' as their top complaint...'",
                height=80
            )
            
            visualization = st.text_input(
                "3. VISUALIZATION: What chart will you show?",
                placeholder="e.g., 'Bar chart comparing churn reasons, with response time highlighted'"
            )
            
            action = st.text_area(
                "4. ACTION: What should happen next?",
                placeholder="e.g., 'Recommend investing $50K in chat support to reduce response time from 4 hours to 30 minutes...'",
                height=80
            )
            
            if st.button("Generate Story Summary", type="primary", icon=":material/auto_awesome:"):
                if context and insight and action:
                    st.markdown("---")
                    st.markdown("### Your Data Story")
                    
                    story = f"""## Data Story: Executive Summary

### Context
{context}

### Key Insight
{insight}

### Visualization
*{visualization if visualization else '[Add your chart description]'}*

### Recommended Action
{action}

---
*Story Structure: Setup â†’ Insight â†’ Action*
"""
                    st.markdown(story)
                    
                    st.download_button(
                        "Download Story Framework",
                        data=story,
                        file_name="data_story.md",
                        mime="text/markdown",
                        icon=":material/download:"
                    )
                else:
                    st.warning("Please fill in all sections to generate your story.")
    
    elif playground_tab == "Statistical Analysis":
        mui_subheader("query_stats", "Statistical Analysis Tool")
        st.markdown("Perform correlation, regression, ANOVA, histogram, and covariance analysis on your data!")
        
        from scipy import stats
        import numpy as np
        
        # Initialize sample data for statistical analysis
        if 'stats_data' not in st.session_state:
            np.random.seed(42)
            st.session_state.stats_data = pd.DataFrame({
                'Advertising': [10, 15, 20, 25, 30, 35, 40, 45, 50, 55],
                'Sales': [100, 120, 150, 170, 200, 220, 250, 280, 300, 320],
                'Region': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],
                'Quarter': ['Q1', 'Q1', 'Q2', 'Q2', 'Q3', 'Q3', 'Q4', 'Q4', 'Q1', 'Q2'],
                'Employees': [5, 8, 10, 12, 15, 18, 20, 22, 25, 28]
            })
        
        st.markdown("### Your Data (Edit to add your own!)")
        st.caption("Click cells to edit. Use + to add rows.")
        st.session_state.stats_data = st.data_editor(
            st.session_state.stats_data,
            num_rows="dynamic",
            use_container_width=True,
            key="stats_data_editor"
        )
        
        stats_data = st.session_state.stats_data
        numeric_cols = stats_data.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = stats_data.select_dtypes(include=['object']).columns.tolist()
        
        st.markdown("### Choose Analysis Type")
        
        analysis_type = st.selectbox(
            "Analysis:",
            ["Correlation Analysis", "Linear Regression", "ANOVA (Analysis of Variance)", 
             "Histogram", "Covariance Analysis", "Descriptive Statistics"]
        )
        
        st.markdown("---")
        
        if analysis_type == "Correlation Analysis":
            st.markdown("**Correlation** measures the strength and direction of relationship between two variables.")
            st.markdown("- Values range from -1 to +1")
            st.markdown("- +1 = perfect positive correlation, -1 = perfect negative correlation, 0 = no correlation")
            
            if len(numeric_cols) >= 2:
                col1, col2 = st.columns(2)
                with col1:
                    var1 = st.selectbox("Variable 1:", numeric_cols)
                with col2:
                    var2 = st.selectbox("Variable 2:", [c for c in numeric_cols if c != var1])
                
                if st.button("Calculate Correlation", type="primary", icon=":material/insights:"):
                    try:
                        correlation, p_value = stats.pearsonr(stats_data[var1], stats_data[var2])
                        
                        st.markdown("### Results")
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Correlation (r)", f"{correlation:.4f}")
                        with col2:
                            st.metric("R-squared", f"{correlation**2:.4f}")
                        with col3:
                            st.metric("P-value", f"{p_value:.4f}")
                        
                        # Interpretation
                        if abs(correlation) >= 0.7:
                            strength = "strong"
                        elif abs(correlation) >= 0.4:
                            strength = "moderate"
                        else:
                            strength = "weak"
                        
                        direction = "positive" if correlation > 0 else "negative"
                        
                        st.success(f"There is a **{strength} {direction}** correlation between {var1} and {var2}.")
                        
                        if p_value < 0.05:
                            st.info("The correlation is statistically significant (p < 0.05).")
                        else:
                            st.warning("The correlation is NOT statistically significant (p >= 0.05).")
                        
                        # Show scatter plot
                        st.markdown("### Scatter Plot")
                        import altair as alt
                        scatter = alt.Chart(stats_data).mark_circle(size=60).encode(
                            x=var1,
                            y=var2,
                            tooltip=[var1, var2]
                        ).properties(width=600, height=300)
                        
                        # Add trend line
                        line = scatter.transform_regression(var1, var2).mark_line(color='red')
                        st.altair_chart(scatter + line, use_container_width=True)
                        
                        st.code(f"Excel: =CORREL({var1}:{var1}, {var2}:{var2})")
                        st.code(f"Python: scipy.stats.pearsonr(df['{var1}'], df['{var2}'])")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
            else:
                st.warning("Need at least 2 numeric columns for correlation analysis.")
        
        elif analysis_type == "Linear Regression":
            st.markdown("**Linear Regression** finds the best-fit line to predict one variable from another.")
            st.markdown("Formula: y = mx + b (where m = slope, b = intercept)")
            
            if len(numeric_cols) >= 2:
                col1, col2 = st.columns(2)
                with col1:
                    x_var = st.selectbox("Independent Variable (X):", numeric_cols)
                with col2:
                    y_var = st.selectbox("Dependent Variable (Y):", [c for c in numeric_cols if c != x_var])
                
                if st.button("Run Regression", type="primary", icon=":material/show_chart:"):
                    try:
                        slope, intercept, r_value, p_value, std_err = stats.linregress(
                            stats_data[x_var], stats_data[y_var]
                        )
                        
                        st.markdown("### Regression Results")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Slope (m)", f"{slope:.4f}")
                        with col2:
                            st.metric("Intercept (b)", f"{intercept:.4f}")
                        with col3:
                            st.metric("R-squared", f"{r_value**2:.4f}")
                        with col4:
                            st.metric("P-value", f"{p_value:.4f}")
                        
                        st.success(f"**Equation:** {y_var} = {slope:.2f} * {x_var} + {intercept:.2f}")
                        
                        st.markdown("**Interpretation:**")
                        st.markdown(f"- For every 1 unit increase in {x_var}, {y_var} increases by {slope:.2f}")
                        st.markdown(f"- The model explains {r_value**2*100:.1f}% of the variance in {y_var}")
                        
                        # Prediction tool
                        st.markdown("### Make a Prediction")
                        pred_x = st.number_input(f"Enter {x_var} value:", value=float(stats_data[x_var].mean()))
                        pred_y = slope * pred_x + intercept
                        st.info(f"Predicted {y_var}: **{pred_y:.2f}**")
                        
                        # Show regression plot
                        import altair as alt
                        scatter = alt.Chart(stats_data).mark_circle(size=60).encode(
                            x=x_var,
                            y=y_var,
                            tooltip=[x_var, y_var]
                        ).properties(width=600, height=300)
                        line = scatter.transform_regression(x_var, y_var).mark_line(color='red')
                        st.altair_chart(scatter + line, use_container_width=True)
                        
                        st.code(f"Excel: =SLOPE({y_var}:{y_var}, {x_var}:{x_var}), =INTERCEPT({y_var}:{y_var}, {x_var}:{x_var})")
                        st.code(f"Python: scipy.stats.linregress(df['{x_var}'], df['{y_var}'])")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
            else:
                st.warning("Need at least 2 numeric columns for regression analysis.")
        
        elif analysis_type == "ANOVA (Analysis of Variance)":
            st.markdown("**ANOVA** tests if there are significant differences between group means.")
            st.markdown("- Used when comparing means across 2+ groups")
            st.markdown("- Null hypothesis: All group means are equal")
            
            if categorical_cols and numeric_cols:
                col1, col2 = st.columns(2)
                with col1:
                    group_col = st.selectbox("Grouping Variable (categorical):", categorical_cols)
                with col2:
                    value_col = st.selectbox("Value Variable (numeric):", numeric_cols)
                
                if st.button("Run ANOVA", type="primary", icon=":material/query_stats:"):
                    try:
                        groups = [group[value_col].values for name, group in stats_data.groupby(group_col)]
                        
                        if len(groups) >= 2:
                            f_stat, p_value = stats.f_oneway(*groups)
                            
                            st.markdown("### ANOVA Results")
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                st.metric("F-statistic", f"{f_stat:.4f}")
                            with col2:
                                st.metric("P-value", f"{p_value:.4f}")
                            
                            if p_value < 0.05:
                                st.success("**Result:** The differences between groups ARE statistically significant (p < 0.05).")
                                st.markdown("At least one group mean is different from the others.")
                            else:
                                st.info("**Result:** The differences between groups are NOT statistically significant (p >= 0.05).")
                                st.markdown("The group means are not significantly different.")
                            
                            # Show group statistics
                            st.markdown("### Group Statistics")
                            group_stats = stats_data.groupby(group_col)[value_col].agg(['count', 'mean', 'std'])
                            group_stats.columns = ['Count', 'Mean', 'Std Dev']
                            st.dataframe(group_stats, use_container_width=True)
                            
                            # Box plot
                            import altair as alt
                            box = alt.Chart(stats_data).mark_boxplot().encode(
                                x=group_col,
                                y=value_col
                            ).properties(width=600, height=300)
                            st.altair_chart(box, use_container_width=True)
                            
                            st.code("Excel: Data Analysis ToolPak > Anova: Single Factor")
                            st.code(f"Python: scipy.stats.f_oneway(*groups)")
                        else:
                            st.warning("Need at least 2 groups for ANOVA.")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
            else:
                st.warning("Need at least 1 categorical column and 1 numeric column for ANOVA.")
        
        elif analysis_type == "Histogram":
            st.markdown("**Histogram** shows the distribution of values in a dataset.")
            st.markdown("- Helps identify patterns like normal distribution, skewness, outliers")
            
            if numeric_cols:
                hist_col = st.selectbox("Select column:", numeric_cols)
                bins = st.slider("Number of bins:", 5, 30, 10)
                
                if st.button("Create Histogram", type="primary", icon=":material/bar_chart:"):
                    try:
                        import altair as alt
                        
                        hist = alt.Chart(stats_data).mark_bar().encode(
                            alt.X(hist_col, bin=alt.Bin(maxbins=bins)),
                            y='count()'
                        ).properties(width=600, height=300)
                        st.altair_chart(hist, use_container_width=True)
                        
                        # Distribution statistics
                        st.markdown("### Distribution Statistics")
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Mean", f"{stats_data[hist_col].mean():.2f}")
                        with col2:
                            st.metric("Median", f"{stats_data[hist_col].median():.2f}")
                        with col3:
                            st.metric("Std Dev", f"{stats_data[hist_col].std():.2f}")
                        with col4:
                            skewness = stats.skew(stats_data[hist_col])
                            st.metric("Skewness", f"{skewness:.2f}")
                        
                        # Interpretation
                        if abs(skewness) < 0.5:
                            st.success("The distribution is approximately **symmetric** (normal-like).")
                        elif skewness > 0:
                            st.info("The distribution is **right-skewed** (tail extends to the right).")
                        else:
                            st.info("The distribution is **left-skewed** (tail extends to the left).")
                        
                        st.code("Excel: Insert > Charts > Histogram")
                        st.code(f"Python: plt.hist(df['{hist_col}'], bins={bins})")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
            else:
                st.warning("Need at least 1 numeric column for histogram.")
        
        elif analysis_type == "Covariance Analysis":
            st.markdown("**Covariance** measures how two variables change together.")
            st.markdown("- Positive: variables increase together")
            st.markdown("- Negative: one increases as other decreases")
            st.markdown("- Unlike correlation, covariance is not standardized (affected by scale)")
            
            if len(numeric_cols) >= 2:
                if st.button("Calculate Covariance Matrix", type="primary", icon=":material/table_chart:"):
                    try:
                        cov_matrix = stats_data[numeric_cols].cov()
                        
                        st.markdown("### Covariance Matrix")
                        st.dataframe(cov_matrix.round(2), use_container_width=True)
                        
                        st.markdown("### Interpretation")
                        st.markdown("- **Diagonal values**: Variance of each variable")
                        st.markdown("- **Off-diagonal values**: Covariance between variable pairs")
                        st.markdown("- Larger absolute values = stronger relationship")
                        
                        # Also show correlation for comparison
                        st.markdown("### Correlation Matrix (for comparison)")
                        corr_matrix = stats_data[numeric_cols].corr()
                        st.dataframe(corr_matrix.round(2), use_container_width=True)
                        
                        st.info("Correlation is preferred when comparing relationships because it's standardized (-1 to +1).")
                        
                        st.code("Excel: Data Analysis ToolPak > Covariance")
                        st.code("Python: df.cov()")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
            else:
                st.warning("Need at least 2 numeric columns for covariance analysis.")
        
        elif analysis_type == "Descriptive Statistics":
            st.markdown("**Descriptive Statistics** summarize the main features of a dataset.")
            
            if numeric_cols:
                selected_col = st.selectbox("Select column:", numeric_cols)
                
                if st.button("Calculate Statistics", type="primary", icon=":material/calculate:"):
                    try:
                        data = stats_data[selected_col]
                        
                        st.markdown("### Summary Statistics")
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Count", f"{len(data)}")
                            st.metric("Mean", f"{data.mean():.2f}")
                            st.metric("Median", f"{data.median():.2f}")
                        with col2:
                            st.metric("Std Dev", f"{data.std():.2f}")
                            st.metric("Variance", f"{data.var():.2f}")
                            st.metric("Range", f"{data.max() - data.min():.2f}")
                        with col3:
                            st.metric("Min", f"{data.min():.2f}")
                            st.metric("Max", f"{data.max():.2f}")
                            st.metric("Sum", f"{data.sum():.2f}")
                        
                        st.markdown("### Quartiles")
                        q1 = data.quantile(0.25)
                        q2 = data.quantile(0.50)
                        q3 = data.quantile(0.75)
                        iqr = q3 - q1
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Q1 (25%)", f"{q1:.2f}")
                        with col2:
                            st.metric("Q2 (50%)", f"{q2:.2f}")
                        with col3:
                            st.metric("Q3 (75%)", f"{q3:.2f}")
                        with col4:
                            st.metric("IQR", f"{iqr:.2f}")
                        
                        st.code("Excel: =AVERAGE(), =MEDIAN(), =STDEV(), =QUARTILE()")
                        st.code(f"Python: df['{selected_col}'].describe()")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
            else:
                st.warning("Need at least 1 numeric column.")
    
    elif playground_tab == "Power Query Simulator":
        mui_subheader("bolt", "Power Query Simulator")
        st.markdown("Learn data transformation steps like Power Query in Excel!")
        
        # Sample raw data that needs cleaning
        if 'pq_raw_data' not in st.session_state:
            st.session_state.pq_raw_data = pd.DataFrame({
                'Date': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19'],
                'Product_ID': ['P001', 'P002', 'P001', 'P003', 'P002'],
                'Sales_Amount': [1500, 2200, 1800, None, 2500],
                'Region': ['north', 'SOUTH', 'North', 'east', 'South'],
                'Notes': ['  Good sale  ', 'Discount applied', None, 'New customer', 'Repeat']
            })
        
        st.markdown("### Source Data")
        st.caption("This data has common issues: missing values, inconsistent casing, extra spaces")
        st.session_state.pq_raw_data = st.data_editor(
            st.session_state.pq_raw_data,
            num_rows="dynamic",
            use_container_width=True,
            key="pq_raw_editor"
        )
        
        raw_data = st.session_state.pq_raw_data.copy()
        
        st.markdown("### Transform Steps")
        st.markdown("Select transformations to apply (like Power Query steps):")
        
        transformations = {
            "Remove Duplicates": st.checkbox("Remove Duplicates", key="pq_dedup"),
            "Fill Missing Values": st.checkbox("Fill Missing Values (with 0 or 'Unknown')", key="pq_fill"),
            "Standardize Text Case": st.checkbox("Standardize Text Case (Title Case)", key="pq_case"),
            "Trim Whitespace": st.checkbox("Trim Whitespace", key="pq_trim"),
            "Filter Rows": st.checkbox("Filter Rows (remove nulls)", key="pq_filter"),
            "Add Calculated Column": st.checkbox("Add Calculated Column", key="pq_calc"),
            "Change Data Types": st.checkbox("Convert Date Column", key="pq_types"),
            "Sort Data": st.checkbox("Sort Data", key="pq_sort")
        }
        
        if st.button("Apply Transformations", type="primary", icon=":material/auto_fix_high:"):
            transformed = raw_data.copy()
            steps_applied = []
            
            try:
                if transformations["Remove Duplicates"]:
                    before = len(transformed)
                    transformed = transformed.drop_duplicates()
                    steps_applied.append(f"Removed Duplicates: {before - len(transformed)} rows removed")
                
                if transformations["Fill Missing Values"]:
                    for col in transformed.columns:
                        if transformed[col].dtype == 'object':
                            transformed[col] = transformed[col].fillna('Unknown')
                        else:
                            transformed[col] = transformed[col].fillna(0)
                    steps_applied.append("Filled Missing Values: Numeric with 0, Text with 'Unknown'")
                
                if transformations["Standardize Text Case"]:
                    for col in transformed.select_dtypes(include=['object']).columns:
                        transformed[col] = transformed[col].astype(str).str.title()
                    steps_applied.append("Standardized Text Case: Converted to Title Case")
                
                if transformations["Trim Whitespace"]:
                    for col in transformed.select_dtypes(include=['object']).columns:
                        transformed[col] = transformed[col].astype(str).str.strip()
                    steps_applied.append("Trimmed Whitespace: Removed leading/trailing spaces")
                
                if transformations["Filter Rows"]:
                    before = len(transformed)
                    transformed = transformed.dropna()
                    steps_applied.append(f"Filtered Rows: Removed {before - len(transformed)} rows with nulls")
                
                if transformations["Add Calculated Column"]:
                    if 'Sales_Amount' in transformed.columns:
                        transformed['Sales_Amount'] = pd.to_numeric(transformed['Sales_Amount'], errors='coerce')
                        transformed['Tax_10pct'] = transformed['Sales_Amount'] * 0.10
                        steps_applied.append("Added Column: Tax_10pct = Sales_Amount * 0.10")
                
                if transformations["Change Data Types"]:
                    if 'Date' in transformed.columns:
                        transformed['Date'] = pd.to_datetime(transformed['Date'], errors='coerce')
                        transformed['Year'] = transformed['Date'].dt.year
                        transformed['Month'] = transformed['Date'].dt.month
                        steps_applied.append("Changed Types: Converted Date, extracted Year and Month")
                
                if transformations["Sort Data"]:
                    if 'Date' in transformed.columns:
                        transformed = transformed.sort_values('Date')
                        steps_applied.append("Sorted Data: By Date ascending")
                
                st.markdown("### Applied Steps")
                for i, step in enumerate(steps_applied, 1):
                    st.markdown(f"{i}. {step}")
                
                if not steps_applied:
                    st.info("No transformations selected. Check some boxes above.")
                else:
                    st.markdown("### Transformed Data")
                    st.dataframe(transformed, use_container_width=True)
                    
                    st.markdown("### Power Query M Code Equivalent")
                    m_code = "let\n    Source = Excel.CurrentWorkbook(){[Name=\"Table1\"]}[Content],\n"
                    for step in steps_applied:
                        m_code += f"    // {step}\n"
                    m_code += "    Result = Source\nin\n    Result"
                    st.code(m_code, language="text")
                    
            except Exception as e:
                st.error(f"Error: {str(e)}")
        
        st.markdown("---")
        st.markdown("**Common Power Query Operations:**")
        st.markdown("- **Remove Duplicates**: Eliminate duplicate rows")
        st.markdown("- **Fill Down/Replace Nulls**: Handle missing data")
        st.markdown("- **Change Type**: Convert data types")
        st.markdown("- **Split/Merge Columns**: Restructure data")
        st.markdown("- **Pivot/Unpivot**: Reshape tables")
        st.markdown("- **Group By**: Aggregate data")

    elif playground_tab == "Excel Workbook Profiler":
        mui_subheader("upload_file", "Excel Workbook Profiler")
        st.markdown("Upload any Excel file to quickly profile important data by sheet.")

        with st.expander("Tutor Solution: Why this solves the task and how to use it in exams", expanded=False):
            st.markdown("""
### Why this solves the task
In spreadsheet and data-analysis tasks, the **first requirement** is usually to understand data quality and structure before transforming anything.

This profiler solves that by giving you immediate answers to the core diagnostic questions:
- **Scope:** How many sheets, rows, and columns are included?
- **Quality:** Are there missing values or duplicate rows?
- **Structure:** What are the data types, and which columns are numeric/text/date?
- **Signal:** What are the key distributions and top categories?

Because these checks are automatic and repeatable, you avoid manual mistakes and can justify each cleaning decision with evidence.

### Real tutor method (exam-ready)
Use this sequence in every exam dataset:
1. **Profile raw input** (before changes).
2. **Document findings** (missing values, duplicates, weak columns, candidate keys).
3. **Export evidence** (`Summary CSV` + `JSON Profile`) for your appendix/report.
4. **Transform data** in Excel/Power Query/Python.
5. **Re-profile final output** and compare before vs after.

### How to explain this in your report/defense
Use a short structure:
- **Task:** â€œAssess and prepare the workbook for analysis.â€
- **Method:** â€œI ran a standardized workbook profile to evaluate structure and data quality.â€
- **Evidence:** â€œProfile results showed X missing values, Y duplicates, and Z relevant numeric fields.â€
- **Action:** â€œBased on this, I cleaned nulls, removed duplicates, standardized types, and exported analysis-ready data.â€
- **Result:** â€œThe final dataset is consistent, traceable, and ready for statistical/visual analysis.â€

### Why teachers like this approach
- It is **systematic** (same method every time).
- It is **auditable** (exports prove what you found).
- It is **professional** (mirrors real ETL/data quality workflow).
""")

        uploaded_workbook = st.file_uploader(
            "Upload workbook (.xlsx, .xlsm, .xls)",
            type=["xlsx", "xlsm", "xls"],
            key="excel_profiler_upload"
        )

        if uploaded_workbook:
            try:
                workbook = pd.ExcelFile(uploaded_workbook)
                st.success(f"Loaded workbook with {len(workbook.sheet_names)} sheet(s)")

                col1, col2 = st.columns(2)
                with col1:
                    selected_sheet = st.selectbox(
                        "Sheet to profile:",
                        options=["All sheets"] + workbook.sheet_names,
                        key="excel_profiler_sheet"
                    )
                with col2:
                    top_n = st.slider("Top values per text column:", 1, 10, 3, key="excel_profiler_topn")

                if st.button("Analyze Workbook", type="primary", icon=":material/analytics:"):
                    target_sheets = workbook.sheet_names if selected_sheet == "All sheets" else [selected_sheet]
                    summary_rows = []
                    sheet_profiles = []
                    sheet_details = {}

                    for sheet_name in target_sheets:
                        df = pd.read_excel(workbook, sheet_name=sheet_name)

                        row_count = int(df.shape[0])
                        col_count = int(df.shape[1])
                        missing_cells = int(df.isna().sum().sum()) if col_count > 0 else 0
                        total_cells = int(row_count * col_count) if row_count > 0 and col_count > 0 else 0
                        missing_pct = (missing_cells / total_cells * 100) if total_cells > 0 else 0.0
                        duplicate_rows = int(df.duplicated().sum()) if row_count > 0 else 0
                        duplicate_pct = (duplicate_rows / row_count * 100) if row_count > 0 else 0.0
                        numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
                        datetime_cols = df.select_dtypes(include=["datetime", "datetimetz"]).columns.tolist()
                        object_cols = df.select_dtypes(include=["object"]).columns.tolist()

                        candidate_keys = []
                        constant_cols = []
                        if row_count > 0:
                            for col in df.columns:
                                missing_count = int(df[col].isna().sum())
                                unique_count = int(df[col].nunique(dropna=True))
                                if missing_count == 0 and unique_count == row_count:
                                    candidate_keys.append(col)
                                if unique_count <= 1:
                                    constant_cols.append(col)

                        date_like_cols = []
                        for col in object_cols:
                            series = df[col].dropna().astype(str).str.strip()
                            if len(series) >= 3:
                                parsed = pd.to_datetime(series, errors="coerce")
                                parse_ratio = float(parsed.notna().mean())
                                if parse_ratio >= 0.80:
                                    date_like_cols.append({
                                        "column": col,
                                        "parse_success_pct": round(parse_ratio * 100, 1)
                                    })

                        outlier_rows = []
                        outlier_total = 0
                        for col in numeric_cols:
                            series = pd.to_numeric(df[col], errors="coerce").dropna()
                            if len(series) < 4:
                                continue
                            q1 = series.quantile(0.25)
                            q3 = series.quantile(0.75)
                            iqr = q3 - q1
                            if pd.isna(iqr) or iqr <= 0:
                                continue
                            lower_bound = q1 - 1.5 * iqr
                            upper_bound = q3 + 1.5 * iqr
                            outlier_count = int(((series < lower_bound) | (series > upper_bound)).sum())
                            if outlier_count > 0:
                                outlier_rows.append({
                                    "Column": col,
                                    "Outlier Count": outlier_count,
                                    "Outlier %": round((outlier_count / len(series)) * 100, 2),
                                    "Lower Bound": round(float(lower_bound), 4),
                                    "Upper Bound": round(float(upper_bound), 4)
                                })
                            outlier_total += outlier_count

                        outlier_pct = 0.0
                        if row_count > 0 and len(numeric_cols) > 0:
                            outlier_pct = (outlier_total / (row_count * len(numeric_cols))) * 100

                        penalty_missing = min(35.0, missing_pct * 0.7)
                        penalty_duplicates = min(20.0, duplicate_pct)
                        penalty_constant = min(10.0, len(constant_cols) * 2.0)
                        penalty_outliers = min(15.0, outlier_pct * 0.6)
                        penalty_no_key = 8.0 if row_count > 0 and col_count > 1 and len(candidate_keys) == 0 else 0.0
                        quality_score = max(
                            0.0,
                            round(100.0 - (penalty_missing + penalty_duplicates + penalty_constant + penalty_outliers + penalty_no_key), 1)
                        )

                        if quality_score >= 85:
                            quality_band = "Strong"
                        elif quality_score >= 65:
                            quality_band = "Needs cleanup"
                        else:
                            quality_band = "High risk"

                        issues = []
                        recommendations = []

                        if missing_pct > 0:
                            issues.append(f"Missing values: {missing_cells} cells ({missing_pct:.2f}%).")
                            recommendations.append("Handle missing values column-by-column (impute, fill forward, or remove where justified).")
                        if duplicate_rows > 0:
                            issues.append(f"Duplicate rows: {duplicate_rows} rows ({duplicate_pct:.2f}%).")
                            recommendations.append("Remove duplicates using business keys and keep the most recent valid record.")
                        if len(constant_cols) > 0:
                            issues.append(f"Low-information columns: {', '.join(constant_cols[:5])}{'...' if len(constant_cols) > 5 else ''}.")
                            recommendations.append("Drop or ignore constant columns before modeling/visualization.")
                        if len(candidate_keys) == 0 and row_count > 0 and col_count > 1:
                            issues.append("No single-column candidate key detected.")
                            recommendations.append("Create or validate a composite key to prevent duplicate business records.")
                        if len(date_like_cols) > 0:
                            cols_txt = ", ".join([item["column"] for item in date_like_cols[:5]])
                            issues.append(f"Date-like text columns detected: {cols_txt}{'...' if len(date_like_cols) > 5 else ''}.")
                            recommendations.append("Convert date-like text to true datetime types before trend analysis.")
                        if outlier_total > 0:
                            issues.append(f"Potential numeric outliers detected: {outlier_total} values by IQR rule.")
                            recommendations.append("Review outliers with domain context before removing or capping values.")
                        if not issues:
                            issues.append("No major quality issues detected by automated checks.")
                            recommendations.append("Proceed to analysis, then re-profile after transformations to confirm consistency.")

                        summary_rows.append({
                            "Sheet": sheet_name,
                            "Rows": row_count,
                            "Columns": col_count,
                            "Missing Cells": missing_cells,
                            "Missing %": round(missing_pct, 2),
                            "Duplicate Rows": duplicate_rows,
                            "Duplicate %": round(duplicate_pct, 2),
                            "Numeric Columns": len(numeric_cols),
                            "Datetime Columns": len(datetime_cols),
                            "Candidate Keys": len(candidate_keys),
                            "Quality Score": quality_score,
                            "Quality Band": quality_band
                        })

                        numeric_summary_records = []
                        if numeric_cols:
                            numeric_summary_records = (
                                df[numeric_cols]
                                .describe()
                                .transpose()
                                [["count", "mean", "std", "min", "max"]]
                                .reset_index()
                                .rename(columns={"index": "Column"})
                                .to_dict(orient="records")
                            )

                        text_top_values = {}
                        text_cols = [
                            col for col in df.columns
                            if df[col].dtype == "object" and df[col].nunique(dropna=True) <= 50
                        ]
                        for col in text_cols:
                            top_values = (
                                df[col]
                                .dropna()
                                .astype(str)
                                .value_counts()
                                .head(top_n)
                                .reset_index()
                            )
                            top_values.columns = ["Value", "Count"]
                            text_top_values[col] = top_values.to_dict(orient="records")

                        column_profile_df = pd.DataFrame({
                            "Column": df.columns,
                            "Type": [str(df[col].dtype) for col in df.columns],
                            "Missing": [int(df[col].isna().sum()) for col in df.columns],
                            "Missing %": [round((df[col].isna().sum() / row_count) * 100, 2) if row_count > 0 else 0.0 for col in df.columns],
                            "Unique": [int(df[col].nunique(dropna=True)) for col in df.columns]
                        })

                        outlier_df = pd.DataFrame(outlier_rows) if outlier_rows else pd.DataFrame(columns=["Column", "Outlier Count", "Outlier %", "Lower Bound", "Upper Bound"])

                        sheet_profiles.append({
                            "sheet": sheet_name,
                            "rows": row_count,
                            "columns": col_count,
                            "missing_cells": missing_cells,
                            "missing_pct": round(missing_pct, 2),
                            "duplicate_rows": duplicate_rows,
                            "duplicate_pct": round(duplicate_pct, 2),
                            "candidate_keys": candidate_keys,
                            "quality_score": quality_score,
                            "quality_band": quality_band,
                            "issues": issues,
                            "recommendations": recommendations,
                            "date_like_columns": date_like_cols,
                            "column_profile": column_profile_df.to_dict(orient="records"),
                            "numeric_summary": numeric_summary_records,
                            "outlier_summary": outlier_rows,
                            "text_top_values": text_top_values,
                            "preview": df.head(10).to_dict(orient="records")
                        })

                        sheet_details[sheet_name] = {
                            "dataframe": df,
                            "column_profile": column_profile_df,
                            "numeric_cols": numeric_cols,
                            "text_cols": text_cols,
                            "outlier_df": outlier_df,
                            "numeric_summary_df": pd.DataFrame(numeric_summary_records),
                            "issues": issues,
                            "recommendations": recommendations,
                            "candidate_keys": candidate_keys,
                            "date_like_cols": date_like_cols,
                            "quality_score": quality_score,
                            "quality_band": quality_band,
                            "missing_cells": missing_cells,
                            "missing_pct": missing_pct,
                            "duplicate_rows": duplicate_rows,
                            "duplicate_pct": duplicate_pct
                        }

                    st.markdown("### Workbook Summary")
                    summary_df = pd.DataFrame(summary_rows)
                    st.dataframe(summary_df, use_container_width=True, hide_index=True)

                    avg_quality = round(float(summary_df["Quality Score"].mean()), 1) if not summary_df.empty else 0.0
                    high_risk_sheets = int((summary_df["Quality Band"] == "High risk").sum()) if not summary_df.empty else 0
                    sheets_with_issues = int((summary_df["Missing Cells"] > 0).sum() + (summary_df["Duplicate Rows"] > 0).sum()) if not summary_df.empty else 0
                    col_a, col_b, col_c = st.columns(3)
                    with col_a:
                        st.metric("Average Quality Score", f"{avg_quality}")
                    with col_b:
                        st.metric("High Risk Sheets", f"{high_risk_sheets}")
                    with col_c:
                        st.metric("Sheets with Data Quality Flags", f"{sheets_with_issues}")

                    st.markdown("### Recommended Cleaning Plan")
                    plan_lines = []
                    for row in summary_rows:
                        plan_lines.append(f"- {row['Sheet']}: {row['Quality Band']} (score {row['Quality Score']})")
                    st.markdown("\n".join(plan_lines))

                    summary_csv = summary_df.to_csv(index=False).encode("utf-8")
                    st.download_button(
                        "Download Summary CSV",
                        data=summary_csv,
                        file_name=f"{uploaded_workbook.name.rsplit('.', 1)[0]}_summary.csv",
                        mime="text/csv",
                        icon=":material/download:"
                    )

                    if selected_sheet != "All sheets":
                        sheet_df = sheet_details[selected_sheet]["dataframe"]
                        sheet_csv = sheet_df.to_csv(index=False).encode("utf-8")
                        st.download_button(
                            f"Download {selected_sheet} as CSV",
                            data=sheet_csv,
                            file_name=f"{selected_sheet}.csv",
                            mime="text/csv",
                            icon=":material/table_view:"
                        )

                    profile_payload = {
                        "workbook": uploaded_workbook.name,
                        "generated_at": datetime.utcnow().isoformat(),
                        "sheet_mode": selected_sheet,
                        "top_values_limit": top_n,
                        "summary": summary_rows,
                        "sheets": sheet_profiles
                    }
                    st.download_button(
                        "Download JSON Profile",
                        data=json.dumps(profile_payload, indent=2, default=str),
                        file_name=f"{uploaded_workbook.name.rsplit('.', 1)[0]}_profile.json",
                        mime="application/json",
                        icon=":material/download:"
                    )

                    report_lines = [
                        "# Exam Method Report",
                        "",
                        "## Dataset Overview",
                        f"- Workbook: {uploaded_workbook.name}",
                        f"- Generated at (UTC): {profile_payload['generated_at']}",
                        f"- Sheet mode: {selected_sheet}",
                        f"- Sheets analyzed: {len(target_sheets)}",
                        f"- Average quality score: {avg_quality}",
                        "",
                        "## Method",
                        "1. Imported dataset into workbook profiling workflow.",
                        "2. Evaluated structure (rows, columns, data types, keys).",
                        "3. Evaluated quality (missing values, duplicates, outliers).",
                        "4. Generated evidence exports (CSV + JSON) for traceability.",
                        "5. Prepared sheet-by-sheet cleaning actions before transformation.",
                        "",
                        "## Sheet-by-Sheet Findings"
                    ]

                    for sheet_item in sheet_profiles:
                        report_lines.extend([
                            "",
                            f"### {sheet_item['sheet']}",
                            f"- Rows: {sheet_item['rows']}",
                            f"- Columns: {sheet_item['columns']}",
                            f"- Missing cells: {sheet_item['missing_cells']} ({sheet_item['missing_pct']}%)",
                            f"- Duplicate rows: {sheet_item['duplicate_rows']} ({sheet_item['duplicate_pct']}%)",
                            f"- Quality score: {sheet_item['quality_score']} ({sheet_item['quality_band']})",
                            f"- Candidate keys: {', '.join(sheet_item['candidate_keys']) if sheet_item['candidate_keys'] else 'None detected'}",
                            "- Issues:"
                        ])
                        for issue in sheet_item["issues"]:
                            report_lines.append(f"  - {issue}")
                        report_lines.append("- Recommended actions:")
                        for action in sheet_item["recommendations"]:
                            report_lines.append(f"  - {action}")

                    report_lines.extend([
                        "",
                        "## Assignment Mapping",
                        "- Question 1: Use the profiling evidence to justify Power Query steps (headers, column removal, sort, index, calculated Total Bags Sold, save XLSX, export CSV).",
                        "- Question 2: Repeat the same profiling-to-cleaning method for a self-selected dataset, then document blanks/errors handling and final schema.",
                        "",
                        "## Conclusion",
                        "This method provides a repeatable, auditable workflow that supports both practical execution and exam-grade documentation.",
                        ""
                    ])

                    exam_report_md = "\n".join(report_lines)
                    st.download_button(
                        "Download Exam Method Report (.md)",
                        data=exam_report_md,
                        file_name=f"{uploaded_workbook.name.rsplit('.', 1)[0]}_exam_method_report.md",
                        mime="text/markdown",
                        icon=":material/description:"
                    )

                    with st.expander("Power Query Exam Checklist", expanded=False):
                        st.markdown("Use this checklist directly in the interface while solving Question 1 and Question 2.")

                        st.markdown("**Import & Structure**")
                        st.markdown("- Open new workbook and import source dataset")
                        st.markdown("- Open Power Query Editor")
                        st.markdown("- Promote first row to headers")
                        st.markdown("- Confirm correct data types (Date, Number, Text)")

                        st.markdown("**Cleaning**")
                        st.markdown("- Remove irrelevant columns")
                        st.markdown("- Remove blank rows")
                        st.markdown("- Remove rows with errors")
                        st.markdown("- Remove duplicates (if present)")
                        st.markdown("- Standardize text formatting/casing where needed")

                        st.markdown("**Required Transformations (Avocado Task)**")
                        st.markdown("- Remove: Index, Total Volume, 4046, 4225, 4770, Total Bags, Type, Year, Region")
                        st.markdown("- Sort by Date")
                        st.markdown("- Add Index column starting from 1 and move to column A")
                        st.markdown("- Add Total Bags Sold = Small Bags + Large Bags + XLarge Bags")

                        st.markdown("**Output**")
                        st.markdown("- Close & Load transformed table into Excel")
                        st.markdown("- Save as .XLSX")
                        st.markdown("- Export .CSV")
                        st.markdown("- Keep evidence files (summary/profile/report) for appendix")

                        st.markdown("**Defense Script (Short)**")
                        st.markdown("- Task: What was required?")
                        st.markdown("- Method: What transformation sequence was used?")
                        st.markdown("- Evidence: What quality issues were found and fixed?")
                        st.markdown("- Result: Why the final dataset is analysis-ready")

                    for sheet_name in target_sheets:
                        df = sheet_details[sheet_name]["dataframe"]
                        details = sheet_details[sheet_name]
                        with st.expander(f"Details: {sheet_name}", expanded=(len(target_sheets) == 1)):
                            if df.empty and len(df.columns) == 0:
                                st.info("This sheet is empty.")
                                continue

                            m1, m2, m3, m4 = st.columns(4)
                            with m1:
                                st.metric("Quality Score", f"{details['quality_score']}")
                            with m2:
                                st.metric("Missing Cells", f"{details['missing_cells']}")
                            with m3:
                                st.metric("Duplicate Rows", f"{details['duplicate_rows']}")
                            with m4:
                                st.metric("Quality Band", details["quality_band"])

                            st.markdown("**Detected issues**")
                            for issue in details["issues"]:
                                if issue.startswith("No major quality issues"):
                                    st.success(issue)
                                else:
                                    st.warning(issue)

                            st.markdown("**Recommended next actions**")
                            for i, action in enumerate(details["recommendations"], start=1):
                                st.markdown(f"{i}. {action}")

                            if details["candidate_keys"]:
                                st.info(f"Candidate key columns: {', '.join(details['candidate_keys'])}")
                            else:
                                st.info("No single-column candidate key identified automatically.")

                            if details["date_like_cols"]:
                                date_like_df = pd.DataFrame(details["date_like_cols"])
                                st.markdown("**Date-like text columns (convert to datetime)**")
                                st.dataframe(date_like_df, use_container_width=True, hide_index=True)

                            st.markdown("**Column profile**")
                            col_profile = details["column_profile"]
                            st.dataframe(col_profile, use_container_width=True, hide_index=True)

                            numeric_cols = details["numeric_cols"]
                            if numeric_cols and not details["numeric_summary_df"].empty:
                                st.markdown("**Numeric summary**")
                                numeric_summary = details["numeric_summary_df"]
                                st.dataframe(numeric_summary, use_container_width=True, hide_index=True)

                            if not details["outlier_df"].empty:
                                st.markdown("**Outlier summary (IQR method)**")
                                st.dataframe(details["outlier_df"], use_container_width=True, hide_index=True)

                            text_cols = details["text_cols"]
                            if text_cols:
                                st.markdown("**Top values in text columns**")
                                for col in text_cols:
                                    top_values = (
                                        df[col]
                                        .dropna()
                                        .astype(str)
                                        .value_counts()
                                        .head(top_n)
                                        .reset_index()
                                    )
                                    top_values.columns = ["Value", "Count"]
                                    st.markdown(f"- {col}")
                                    st.dataframe(top_values, use_container_width=True, hide_index=True)

                            st.markdown("**Preview**")
                            st.dataframe(df.head(10), use_container_width=True)

            except Exception as exc:
                st.error(f"Could not read workbook: {str(exc)}")
        else:
            st.caption("Upload an Excel file to begin profiling.")
    
    elif playground_tab == "Z-Score & Outlier Tool":
        mui_subheader("straighten", "Z-Score & Outlier Detection")
        st.markdown("Calculate z-scores and identify outliers in your data!")
        
        from scipy import stats
        import numpy as np
        
        st.markdown("### What is a Z-Score?")
        st.markdown("""
        - **Z-score** measures how many standard deviations a value is from the mean
        - Formula: z = (x - mean) / standard_deviation
        - Values with |z| > 2 or 3 are often considered outliers
        """)
        
        # Sample data with potential outliers
        if 'zscore_data' not in st.session_state:
            st.session_state.zscore_data = pd.DataFrame({
                'Employee': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry', 'Ivy', 'Jack'],
                'Salary': [52000, 55000, 48000, 150000, 53000, 51000, 54000, 49000, 56000, 52000],
                'Performance': [85, 78, 92, 88, 15, 82, 79, 91, 84, 80],
                'Hours_Worked': [42, 45, 40, 38, 80, 41, 43, 39, 44, 42]
            })
        
        st.markdown("### Your Data (Edit to add your own!)")
        st.caption("This sample data contains some outliers. Can you spot them?")
        st.session_state.zscore_data = st.data_editor(
            st.session_state.zscore_data,
            num_rows="dynamic",
            use_container_width=True,
            key="zscore_data_editor"
        )
        
        zscore_data = st.session_state.zscore_data
        numeric_cols = zscore_data.select_dtypes(include=['number']).columns.tolist()
        
        if numeric_cols:
            col1, col2 = st.columns(2)
            with col1:
                selected_col = st.selectbox("Select column to analyze:", numeric_cols)
            with col2:
                threshold = st.slider("Z-score threshold for outliers:", 1.0, 4.0, 2.0, 0.5)
            
            if st.button("Calculate Z-Scores", type="primary", icon=":material/calculate:"):
                try:
                    data = zscore_data[selected_col]
                    mean = data.mean()
                    std = data.std()
                    
                    # Calculate z-scores
                    z_scores = (data - mean) / std
                    
                    # Create results dataframe
                    results = zscore_data.copy()
                    results['Z_Score'] = z_scores.round(2)
                    results['Is_Outlier'] = abs(z_scores) > threshold
                    
                    st.markdown("### Statistics")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Mean", f"{mean:.2f}")
                    with col2:
                        st.metric("Std Dev", f"{std:.2f}")
                    with col3:
                        outlier_count = results['Is_Outlier'].sum()
                        st.metric("Outliers Found", f"{outlier_count}")
                    
                    st.markdown("### Results with Z-Scores")
                    
                    # Highlight outliers
                    def highlight_outliers(row):
                        if row['Is_Outlier']:
                            return ['background-color: #ffcccc'] * len(row)
                        return [''] * len(row)
                    
                    styled = results.style.apply(highlight_outliers, axis=1)
                    st.dataframe(results, use_container_width=True)
                    
                    if outlier_count > 0:
                        st.markdown("### Identified Outliers")
                        outliers = results[results['Is_Outlier']]
                        st.dataframe(outliers, use_container_width=True)
                        
                        st.warning(f"Found {outlier_count} outlier(s) with |z-score| > {threshold}")
                        
                        # Show how to handle outliers
                        st.markdown("### How to Handle Outliers")
                        handling = st.selectbox(
                            "Choose handling method:",
                            ["View only", "Remove outliers", "Cap outliers (Winsorization)", "Replace with mean"]
                        )
                        
                        if handling == "Remove outliers":
                            cleaned = zscore_data[abs(z_scores) <= threshold]
                            st.markdown("**Data with outliers removed:**")
                            st.dataframe(cleaned, use_container_width=True)
                            st.info(f"Removed {len(zscore_data) - len(cleaned)} rows")
                        
                        elif handling == "Cap outliers (Winsorization)":
                            lower_bound = mean - threshold * std
                            upper_bound = mean + threshold * std
                            capped = zscore_data.copy()
                            capped[selected_col] = capped[selected_col].clip(lower_bound, upper_bound)
                            st.markdown("**Data with outliers capped:**")
                            st.dataframe(capped, use_container_width=True)
                            st.info(f"Values capped to range [{lower_bound:.2f}, {upper_bound:.2f}]")
                        
                        elif handling == "Replace with mean":
                            replaced = zscore_data.copy()
                            replaced.loc[abs(z_scores) > threshold, selected_col] = mean
                            st.markdown("**Data with outliers replaced by mean:**")
                            st.dataframe(replaced, use_container_width=True)
                    else:
                        st.success(f"No outliers found with |z-score| > {threshold}")
                    
                    # Visualization
                    st.markdown("### Z-Score Distribution")
                    import altair as alt
                    
                    z_df = pd.DataFrame({
                        'Value': data,
                        'Z_Score': z_scores,
                        'Is_Outlier': abs(z_scores) > threshold
                    })
                    
                    scatter = alt.Chart(z_df).mark_circle(size=100).encode(
                        x='Value',
                        y='Z_Score',
                        color=alt.condition(
                            alt.datum.Is_Outlier,
                            alt.value('red'),
                            alt.value('steelblue')
                        ),
                        tooltip=['Value', 'Z_Score']
                    ).properties(width=600, height=300)
                    
                    # Add threshold lines
                    rule_upper = alt.Chart(pd.DataFrame({'y': [threshold]})).mark_rule(color='orange', strokeDash=[5,5]).encode(y='y')
                    rule_lower = alt.Chart(pd.DataFrame({'y': [-threshold]})).mark_rule(color='orange', strokeDash=[5,5]).encode(y='y')
                    
                    st.altair_chart(scatter + rule_upper + rule_lower, use_container_width=True)
                    st.caption("Red points are outliers. Orange lines show the threshold.")
                    
                    st.code(f"Excel: =(A2-AVERAGE(A:A))/STDEV(A:A)")
                    st.code(f"Python: scipy.stats.zscore(df['{selected_col}'])")
                    
                except Exception as e:
                    st.error(f"Error: {str(e)}")
        else:
            st.warning("Need at least 1 numeric column.")
    
    elif playground_tab == "Ethical Analysis Critique":
        mui_subheader("gavel", "Ethical Analysis Critique")
        st.markdown("*Practice assessing and critiquing analysis approaches using ethical principles*")
        st.markdown("---")
        
        ethical_scenarios = {
            "Hiring Algorithm Bias": {
                "scenario": """
**Scenario: AI-Powered Hiring Recommendations**

A company has deployed an AI model to screen job applications. The model was trained on 5 years of historical hiring decisions.

**Analysis Approach Used:**
- Model trained on past successful hires
- Features include: education, experience, skills, resume keywords
- 87% accuracy on test data
- No demographic data explicitly used

**Recent Findings:**
- Female candidates rejected at 2x rate of male candidates
- Candidates from certain universities favored regardless of qualifications
- Model seems to prefer resumes with "aggressive" language

**The data team says:** "The model is objective - it's just learning from data."
                """,
                "critique_points": [
                    "Historical bias: Training on past decisions encodes past discrimination",
                    "Proxy discrimination: Non-demographic features may correlate with protected groups",
                    "Outcome fairness: Disparate impact on protected groups requires investigation",
                    "Transparency: Can decisions be explained to rejected candidates?",
                    "Accountability: Who is responsible when the model discriminates?",
                    "Alternative approaches: Blind resume screening, structured interviews, diverse training data"
                ],
                "suggested_alternatives": [
                    "Audit model for demographic disparities before deployment",
                    "Remove features that correlate with protected characteristics",
                    "Use balanced training data or apply fairness constraints",
                    "Implement human review for borderline decisions",
                    "Regularly monitor outcomes for bias after deployment"
                ]
            },
            "Healthcare Risk Scoring": {
                "scenario": """
**Scenario: Patient Risk Prediction**

A hospital uses a predictive model to identify high-risk patients who need extra care.

**Analysis Approach Used:**
- Model predicts risk based on past healthcare costs
- Higher predicted cost = higher risk score
- Used to allocate care management resources

**Problem Discovered:**
- Black patients assigned systematically lower risk scores than equally sick white patients
- Root cause: Using healthcare cost as proxy for health need
- Black patients historically had less access to care, so lower costs
- Result: Sicker Black patients denied extra care resources

**The vendor says:** "We don't use race in the model, so it can't be biased."
                """,
                "critique_points": [
                    "Construct validity: Healthcare cost â‰  health need",
                    "Historical inequity: Past access barriers encoded in data",
                    "Disparate impact: Equal treatment doesn't mean equitable outcomes",
                    "Proxy variables: Cost correlates with race due to systemic factors",
                    "Harm potential: Life-or-death consequences of misallocation",
                    "The 'colorblind' fallacy: Not using race doesn't prevent racial bias"
                ],
                "suggested_alternatives": [
                    "Use actual health outcomes (diagnoses, lab results) instead of costs",
                    "Validate model performance across demographic groups",
                    "Apply equity-aware adjustments to scores",
                    "Involve affected communities in model design",
                    "Regular audits for disparate impact"
                ]
            },
            "Credit Scoring Transparency": {
                "scenario": """
**Scenario: Loan Application Denial**

A bank uses a machine learning model to approve or deny loan applications.

**Analysis Approach Used:**
- Complex neural network with 200+ features
- Very high accuracy (94% on test data)
- Considers: income, employment, credit history, spending patterns, social media activity

**Customer Complaint:**
A customer with good credit history and stable income was denied. When they asked why:
- Bank said: "Our model determined you're high risk"
- Customer asked for specifics: "We can't explain the model's decision"
- Customer discovered neighbors with similar profiles were approved

**The bank says:** "The model is proprietary and we can't reveal how it works."
                """,
                "critique_points": [
                    "Explainability: Customers have right to understand decisions affecting them",
                    "GDPR Article 22: Right to explanation for automated decisions",
                    "Fairness: Similar cases should have similar outcomes",
                    "Social media data: Ethical concerns about using personal data",
                    "Accountability gap: Who is responsible if model is wrong?",
                    "Trust: Unexplainable decisions erode public trust"
                ],
                "suggested_alternatives": [
                    "Use interpretable models (decision trees, logistic regression)",
                    "Implement SHAP/LIME for feature importance explanations",
                    "Provide customers with key factors in their decision",
                    "Remove social media features (privacy concerns)",
                    "Create appeals process with human review"
                ]
            },
            "Predictive Policing": {
                "scenario": """
**Scenario: Crime Prediction Hotspots**

A city police department uses predictive analytics to allocate patrol resources.

**Analysis Approach Used:**
- Model trained on historical arrest data
- Predicts "crime hotspots" for next week
- More officers sent to high-prediction areas

**Observed Feedback Loop:**
- Model sends more officers to historically over-policed areas
- More officers â†’ more arrests (for same crime rates)
- More arrests â†’ model predicts more crime there
- Cycle reinforces itself regardless of actual crime rates

**Police chief says:** "We're just following the data."
                """,
                "critique_points": [
                    "Feedback loops: Predictions become self-fulfilling",
                    "Historical bias: Arrest data reflects policing patterns, not crime rates",
                    "Disparate impact: Certain communities disproportionately affected",
                    "Measurement error: Arrests â‰  crimes committed",
                    "Amplification: Model amplifies existing biases over time",
                    "Community harm: Erodes trust, over-surveillance of communities"
                ],
                "suggested_alternatives": [
                    "Use victimization surveys instead of arrest data",
                    "Include decay factor to break feedback loops",
                    "Monitor and cap predictions for any single area",
                    "Community input on policing priorities",
                    "Regular equity audits of model predictions vs outcomes"
                ]
            }
        }
        
        selected_scenario = st.selectbox("Select a scenario to critique:", list(ethical_scenarios.keys()))
        
        scenario_data = ethical_scenarios[selected_scenario]
        
        st.markdown("### The Scenario")
        st.markdown(scenario_data["scenario"])
        
        st.markdown("---")
        st.markdown("### Your Ethical Critique")
        st.markdown("*Consider: What ethical issues do you see? What principles are violated? What would you recommend?*")
        
        user_critique = st.text_area(
            "Write your critique here:",
            height=200,
            placeholder="Identify ethical issues, violated principles, and suggest alternatives..."
        )
        
        col1, col2 = st.columns(2)
        with col1:
            show_critique = st.button("Show Expert Critique", type="primary", icon=":material/psychology:")
        with col2:
            show_alternatives = st.button("Show Recommended Alternatives")
        
        if show_critique:
            st.markdown("### Expert Critique Points")
            for point in scenario_data["critique_points"]:
                st.markdown(f"- {point}")
        
        if show_alternatives:
            st.markdown("### Recommended Alternatives")
            for alt in scenario_data["suggested_alternatives"]:
                st.markdown(f"âœ“ {alt}")
        
        st.markdown("---")
        st.markdown("### Ethical Principles Checklist")
        
        principles = [
            ("Fairness", "Does the approach treat all groups equitably?"),
            ("Transparency", "Can decisions be explained to affected parties?"),
            ("Accountability", "Is there clear responsibility for outcomes?"),
            ("Privacy", "Is personal data used appropriately?"),
            ("Non-maleficence", "Could this cause harm to individuals or groups?"),
            ("Autonomy", "Do people have meaningful choice about data use?")
        ]
        
        st.markdown("*Rate this analysis approach on each principle:*")
        
        for principle, description in principles:
            col1, col2 = st.columns([1, 3])
            with col1:
                st.selectbox(
                    principle,
                    ["Not assessed", "Serious concern", "Minor concern", "Acceptable", "Good practice"],
                    key=f"eth_{principle}"
                )
            with col2:
                st.caption(description)
    
    elif playground_tab == "Error Detection Workshop":
        mui_subheader("bug_report", "Error Detection Workshop")
        st.markdown("*Practice identifying erroneous data and facilitating solution discussions*")
        st.markdown("---")
        
        if 'error_workshop_data' not in st.session_state:
            st.session_state.error_workshop_data = pd.DataFrame({
                'Customer_ID': ['C001', 'C002', 'C003', 'C004', 'C002', 'C006', 'C007', 'C008'],
                'Name': ['Alice Brown', 'Bob Smith', 'Carol White', 'David Lee', 'Bob Smith', 'Eve Clark', 'Frank Miller', 'Grace Kim'],
                'Age': [28, 35, -5, 42, 35, 150, 31, 29],
                'Email': ['alice@email.com', 'bob@email', 'carol@email.com', '', 'bob@email', 'eve@email.com', 'frank@email.com', 'grace@email.com'],
                'Order_Amount': [150.00, 2500.00, 89.50, 320.00, 2500.00, 175.00, 999999.99, 210.00],
                'Order_Date': ['2024-01-15', '2024-13-01', '2024-02-28', '2024-03-10', '2024-01-15', '2024-04-05', '2024-05-20', '1924-06-15'],
                'Region': ['North', 'South', 'East', 'West', 'South', 'north', 'NORTH', 'East']
            })
        
        st.markdown("### Dataset with Errors")
        st.markdown("*This dataset contains various data quality issues. Can you identify them all?*")
        
        st.dataframe(st.session_state.error_workshop_data, use_container_width=True)
        
        st.markdown("---")
        st.markdown("### Error Identification")
        st.markdown("*Select all the errors you can identify:*")
        
        error_types = {
            "Duplicate records": {
                "hint": "Look for identical Customer_ID and Name combinations",
                "location": "Rows 2 and 5 (C002 - Bob Smith appears twice)",
                "impact": "Double-counting customers, inflated metrics",
                "solution": "Deduplicate based on Customer_ID, keep most recent record"
            },
            "Invalid age values": {
                "hint": "Age should be between 0 and ~120",
                "location": "Row 3: Age=-5, Row 6: Age=150",
                "impact": "Statistical calculations will be skewed, demographic analysis invalid",
                "solution": "Flag for review, set to NULL or impute based on customer segment"
            },
            "Invalid email format": {
                "hint": "Emails should contain @ and a domain extension",
                "location": "Row 2: 'bob@email' (missing .com), Row 4: empty",
                "impact": "Email campaigns will fail, communication gaps",
                "solution": "Validate format, request corrections from customers"
            },
            "Outlier amounts": {
                "hint": "Look for unusually high or low order amounts",
                "location": "Row 7: $999,999.99 is suspiciously high",
                "impact": "Revenue metrics distorted, may indicate data entry error or fraud",
                "solution": "Investigate source, verify with order system, cap or remove if erroneous"
            },
            "Invalid date format": {
                "hint": "Check for impossible dates",
                "location": "Row 2: '2024-13-01' (month 13 doesn't exist)",
                "impact": "Date parsing will fail, time-series analysis broken",
                "solution": "Parse dates strictly, flag failures for manual review"
            },
            "Historical date anomaly": {
                "hint": "Check if dates make business sense",
                "location": "Row 8: '1924-06-15' is 100 years ago",
                "impact": "Time-based analysis will include ancient outliers",
                "solution": "Set business rules for valid date ranges (e.g., company founding date)"
            },
            "Inconsistent region coding": {
                "hint": "Check for case sensitivity issues",
                "location": "'North', 'north', 'NORTH' are all different",
                "impact": "Grouping by region will split what should be one category",
                "solution": "Standardize to consistent case (e.g., Title Case)"
            }
        }
        
        identified_errors = []
        for error_type in error_types.keys():
            if st.checkbox(error_type, key=f"err_{error_type}"):
                identified_errors.append(error_type)
        
        if st.button("Check My Answers", type="primary", icon=":material/task_alt:"):
            score = len(identified_errors)
            total = len(error_types)
            
            if score == total:
                st.success(f"Excellent! You found all {total} error types!")
            elif score >= total * 0.7:
                st.info(f"Good job! You found {score}/{total} error types.")
            else:
                st.warning(f"You found {score}/{total} error types. Keep looking!")
            
            st.markdown("### Detailed Error Analysis")
            
            for error_type, details in error_types.items():
                with st.expander(f"{'âœ“' if error_type in identified_errors else 'âœ—'} {error_type}"):
                    st.markdown(f"**Location:** {details['location']}")
                    st.markdown(f"**Impact:** {details['impact']}")
                    st.markdown(f"**Solution:** {details['solution']}")
        
        st.markdown("---")
        st.markdown("### Facilitating Solution Discussions")
        st.markdown("*Practice communicating data issues to stakeholders*")
        
        discussion_scenario = st.selectbox(
            "Select a discussion scenario:",
            [
                "Presenting errors to business stakeholders",
                "Discussing impact with analytics team",
                "Proposing fixes to data engineering",
                "Escalating critical issues to management"
            ]
        )
        
        discussion_templates = {
            "Presenting errors to business stakeholders": """
**Template: Business Stakeholder Briefing**

1. **Summary Statement** (plain language)
   "We've found some data quality issues that could affect our [X] reports."

2. **Business Impact** (focus on what they care about)
   - "Our customer count may be inflated by X% due to duplicates"
   - "Revenue figures could be off because of data entry errors"

3. **Recommended Actions**
   - "We recommend pausing the report until data is cleaned"
   - "This will take approximately X days to fix"

4. **Ask**
   - "Do you need the approximate numbers now, or can you wait for accurate data?"
            """,
            "Discussing impact with analytics team": """
**Template: Analytics Team Discussion**

1. **Technical Summary**
   "I've identified X types of data quality issues in the customer dataset"

2. **Specific Issues** (with evidence)
   - Issue 1: Duplicates on Customer_ID (X records affected)
   - Issue 2: Invalid values in Age column (Y records affected)

3. **Impact on Analysis**
   - "Demographic segmentation will be skewed"
   - "Time-series trends may show artifacts"

4. **Proposed Validation Rules**
   - Age: 0 < age < 120
   - Email: regex pattern matching
   - Date: within business operation period

5. **Discussion Points**
   - "How should we handle edge cases?"
   - "What's our tolerance for missing data?"
            """,
            "Proposing fixes to data engineering": """
**Template: Data Engineering Request**

1. **Issue Identification**
   "Request to add data validation rules to the ingestion pipeline"

2. **Specific Rules Needed**
   ```
   - Deduplicate on Customer_ID (keep latest)
   - Validate: Age BETWEEN 0 AND 120
   - Validate: Email matches pattern '@.+\\.'
   - Validate: Order_Date > '2020-01-01'
   - Standardize: Region to Title Case
   ```

3. **Priority Justification**
   "These issues affect X% of records and impact downstream reports"

4. **Suggested Implementation**
   - Add validation at ingestion
   - Route failures to quarantine table
   - Alert on failure rate threshold
            """,
            "Escalating critical issues to management": """
**Template: Management Escalation**

1. **Issue Summary** (1-2 sentences)
   "We've discovered data quality issues that affect the accuracy of [critical report/decision]"

2. **Business Risk** (quantified if possible)
   - "Customer metrics may be overstated by X%"
   - "This affects decisions worth $Y"

3. **Current Status**
   - Issue identified on [date]
   - Root cause: [brief explanation]
   - Estimated fix time: [duration]

4. **Decision Required**
   - "Should we pause reporting until fixed?"
   - "What's the acceptable error margin?"

5. **Recommendation**
   "We recommend [specific action] because [reasoning]"
            """
        }
        
        st.markdown(discussion_templates[discussion_scenario])
        
        st.markdown("---")
        st.markdown("### Practice Your Communication")
        user_discussion = st.text_area(
            f"Write your own message for: {discussion_scenario}",
            height=150,
            placeholder="Draft your message to stakeholders..."
        )
        
        if user_discussion and st.button("Get Feedback"):
            st.info("**Tips for effective communication:**\n"
                   "- Lead with impact, not technical details\n"
                   "- Quantify issues when possible\n"
                   "- Always propose solutions, not just problems\n"
                   "- Be clear about what decision or action you need")
    
    elif playground_tab == "Confidence Level Planner":
        mui_subheader("psychology", "Confidence Level Planner")
        st.markdown("*Develop work methods for handling data with different confidence levels*")
        st.markdown("---")
        
        st.markdown("### Understanding Confidence Levels")
        
        confidence_info = pd.DataFrame({
            'Confidence Level': ['99%', '95%', '90%', '80%', '70%'],
            'Z-Score': [2.576, 1.960, 1.645, 1.282, 1.036],
            'Margin of Error (n=100)': ['Â±12.9%', 'Â±9.8%', 'Â±8.2%', 'Â±6.4%', 'Â±5.2%'],
            'Typical Use': ['Medical/safety decisions', 'Academic research', 'Quality control', 'Business planning', 'Quick estimates']
        })
        st.dataframe(confidence_info, use_container_width=True)
        
        st.markdown("---")
        st.markdown("### Confidence Level Decision Framework")
        
        problem_domains = {
            "Medical/Healthcare": {
                "recommended_confidence": "99%",
                "reasoning": "Patient safety is paramount; false negatives could mean missed diagnoses",
                "work_methods": [
                    "Require large sample sizes for clinical decisions",
                    "Always report confidence intervals in results",
                    "Use conservative estimates (lower bound of CI)",
                    "Require peer review before recommendations",
                    "Document uncertainty explicitly in reports"
                ],
                "decision_rules": {
                    "High confidence (>99%)": "Proceed with treatment recommendation",
                    "Medium confidence (95-99%)": "Recommend additional testing",
                    "Low confidence (<95%)": "Do not make clinical recommendations"
                }
            },
            "Financial/Investment": {
                "recommended_confidence": "95%",
                "reasoning": "Balance between action and risk; financial impact can be significant",
                "work_methods": [
                    "Use 95% CI for major investment decisions",
                    "90% acceptable for operational decisions",
                    "Always calculate potential loss at confidence bounds",
                    "Stress test with worst-case scenarios",
                    "Document assumptions and limitations"
                ],
                "decision_rules": {
                    "High confidence (>95%)": "Recommend investment/action",
                    "Medium confidence (90-95%)": "Present options with risk assessment",
                    "Low confidence (<90%)": "Recommend gathering more data"
                }
            },
            "Marketing/Customer Analytics": {
                "recommended_confidence": "90%",
                "reasoning": "Speed often matters; decisions are reversible; cost of error is lower",
                "work_methods": [
                    "Use 90% CI for campaign decisions",
                    "80% acceptable for exploratory analysis",
                    "A/B test with statistical significance checks",
                    "Monitor results and iterate quickly",
                    "Build in review cycles to catch errors"
                ],
                "decision_rules": {
                    "High confidence (>90%)": "Launch campaign/feature",
                    "Medium confidence (80-90%)": "Small-scale pilot test first",
                    "Low confidence (<80%)": "Continue testing or try different approach"
                }
            },
            "Operations/Supply Chain": {
                "recommended_confidence": "95%",
                "reasoning": "Inventory and logistics decisions have cost implications both ways",
                "work_methods": [
                    "Use 95% CI for demand forecasting",
                    "Build safety stock based on confidence bounds",
                    "Monitor forecast accuracy over time",
                    "Adjust confidence requirements by product criticality",
                    "Document service level agreements tied to confidence"
                ],
                "decision_rules": {
                    "High confidence (>95%)": "Set optimal inventory levels",
                    "Medium confidence (90-95%)": "Add safety stock buffer",
                    "Low confidence (<90%)": "Increase safety stock significantly or source alternatives"
                }
            },
            "Research/Academic": {
                "recommended_confidence": "95% (p < 0.05)",
                "reasoning": "Scientific credibility requires rigorous standards",
                "work_methods": [
                    "Pre-register hypotheses before analysis",
                    "Use 95% CI as minimum for publication",
                    "Report exact p-values, not just significance",
                    "Calculate and report effect sizes",
                    "Acknowledge limitations and confidence bounds"
                ],
                "decision_rules": {
                    "Statistically significant (p<0.05)": "Report finding with confidence interval",
                    "Marginally significant (0.05<p<0.10)": "Report as suggestive, needs replication",
                    "Not significant (p>0.10)": "Report null result, discuss sample size"
                }
            }
        }
        
        selected_domain = st.selectbox("Select a problem domain:", list(problem_domains.keys()))
        
        domain_data = problem_domains[selected_domain]
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Recommended Confidence Level", domain_data["recommended_confidence"])
        with col2:
            st.info(f"**Reasoning:** {domain_data['reasoning']}")
        
        st.markdown("### Work Methods for This Domain")
        for method in domain_data["work_methods"]:
            st.markdown(f"â€¢ {method}")
        
        st.markdown("### Decision Rules Based on Confidence")
        for level, action in domain_data["decision_rules"].items():
            st.markdown(f"**{level}:** {action}")
        
        st.markdown("---")
        st.markdown("### Interactive Confidence Calculator")
        
        col1, col2 = st.columns(2)
        with col1:
            sample_size = st.number_input("Sample Size (n):", min_value=10, max_value=10000, value=100)
            sample_mean = st.number_input("Sample Mean:", value=50.0)
            sample_std = st.number_input("Sample Std Dev:", min_value=0.1, value=10.0)
        
        with col2:
            confidence_level = st.selectbox(
                "Confidence Level:",
                ["99%", "95%", "90%", "80%"],
                index=1
            )
            
            z_scores = {"99%": 2.576, "95%": 1.960, "90%": 1.645, "80%": 1.282}
            z = z_scores[confidence_level]
            
            margin_error = z * (sample_std / (sample_size ** 0.5))
            lower_bound = sample_mean - margin_error
            upper_bound = sample_mean + margin_error
        
        st.markdown("### Results")
        
        result_col1, result_col2, result_col3 = st.columns(3)
        with result_col1:
            st.metric("Lower Bound", f"{lower_bound:.2f}")
        with result_col2:
            st.metric("Point Estimate", f"{sample_mean:.2f}")
        with result_col3:
            st.metric("Upper Bound", f"{upper_bound:.2f}")
        
        st.markdown(f"**{confidence_level} Confidence Interval:** [{lower_bound:.2f}, {upper_bound:.2f}]")
        st.markdown(f"**Margin of Error:** Â±{margin_error:.2f} ({(margin_error/sample_mean)*100:.1f}% of mean)")
        
        st.markdown("---")
        st.markdown("### Create Your Work Method Document")
        
        st.markdown("*Based on your domain and confidence requirements, document your work method:*")
        
        work_method_template = st.text_area(
            "Your Work Method Document:",
            value=f"""WORK METHOD: {selected_domain} Analysis

CONFIDENCE STANDARD: {domain_data['recommended_confidence']}

DATA REQUIREMENTS:
- Minimum sample size: [specify based on required precision]
- Data quality checks: [list validation rules]
- Exclusion criteria: [what data to remove]

ANALYSIS PROTOCOL:
1. Calculate point estimate and confidence interval
2. Apply decision rules based on confidence level
3. Document assumptions and limitations
4. Review with [stakeholder/peer]

REPORTING REQUIREMENTS:
- Always report confidence intervals
- Flag results below confidence threshold
- Document sample size and methodology

ESCALATION:
- If confidence < threshold: [specific action]
- If data quality issues: [specific action]
""",
            height=400
        )
        
        if st.button("Save Work Method (Download)", type="primary", icon=":material/save:"):
            st.download_button(
                label="Download Work Method Document",
                data=work_method_template,
                file_name=f"work_method_{selected_domain.lower().replace('/', '_')}.txt",
                mime="text/plain",
                icon=":material/download:"
            )
    
    elif playground_tab == "BI & Big Data Explorer":
        mui_subheader("hub", "BI & Big Data Explorer")
        st.markdown("*Explore Business Intelligence concepts and identify Big Data scenarios*")
        st.markdown("---")
        
        explorer_mode = st.radio(
            "Choose exploration mode:",
            ["BI vs Big Data Classifier", "Data Volume Calculator", "BI Tool Selector"],
            horizontal=True
        )
        
        if explorer_mode == "BI vs Big Data Classifier":
            st.markdown("### Classify Your Data Scenario")
            st.markdown("*Answer these questions to determine if your scenario is BI or Big Data*")
            
            col1, col2 = st.columns(2)
            with col1:
                data_volume = st.select_slider(
                    "Data Volume:",
                    options=["Megabytes", "Gigabytes", "Terabytes", "Petabytes"],
                    value="Gigabytes"
                )
                data_velocity = st.select_slider(
                    "Data Velocity:",
                    options=["Daily updates", "Hourly updates", "Real-time streaming", "Millisecond events"],
                    value="Daily updates"
                )
            with col2:
                data_variety = st.multiselect(
                    "Data Types (Variety):",
                    ["Structured tables", "Text documents", "Images", "Videos", "Sensor data", "Social media"],
                    default=["Structured tables"]
                )
                analysis_goal = st.selectbox(
                    "Primary Goal:",
                    ["Report what happened", "Understand why it happened", "Predict what will happen", "Automate decisions"]
                )
            
            if st.button("Classify Scenario", type="primary", icon=":material/category:"):
                score = 0
                reasons = []
                
                if data_volume in ["Terabytes", "Petabytes"]:
                    score += 2
                    reasons.append(f"High volume ({data_volume})")
                if data_velocity in ["Real-time streaming", "Millisecond events"]:
                    score += 2
                    reasons.append(f"High velocity ({data_velocity})")
                if len(data_variety) > 2:
                    score += 1
                    reasons.append(f"High variety ({len(data_variety)} types)")
                if "Images" in data_variety or "Videos" in data_variety or "Sensor data" in data_variety:
                    score += 1
                    reasons.append("Unstructured data types")
                if analysis_goal in ["Predict what will happen", "Automate decisions"]:
                    score += 1
                    reasons.append(f"Advanced analytics goal")
                
                st.markdown("---")
                if score >= 4:
                    st.error("ðŸ”´ **Big Data Scenario**")
                    st.markdown("Your scenario exhibits Big Data characteristics:")
                    for r in reasons:
                        st.markdown(f"- {r}")
                    st.markdown("**Recommended Tools:** Hadoop, Spark, Python, Cloud Data Platforms")
                elif score >= 2:
                    st.warning("ðŸŸ¡ **Hybrid Scenario**")
                    st.markdown("Your scenario has some Big Data elements but may work with BI tools:")
                    for r in reasons:
                        st.markdown(f"- {r}")
                    st.markdown("**Recommended:** Start with BI tools, scale to Big Data if needed")
                else:
                    st.success("ðŸŸ¢ **Business Intelligence Scenario**")
                    st.markdown("Your scenario is well-suited for traditional BI approaches.")
                    st.markdown("**Recommended Tools:** Excel, Tableau, Power BI, SQL databases")
        
        elif explorer_mode == "Data Volume Calculator":
            st.markdown("### Calculate Your Data Volume")
            
            col1, col2 = st.columns(2)
            with col1:
                records_per_day = st.number_input("Records generated per day:", min_value=1, value=1000)
                bytes_per_record = st.number_input("Average bytes per record:", min_value=1, value=500)
                retention_years = st.number_input("Years of data to retain:", min_value=1, value=3)
            
            with col2:
                daily_bytes = records_per_day * bytes_per_record
                yearly_bytes = daily_bytes * 365
                total_bytes = yearly_bytes * retention_years
                
                def format_bytes(b):
                    if b >= 1e15:
                        return f"{b/1e15:.2f} PB"
                    elif b >= 1e12:
                        return f"{b/1e12:.2f} TB"
                    elif b >= 1e9:
                        return f"{b/1e9:.2f} GB"
                    elif b >= 1e6:
                        return f"{b/1e6:.2f} MB"
                    else:
                        return f"{b/1e3:.2f} KB"
                
                st.metric("Daily Data", format_bytes(daily_bytes))
                st.metric("Yearly Data", format_bytes(yearly_bytes))
                st.metric(f"Total ({retention_years} years)", format_bytes(total_bytes))
                
                if total_bytes >= 1e12:
                    st.warning("This volume may require Big Data infrastructure")
                else:
                    st.success("âœ“ This volume is manageable with traditional BI tools")
        
        elif explorer_mode == "BI Tool Selector":
            st.markdown("### Find the Right BI Tool")
            
            user_needs = st.multiselect(
                "What do you need to do?",
                ["Create dashboards", "Ad-hoc analysis", "Share reports", "Self-service analytics",
                 "Advanced calculations", "Real-time monitoring", "Mobile access", "Embedded analytics"]
            )
            
            budget = st.radio("Budget:", ["Free/Low cost", "Medium", "Enterprise"], horizontal=True)
            tech_level = st.radio("Technical level:", ["Beginner", "Intermediate", "Advanced"], horizontal=True)
            
            if st.button("Get Recommendations", icon=":material/lightbulb:"):
                st.markdown("### Recommended Tools")
                
                recommendations = []
                
                if budget == "Free/Low cost" and tech_level == "Beginner":
                    recommendations.append(("Google Sheets + Looker Studio", "Free, easy to use, good for basic dashboards"))
                    recommendations.append(("Excel + Power BI (free tier)", "Familiar interface, powerful calculations"))
                
                if "Create dashboards" in user_needs:
                    recommendations.append(("Tableau Public", "Beautiful visualizations, free for public data"))
                    recommendations.append(("Power BI Desktop", "Free, integrates with Microsoft ecosystem"))
                
                if "Advanced calculations" in user_needs and tech_level in ["Intermediate", "Advanced"]:
                    recommendations.append(("Python + Jupyter", "Maximum flexibility, requires coding"))
                
                if budget == "Enterprise":
                    recommendations.append(("Tableau Server/Cloud", "Full-featured, enterprise security"))
                    recommendations.append(("Power BI Premium", "Microsoft integration, AI features"))
                
                if not recommendations:
                    recommendations.append(("Excel", "Versatile starting point for any analysis"))
                
                for tool, reason in recommendations:
                    st.markdown(f"**{tool}**")
                    st.caption(reason)
    
    elif playground_tab == "KPI Dashboard Builder":
        mui_subheader("dashboard", "KPI Dashboard Builder")
        st.markdown("*Design and visualize Key Performance Indicators for different business functions*")
        st.markdown("---")
        
        business_function = st.selectbox(
            "Select business function:",
            ["Sales", "Marketing", "Finance", "Operations", "HR", "Customer Service"]
        )
        
        kpi_templates = {
            "Sales": {
                "kpis": [
                    {"name": "Revenue", "target": 100000, "actual": 0, "unit": "$", "higher_better": True},
                    {"name": "Deals Closed", "target": 50, "actual": 0, "unit": "", "higher_better": True},
                    {"name": "Conversion Rate", "target": 25, "actual": 0, "unit": "%", "higher_better": True},
                    {"name": "Average Deal Size", "target": 2000, "actual": 0, "unit": "$", "higher_better": True},
                    {"name": "Sales Cycle (days)", "target": 30, "actual": 0, "unit": "days", "higher_better": False}
                ]
            },
            "Marketing": {
                "kpis": [
                    {"name": "Website Traffic", "target": 50000, "actual": 0, "unit": "visits", "higher_better": True},
                    {"name": "Lead Generation", "target": 500, "actual": 0, "unit": "leads", "higher_better": True},
                    {"name": "Cost per Lead", "target": 50, "actual": 0, "unit": "$", "higher_better": False},
                    {"name": "Email Open Rate", "target": 25, "actual": 0, "unit": "%", "higher_better": True},
                    {"name": "Social Engagement", "target": 5, "actual": 0, "unit": "%", "higher_better": True}
                ]
            },
            "Finance": {
                "kpis": [
                    {"name": "Gross Margin", "target": 40, "actual": 0, "unit": "%", "higher_better": True},
                    {"name": "Operating Expenses", "target": 50000, "actual": 0, "unit": "$", "higher_better": False},
                    {"name": "Cash Flow", "target": 25000, "actual": 0, "unit": "$", "higher_better": True},
                    {"name": "Accounts Receivable Days", "target": 30, "actual": 0, "unit": "days", "higher_better": False},
                    {"name": "Budget Variance", "target": 5, "actual": 0, "unit": "%", "higher_better": False}
                ]
            },
            "Operations": {
                "kpis": [
                    {"name": "Production Output", "target": 1000, "actual": 0, "unit": "units", "higher_better": True},
                    {"name": "Defect Rate", "target": 2, "actual": 0, "unit": "%", "higher_better": False},
                    {"name": "On-Time Delivery", "target": 95, "actual": 0, "unit": "%", "higher_better": True},
                    {"name": "Inventory Turnover", "target": 6, "actual": 0, "unit": "x", "higher_better": True},
                    {"name": "Equipment Uptime", "target": 98, "actual": 0, "unit": "%", "higher_better": True}
                ]
            },
            "HR": {
                "kpis": [
                    {"name": "Employee Turnover", "target": 10, "actual": 0, "unit": "%", "higher_better": False},
                    {"name": "Time to Hire", "target": 30, "actual": 0, "unit": "days", "higher_better": False},
                    {"name": "Training Hours", "target": 40, "actual": 0, "unit": "hrs", "higher_better": True},
                    {"name": "Employee Satisfaction", "target": 80, "actual": 0, "unit": "%", "higher_better": True},
                    {"name": "Absenteeism Rate", "target": 3, "actual": 0, "unit": "%", "higher_better": False}
                ]
            },
            "Customer Service": {
                "kpis": [
                    {"name": "Customer Satisfaction", "target": 90, "actual": 0, "unit": "%", "higher_better": True},
                    {"name": "First Response Time", "target": 2, "actual": 0, "unit": "hrs", "higher_better": False},
                    {"name": "Resolution Rate", "target": 85, "actual": 0, "unit": "%", "higher_better": True},
                    {"name": "NPS Score", "target": 50, "actual": 0, "unit": "", "higher_better": True},
                    {"name": "Tickets per Agent", "target": 20, "actual": 0, "unit": "", "higher_better": True}
                ]
            }
        }
        
        st.markdown(f"### {business_function} KPIs")
        st.markdown("*Enter your actual values to see RAG status*")
        
        template = kpi_templates[business_function]
        kpi_data = []
        
        for i, kpi in enumerate(template["kpis"]):
            col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
            with col1:
                st.markdown(f"**{kpi['name']}**")
            with col2:
                actual = st.number_input(
                    f"Actual",
                    value=float(kpi["target"] * 0.9),
                    key=f"kpi_{business_function}_{i}",
                    label_visibility="collapsed"
                )
            with col3:
                st.caption(f"Target: {kpi['target']}{kpi['unit']}")
            with col4:
                if kpi["higher_better"]:
                    pct = (actual / kpi["target"]) * 100 if kpi["target"] > 0 else 0
                else:
                    if actual <= 0:
                        pct = 100
                    else:
                        pct = (kpi["target"] / actual) * 100
                
                if pct >= 100:
                    st.success("ðŸŸ¢")
                elif pct >= 80:
                    st.warning("ðŸŸ¡")
                else:
                    st.error("ðŸ”´")
            
            kpi_data.append({"KPI": kpi["name"], "Actual": actual, "Target": kpi["target"], "Achievement": f"{pct:.0f}%"})
        
        st.markdown("---")
        st.markdown("### Dashboard Summary")
        
        df_kpi = pd.DataFrame(kpi_data)
        st.dataframe(df_kpi, use_container_width=True)
        
        st.markdown("**RAG Legend:** ðŸŸ¢ On track (â‰¥100%) | ðŸŸ¡ At risk (80-99%) | ðŸ”´ Off track (<80%)")
    
    elif playground_tab == "Decision Analysis Tool":
        mui_subheader("rule", "Decision Analysis Tool")
        st.markdown("*Practice the four analytics philosophies: Descriptive, Diagnostic, Predictive, Prescriptive*")
        st.markdown("---")
        
        analysis_type = st.radio(
            "Select analysis type:",
            ["Descriptive (What happened?)", "Diagnostic (Why did it happen?)", 
             "Predictive (What will happen?)", "Prescriptive (What should we do?)"],
            horizontal=False
        )
        
        if "Descriptive" in analysis_type:
            st.markdown("### Descriptive Analysis: What Happened?")
            st.markdown("*Summarize historical data to understand past performance*")
            
            if 'decision_data' not in st.session_state:
                st.session_state.decision_data = pd.DataFrame({
                    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],
                    'Sales': [45000, 52000, 48000, 61000, 55000, 67000],
                    'Customers': [120, 135, 128, 152, 145, 168],
                    'Returns': [12, 8, 15, 9, 11, 7]
                })
            
            st.markdown("**Your Data:**")
            st.session_state.decision_data = st.data_editor(
                st.session_state.decision_data,
                num_rows="dynamic",
                use_container_width=True
            )
            
            if st.button("Generate Descriptive Summary", icon=":material/summarize:"):
                data = st.session_state.decision_data
                st.markdown("### Summary Statistics")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Sales", f"${data['Sales'].sum():,}")
                    st.metric("Avg Monthly Sales", f"${data['Sales'].mean():,.0f}")
                with col2:
                    st.metric("Total Customers", f"{data['Customers'].sum():,}")
                    st.metric("Avg Monthly Customers", f"{data['Customers'].mean():.0f}")
                with col3:
                    st.metric("Total Returns", f"{data['Returns'].sum()}")
                    st.metric("Return Rate", f"{(data['Returns'].sum() / data['Sales'].sum()) * 100:.2f}%")
        
        elif "Diagnostic" in analysis_type:
            st.markdown("### Diagnostic Analysis: Why Did It Happen?")
            st.markdown("*Investigate root causes using the 5 Whys technique*")
            
            problem = st.text_input("What problem are you investigating?", "Sales dropped 20% in March")
            
            st.markdown("**Apply the 5 Whys:**")
            why1 = st.text_input("Why #1:", placeholder="Enter first why...")
            why2 = st.text_input("Why #2:", placeholder="Enter second why...")
            why3 = st.text_input("Why #3:", placeholder="Enter third why...")
            why4 = st.text_input("Why #4:", placeholder="Enter fourth why...")
            why5 = st.text_input("Why #5:", placeholder="Enter fifth why (root cause)...")
            
            if why5:
                st.markdown("---")
                st.success(f"**Root Cause Identified:** {why5}")
                st.markdown("**Next Steps:**")
                st.markdown("1. Validate this root cause with data")
                st.markdown("2. Develop action plan to address it")
                st.markdown("3. Implement preventive measures")
        
        elif "Predictive" in analysis_type:
            st.markdown("### Predictive Analysis: What Will Happen?")
            st.markdown("*Use trends to forecast future outcomes*")
            
            col1, col2 = st.columns(2)
            with col1:
                current_value = st.number_input("Current month value:", value=50000)
                growth_rate = st.slider("Expected monthly growth rate (%):", -20, 50, 5)
                months_ahead = st.slider("Months to forecast:", 1, 12, 6)
            
            with col2:
                st.markdown("### Forecast")
                forecast = []
                value = current_value
                for m in range(1, months_ahead + 1):
                    value = value * (1 + growth_rate / 100)
                    forecast.append({"Month": f"M+{m}", "Forecast": value})
                
                df_forecast = pd.DataFrame(forecast)
                st.dataframe(df_forecast.style.format({"Forecast": "${:,.0f}"}), use_container_width=True)
                
                st.metric(f"Predicted value in {months_ahead} months", f"${value:,.0f}")
                st.metric("Total growth", f"{((value - current_value) / current_value) * 100:.1f}%")
        
        elif "Prescriptive" in analysis_type:
            st.markdown("### Prescriptive Analysis: What Should We Do?")
            st.markdown("*Evaluate options and recommend actions*")
            
            st.markdown("**Define Your Decision Options:**")
            
            option1_name = st.text_input("Option 1:", "Increase marketing spend")
            option1_cost = st.number_input("Cost ($):", value=10000, key="opt1_cost")
            option1_benefit = st.number_input("Expected benefit ($):", value=25000, key="opt1_ben")
            option1_risk = st.slider("Risk level:", 1, 10, 3, key="opt1_risk")
            
            st.markdown("---")
            
            option2_name = st.text_input("Option 2:", "Hire more sales staff")
            option2_cost = st.number_input("Cost ($):", value=50000, key="opt2_cost")
            option2_benefit = st.number_input("Expected benefit ($):", value=80000, key="opt2_ben")
            option2_risk = st.slider("Risk level:", 1, 10, 5, key="opt2_risk")
            
            if st.button("Get Recommendation", icon=":material/lightbulb:"):
                st.markdown("### Decision Matrix")
                
                roi1 = ((option1_benefit - option1_cost) / option1_cost) * 100 if option1_cost > 0 else 0
                roi2 = ((option2_benefit - option2_cost) / option2_cost) * 100 if option2_cost > 0 else 0
                
                score1 = roi1 / (option1_risk + 1)
                score2 = roi2 / (option2_risk + 1)
                
                comparison = pd.DataFrame({
                    'Option': [option1_name, option2_name],
                    'Cost': [f"${option1_cost:,}", f"${option2_cost:,}"],
                    'Benefit': [f"${option1_benefit:,}", f"${option2_benefit:,}"],
                    'ROI': [f"{roi1:.0f}%", f"{roi2:.0f}%"],
                    'Risk': [option1_risk, option2_risk],
                    'Score': [f"{score1:.1f}", f"{score2:.1f}"]
                })
                st.dataframe(comparison, use_container_width=True)
                
                if score1 > score2:
                    st.success(f"**Recommendation:** {option1_name} (higher risk-adjusted return)")
                else:
                    st.success(f"**Recommendation:** {option2_name} (higher risk-adjusted return)")
    
    elif playground_tab == "Project Planning Workshop":
        mui_subheader("event_note", "Project Planning Workshop")
        st.markdown("*Practice planning a data analysis project with proper phases and deliverables*")
        st.markdown("---")
        
        st.markdown("### Define Your Project")
        
        project_name = st.text_input("Project Name:", "Customer Churn Analysis")
        project_duration = st.slider("Total Project Duration (weeks):", 2, 16, 8)
        
        st.markdown("### Project Phases")
        st.markdown("*Allocate percentage of time to each phase (should total 100%)*")
        
        col1, col2 = st.columns(2)
        with col1:
            phase1 = st.slider("1. Problem Definition & Scoping", 0, 100, 15, format="%d%%")
            phase2 = st.slider("2. Data Collection & Cleaning", 0, 100, 25, format="%d%%")
            phase3 = st.slider("3. Exploratory Analysis", 0, 100, 20, format="%d%%")
        with col2:
            phase4 = st.slider("4. In-depth Analysis", 0, 100, 20, format="%d%%")
            phase5 = st.slider("5. Reporting & Presentation", 0, 100, 15, format="%d%%")
            phase6 = st.slider("6. Review & Documentation", 0, 100, 5, format="%d%%")
        
        total = phase1 + phase2 + phase3 + phase4 + phase5 + phase6
        
        if total != 100:
            st.warning(f"Total allocation is {total}%. Please adjust to equal 100%.")
        else:
            st.success("âœ“ Allocation totals 100%")
        
        st.markdown("---")
        st.markdown("### Project Timeline")
        
        phases = [
            ("Problem Definition & Scoping", phase1),
            ("Data Collection & Cleaning", phase2),
            ("Exploratory Analysis", phase3),
            ("In-depth Analysis", phase4),
            ("Reporting & Presentation", phase5),
            ("Review & Documentation", phase6)
        ]
        
        timeline_data = []
        current_week = 0
        for phase_name, pct in phases:
            weeks = (pct / 100) * project_duration
            timeline_data.append({
                "Phase": phase_name,
                "Weeks": f"{weeks:.1f}",
                "Start": f"Week {current_week + 1:.0f}",
                "End": f"Week {current_week + weeks:.0f}"
            })
            current_week += weeks
        
        df_timeline = pd.DataFrame(timeline_data)
        st.dataframe(df_timeline, use_container_width=True)
        
        st.markdown("---")
        st.markdown("### Deliverables Checklist")
        
        deliverables = {
            "Problem Definition & Scoping": ["Problem statement document", "Stakeholder requirements", "Success criteria defined"],
            "Data Collection & Cleaning": ["Data sources identified", "Data quality report", "Clean dataset ready"],
            "Exploratory Analysis": ["Summary statistics", "Initial visualizations", "Key patterns identified"],
            "In-depth Analysis": ["Analysis methodology documented", "Statistical tests completed", "Findings validated"],
            "Reporting & Presentation": ["Executive summary", "Detailed report", "Presentation slides"],
            "Review & Documentation": ["Peer review completed", "Documentation finalized", "Lessons learned"]
        }
        
        for phase_name, items in deliverables.items():
            with st.expander(f"ðŸ“ {phase_name}"):
                for item in items:
                    st.checkbox(item, key=f"del_{phase_name}_{item}")
        
        st.markdown("---")
        if st.button("Generate Project Plan Summary", type="primary", icon=":material/summarize:"):
            plan_summary = f"""PROJECT PLAN: {project_name}
========================================
Duration: {project_duration} weeks

PHASE ALLOCATION:
"""
            for phase_name, pct in phases:
                weeks = (pct / 100) * project_duration
                plan_summary += f"- {phase_name}: {pct}% ({weeks:.1f} weeks)\n"
            
            plan_summary += f"""
KEY MILESTONES:
- Week 1: Project kickoff
- Week {int(project_duration * 0.4)}: Data ready for analysis
- Week {int(project_duration * 0.7)}: Analysis complete
- Week {project_duration}: Final delivery

DELIVERABLES:
- Problem statement and scope document
- Data quality and cleaning report
- Analysis report with findings
- Executive presentation
- Project documentation
"""
            st.text_area("Project Plan Summary:", value=plan_summary, height=400)
            st.download_button(
                "Download Project Plan",
                data=plan_summary,
                file_name=f"project_plan_{project_name.lower().replace(' ', '_')}.txt",
                mime="text/plain",
                icon=":material/download:"
            )
    
    elif playground_tab == "Report Writing Workshop":
        mui_subheader("article", "Report Writing Workshop")
        st.markdown("*Practice writing professional analysis reports with proper structure and clarity*")
        st.markdown("---")
        
        report_mode = st.radio(
            "Choose practice mode:",
            ["Executive Summary Builder", "Clarity Rewriter", "Report Structure Planner", "Caption Writer"],
            horizontal=True
        )
        
        if report_mode == "Executive Summary Builder":
            st.markdown("### Executive Summary Builder")
            st.markdown("*Create a concise executive summary from your analysis findings*")
            
            st.markdown("**Enter your analysis details:**")
            
            col1, col2 = st.columns(2)
            with col1:
                main_finding = st.text_area(
                    "Main Finding:",
                    placeholder="e.g., Customer churn increased 25% last quarter",
                    height=80
                )
                root_cause = st.text_area(
                    "Root Cause/Key Insight:",
                    placeholder="e.g., Analysis of 500 surveys found 67% cited slow support response",
                    height=80
                )
            
            with col2:
                business_impact = st.text_input(
                    "Business Impact:",
                    placeholder="e.g., Estimated $200K in lost annual revenue"
                )
                recommendation = st.text_area(
                    "Recommendation:",
                    placeholder="e.g., Implement chat support system ($50K investment)",
                    height=80
                )
            
            audience = st.selectbox(
                "Target Audience:",
                ["C-Suite Executives", "Department Managers", "Technical Team", "External Clients"]
            )
            
            if st.button("Generate Executive Summary", type="primary", icon=":material/summarize:"):
                if main_finding and recommendation:
                    st.markdown("---")
                    st.markdown("### Your Executive Summary")
                    
                    if audience == "C-Suite Executives":
                        tone = "Focus on business impact and ROI"
                        length = "Ultra-brief (3-5 sentences)"
                    elif audience == "Department Managers":
                        tone = "Balance of impact and actionable details"
                        length = "Brief (5-7 sentences)"
                    elif audience == "Technical Team":
                        tone = "Include methodology context"
                        length = "Moderate detail (7-10 sentences)"
                    else:
                        tone = "Professional, accessible language"
                        length = "Clear and complete (5-8 sentences)"
                    
                    summary = f"""## Executive Summary

**Key Finding:** {main_finding}

**Impact:** {business_impact if business_impact else '[Add business impact]'}

**Analysis Insight:** {root_cause if root_cause else '[Add key insight from analysis]'}

**Recommendation:** {recommendation}

---
*Note: This summary is tailored for {audience}. Tone: {tone}. Recommended length: {length}.*
"""
                    st.markdown(summary)
                    
                    st.download_button(
                        "Download Executive Summary",
                        data=summary,
                        file_name="executive_summary.md",
                        mime="text/markdown",
                        icon=":material/download:"
                    )
                    
                    st.markdown("### Writing Tips for Your Audience:")
                    if audience == "C-Suite Executives":
                        st.info("- Lead with the bottom line\n- Quantify everything\n- Keep to 1 page maximum\n- Focus on decisions, not details")
                    elif audience == "Department Managers":
                        st.info("- Include actionable next steps\n- Show department-level impact\n- Provide timeline expectations\n- Mention resource requirements")
                    elif audience == "Technical Team":
                        st.info("- Can include methodology references\n- Technical terms are acceptable\n- Include data quality notes\n- Reference appendix for details")
                    else:
                        st.info("- Avoid internal jargon\n- Explain technical concepts\n- Focus on value delivered\n- Include clear next steps")
                else:
                    st.warning("Please fill in at least the main finding and recommendation.")
        
        elif report_mode == "Clarity Rewriter":
            st.markdown("### Clarity Rewriter")
            st.markdown("*Practice rewriting complex technical text for non-technical audiences*")
            
            sample_texts = {
                "Statistical finding": "The multivariate regression analysis yielded a statistically significant coefficient (Î²=0.43, p<0.01) for the marketing expenditure variable, indicating a positive relationship with sales outcomes.",
                "Data quality issue": "The dataset exhibited substantial missing value prevalence (23.7% null rate) concentrated primarily in temporal fields, necessitating imputation via mean substitution methodology.",
                "Correlation analysis": "Pearson correlation coefficient analysis revealed a strong positive correlation (r=0.82) between customer satisfaction metrics and repeat purchase behavior, suggesting potential causative linkages.",
                "Forecasting result": "The ARIMA(2,1,1) model generated forecasts with a MAPE of 8.3%, outperforming the baseline exponential smoothing approach by 2.1 percentage points.",
                "Custom text": ""
            }
            
            selected_text = st.selectbox("Choose a text to rewrite:", list(sample_texts.keys()))
            
            if selected_text == "Custom text":
                original_text = st.text_area(
                    "Enter your technical text:",
                    placeholder="Paste your complex technical text here...",
                    height=100
                )
            else:
                original_text = sample_texts[selected_text]
                st.markdown(f"**Original text:** {original_text}")
            
            user_rewrite = st.text_area(
                "Your simplified version:",
                placeholder="Rewrite the text in plain language...",
                height=100
            )
            
            if st.button("Show Expert Rewrite", type="primary", icon=":material/psychology:") and original_text:
                st.markdown("---")
                
                expert_rewrites = {
                    "Statistical finding": "Our analysis found that marketing spending has a strong positive effect on sales. For every additional $1,000 spent on marketing, sales increased by approximately $430. This relationship is statistically reliable.",
                    "Data quality issue": "About 24% of the data was missing, mainly in date-related fields. We filled in these gaps using average values to ensure complete analysis.",
                    "Correlation analysis": "We found a strong link between customer satisfaction and repeat purchases. Customers who are more satisfied are significantly more likely to buy again.",
                    "Forecasting result": "Our forecast model predicted sales with 92% accuracy, performing 2% better than our previous method. This gives us reliable predictions for planning.",
                    "Custom text": "Consider: 1) Replace jargon with plain words, 2) Explain what numbers mean practically, 3) Focus on the 'so what?' for the business."
                }
                
                st.markdown("### Expert Rewrite:")
                st.success(expert_rewrites.get(selected_text, expert_rewrites["Custom text"]))
                
                st.markdown("### Rewriting Principles:")
                st.markdown("""
| Technical Term | Plain Language |
|---------------|----------------|
| Statistically significant | Reliable/consistent pattern |
| Correlation | Relationship/link |
| Coefficient | Effect size/impact |
| Null values | Missing data |
| Imputation | Filling in gaps |
| MAPE/RMSE | Accuracy percentage |
""")
        
        elif report_mode == "Report Structure Planner":
            st.markdown("### Report Structure Planner")
            st.markdown("*Plan the sections and content of your analysis report*")
            
            report_type = st.selectbox(
                "What type of report are you writing?",
                ["Client Report", "Executive Summary", "Technical Report", "Ad-hoc Analysis", "Dashboard Report"]
            )
            
            report_structures = {
                "Client Report": {
                    "sections": ["Executive Summary", "Background & Objectives", "Methodology Overview", "Key Findings", "Recommendations", "Next Steps", "Appendix"],
                    "page_range": "5-15 pages",
                    "tips": "Focus on actionable insights. Minimize technical jargon. Include clear recommendations with expected outcomes."
                },
                "Executive Summary": {
                    "sections": ["Key Finding", "Business Impact", "Recommendation", "Next Steps"],
                    "page_range": "1-2 pages",
                    "tips": "Lead with the conclusion. Quantify everything. Maximum 1-2 pages. Support details in appendix only."
                },
                "Technical Report": {
                    "sections": ["Abstract", "Introduction", "Data Sources", "Methodology", "Results", "Discussion", "Limitations", "Conclusions", "References", "Appendix"],
                    "page_range": "10-30 pages",
                    "tips": "Full methodology documentation. Include code/query references. Reproducibility is key."
                },
                "Ad-hoc Analysis": {
                    "sections": ["Question/Request", "Approach", "Findings", "Answer/Recommendation"],
                    "page_range": "2-5 pages",
                    "tips": "Direct answer to the question asked. Quick turnaround expected. Keep focused."
                },
                "Dashboard Report": {
                    "sections": ["KPI Summary", "Trends", "Alerts/Exceptions", "Actions Needed"],
                    "page_range": "1-3 pages",
                    "tips": "Visual-heavy. Consistent format for regular updates. Clear RAG status indicators."
                }
            }
            
            structure = report_structures[report_type]
            
            st.markdown(f"### {report_type} Structure")
            st.caption(f"Typical length: {structure['page_range']}")
            
            st.markdown("**Recommended Sections:**")
            for i, section in enumerate(structure["sections"], 1):
                st.markdown(f"{i}. {section}")
            
            st.info(f"**Tips:** {structure['tips']}")
            
            st.markdown("---")
            st.markdown("### Plan Your Content")
            
            content_plan = {}
            for section in structure["sections"]:
                content_plan[section] = st.text_input(
                    f"{section}:",
                    placeholder=f"Brief notes for {section}...",
                    key=f"plan_{section}"
                )
            
            if st.button("Generate Report Outline", type="primary", icon=":material/summarize:"):
                outline = f"""# {report_type} Outline

Type: {report_type}
Target Length: {structure['page_range']}

## Sections

"""
                for section, notes in content_plan.items():
                    outline += f"### {section}\n"
                    outline += f"{notes if notes else '[Add content notes]'}\n\n"
                
                outline += f"""---
## Writing Tips
{structure['tips']}
"""
                
                st.text_area("Your Report Outline:", value=outline, height=400)
                st.download_button(
                    "Download Outline",
                    data=outline,
                    file_name=f"{report_type.lower().replace(' ', '_')}_outline.md",
                    mime="text/markdown",
                    icon=":material/download:"
                )
        
        elif report_mode == "Caption Writer":
            st.markdown("### Caption Writer")
            st.markdown("*Practice writing professional captions for figures and tables*")
            
            visual_type = st.selectbox(
                "What type of visual are you captioning?",
                ["Bar Chart", "Line Chart", "Pie Chart", "Table", "Scatter Plot", "Dashboard Screenshot"]
            )
            
            col1, col2 = st.columns(2)
            with col1:
                figure_number = st.number_input("Figure/Table Number:", min_value=1, value=1)
                title = st.text_input("Visual Title:", placeholder="e.g., Quarterly Revenue by Region")
            
            with col2:
                key_insight = st.text_input("Key Insight:", placeholder="e.g., North region leads by 15%")
                data_source = st.text_input("Data Source:", placeholder="e.g., Sales Database, Q4 2025")
            
            if st.button("Generate Caption", type="primary", icon=":material/auto_awesome:"):
                if title:
                    st.markdown("---")
                    st.markdown("### Generated Captions")
                    
                    prefix = "Figure" if visual_type != "Table" else "Table"
                    
                    brief_caption = f"{prefix} {figure_number}: {title}."
                    
                    standard_caption = f"{prefix} {figure_number}: {title}. {key_insight if key_insight else ''}"
                    if data_source:
                        standard_caption += f" Source: {data_source}."
                    
                    detailed_caption = f"{prefix} {figure_number}: {title}. "
                    if key_insight:
                        detailed_caption += f"Key insight: {key_insight}. "
                    detailed_caption += f"This {visual_type.lower()} illustrates [add interpretation]. "
                    if data_source:
                        detailed_caption += f"Data source: {data_source}."
                    
                    st.markdown("**Brief Caption:**")
                    st.code(brief_caption)
                    
                    st.markdown("**Standard Caption (Recommended):**")
                    st.code(standard_caption)
                    
                    st.markdown("**Detailed Caption:**")
                    st.code(detailed_caption)
                    
                    st.markdown("### Caption Best Practices:")
                    st.markdown("""
- âœ“ Always include figure/table number
- âœ“ Use descriptive title (not just "Revenue")
- âœ“ State the key insight readers should take away
- âœ“ Include data source for credibility
- âœ“ Keep captions concise but complete
- âœ“ Place below figures, above tables
""")
                else:
                    st.warning("Please enter at least a title for your visual.")
    
    elif playground_tab == "Exam Project Toolkit":
        mui_subheader("military_tech", "Exam Project Toolkit")
        st.markdown("*Practice planning and executing your comprehensive data analysis project*")
        st.markdown("---")
        
        toolkit_mode = st.radio(
            "Choose practice mode:",
            ["Problem Statement Builder", "Project Scope Planner", "Quality Self-Assessment", "Presentation Planner"],
            horizontal=True
        )
        
        if toolkit_mode == "Problem Statement Builder":
            st.markdown("### Problem Statement Builder")
            st.markdown("*Craft a clear, professional problem statement for your project*")
            
            col1, col2 = st.columns(2)
            with col1:
                organization = st.text_input(
                    "Organization/Client:",
                    placeholder="e.g., Oslo Retail Group, Local Healthcare Clinic"
                )
                topic = st.text_input(
                    "What needs to be understood?",
                    placeholder="e.g., customer churn patterns, inventory inefficiencies"
                )
            
            with col2:
                action = st.text_input(
                    "Purpose (in order to...):",
                    placeholder="e.g., improve retention strategies, reduce waste"
                )
                data_sources = st.text_input(
                    "Data to be examined:",
                    placeholder="e.g., 2 years of customer data, daily sales records"
                )
            
            deliverable = st.text_input(
                "Deliverable:",
                placeholder="e.g., recommendation report, dashboard, presentation"
            )
            deadline = st.text_input(
                "Deadline:",
                placeholder="e.g., March 15, 2026"
            )
            
            if st.button("Generate Problem Statement", type="primary", icon=":material/description:"):
                if organization and topic and action:
                    st.markdown("---")
                    st.markdown("### Your Problem Statement")
                    
                    problem_statement = f"""**{organization}** needs to understand **{topic}** in order to **{action}**. 

This analysis will examine **{data_sources if data_sources else '[specify data sources]'}** to provide **{deliverable if deliverable else '[specify deliverable]'}** by **{deadline if deadline else '[specify deadline]'}**."""
                    
                    st.success(problem_statement)
                    
                    st.markdown("### Quality Check:")
                    checks = [
                        ("Organization/stakeholder identified", bool(organization)),
                        ("Clear question to answer", bool(topic)),
                        ("Business purpose stated", bool(action)),
                        ("Data sources specified", bool(data_sources)),
                        ("Deliverable defined", bool(deliverable)),
                        ("Deadline included", bool(deadline))
                    ]
                    
                    for check, passed in checks:
                        if passed:
                            st.markdown(f"{check}")
                        else:
                            st.markdown(f"{check} - *add this for completeness*")
                    
                    st.download_button(
                        "Download Problem Statement",
                        data=problem_statement,
                        file_name="problem_statement.md",
                        mime="text/markdown",
                        icon=":material/download:"
                    )
                else:
                    st.warning("Please fill in organization, topic, and purpose at minimum.")
        
        elif toolkit_mode == "Project Scope Planner":
            st.markdown("### Project Scope Planner")
            st.markdown("*Define clear boundaries for your project*")
            
            st.markdown("**Project Duration:**")
            project_weeks = st.slider("Total project time (weeks):", 4, 16, 10, key="scope_project_weeks")
            
            st.markdown("---")
            st.markdown("### In Scope vs Out of Scope")
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**In Scope** (what you WILL do)")
                in_scope_1 = st.text_input("Question 1:", placeholder="e.g., Analyze sales trends for 2024-2025", key="scope_in_1")
                in_scope_2 = st.text_input("Question 2:", placeholder="e.g., Identify top-performing products", key="scope_in_2")
                in_scope_3 = st.text_input("Question 3:", placeholder="e.g., Create visualization dashboard", key="scope_in_3")
            
            with col2:
                st.markdown("**Out of Scope** (explicitly excluded)")
                out_scope_1 = st.text_input("Excluded 1:", placeholder="e.g., Competitor analysis", key="scope_out_1")
                out_scope_2 = st.text_input("Excluded 2:", placeholder="e.g., Predictive modeling", key="scope_out_2")
                out_scope_3 = st.text_input("Excluded 3:", placeholder="e.g., Real-time data integration", key="scope_out_3")
            
            st.markdown("---")
            st.markdown("### Time Allocation")
            
            phases = {
                "Planning & Research": st.slider("Planning & Research %:", 10, 30, 20, key="scope_phase_1"),
                "Data Collection & Cleaning": st.slider("Data Collection & Cleaning %:", 15, 35, 25, key="scope_phase_2"),
                "Analysis & Exploration": st.slider("Analysis & Exploration %:", 20, 40, 25, key="scope_phase_3"),
                "Reporting & Visualization": st.slider("Reporting & Visualization %:", 15, 30, 20, key="scope_phase_4"),
                "Review & Finalization": st.slider("Review & Finalization %:", 5, 20, 10, key="scope_phase_5")
            }
            
            total_pct = sum(phases.values())
            if total_pct != 100:
                st.warning(f"Total allocation is {total_pct}%. Adjust to equal 100%.")
            else:
                st.success("Time allocation totals 100%")
            
            if st.button("Generate Project Scope Document", type="primary", icon=":material/description:"):
                st.markdown("---")
                st.markdown("### Project Scope Document")
                
                scope_doc = f"""# Project Scope Document

## Duration
**{project_weeks} weeks**

## In Scope
"""
                for item in [in_scope_1, in_scope_2, in_scope_3]:
                    if item:
                        scope_doc += f"- {item}\n"
                
                scope_doc += """
## Out of Scope
"""
                for item in [out_scope_1, out_scope_2, out_scope_3]:
                    if item:
                        scope_doc += f"- {item}\n"
                
                scope_doc += """
## Timeline
"""
                current_week = 1
                for phase, pct in phases.items():
                    weeks = (pct / 100) * project_weeks
                    end_week = current_week + weeks - 1
                    scope_doc += f"- **{phase}**: Weeks {current_week:.0f}-{end_week:.0f} ({pct}%)\n"
                    current_week = end_week + 1
                
                st.text_area("Scope Document:", value=scope_doc, height=400)
                st.download_button(
                    "Download Scope Document",
                    data=scope_doc,
                    file_name="project_scope.md",
                    mime="text/markdown",
                    icon=":material/download:"
                )
        
        elif toolkit_mode == "Quality Self-Assessment":
            st.markdown("### Quality Self-Assessment")
            st.markdown("*Evaluate your project against professional standards before submission*")
            
            st.markdown("Rate each area honestly (1-5):")
            st.caption("1=Not done, 2=Partial, 3=Adequate, 4=Good, 5=Excellent")
            
            categories = {
                "Project Execution": [
                    "Problem clearly defined and scoped",
                    "Data sourced ethically with documentation",
                    "Methodology appropriate and documented",
                    "Analysis thorough and validated"
                ],
                "Technical Quality": [
                    "Data cleaned properly with decisions documented",
                    "Calculations verified and accurate",
                    "Code/formulas organized and commented",
                    "Process is reproducible"
                ],
                "Report Quality": [
                    "Executive summary is clear and actionable",
                    "Report has logical structure",
                    "Visualizations are accessible and labeled",
                    "Recommendations are specific and supported"
                ],
                "Presentation Quality": [
                    "Within time limit",
                    "Key points are clear",
                    "Visuals are readable from distance",
                    "Prepared for likely questions"
                ]
            }
            
            scores = {}
            for category, items in categories.items():
                st.markdown(f"**{category}**")
                for i, item in enumerate(items):
                    scores[item] = st.slider(item, 1, 5, 3, key=f"qa_{category}_{i}")
            
            if st.button("Generate Assessment Report", type="primary", icon=":material/summarize:"):
                st.markdown("---")
                st.markdown("### Your Assessment Results")
                
                total_score = sum(scores.values())
                max_score = len(scores) * 5
                percentage = (total_score / max_score) * 100
                
                if percentage >= 80:
                    st.success(f"Overall Score: {total_score}/{max_score} ({percentage:.0f}%) - **Ready to submit!**")
                elif percentage >= 60:
                    st.warning(f"Overall Score: {total_score}/{max_score} ({percentage:.0f}%) - **Good progress, address weak areas**")
                else:
                    st.error(f"Overall Score: {total_score}/{max_score} ({percentage:.0f}%) - **Needs more work**")
                
                st.markdown("### Areas to Improve:")
                weak_areas = [item for item, score in scores.items() if score < 3]
                if weak_areas:
                    for area in weak_areas:
                        st.markdown(f"- {area}")
                else:
                    st.markdown("No critical areas identified - great work!")
                
                st.markdown("### Strengths:")
                strong_areas = [item for item, score in scores.items() if score >= 4]
                if strong_areas:
                    for area in strong_areas:
                        st.markdown(f"- {area}")
        
        elif toolkit_mode == "Presentation Planner":
            st.markdown("### Presentation Planner")
            st.markdown("*Plan your project presentation structure*")
            
            total_time = st.slider("Total presentation time (minutes):", 10, 30, 18, key="exam_pres_total_time")
            
            sections = {
                "Opening": {"default": 2, "tip": "Introduce yourself, project, hook with key finding"},
                "Context": {"default": 3, "tip": "Business background, why it matters, objectives"},
                "Methodology": {"default": 3, "tip": "Data sources, tools used, key decisions"},
                "Findings": {"default": 5, "tip": "Main results with visuals, patterns identified"},
                "Recommendations": {"default": 3, "tip": "Actionable suggestions, expected impact"},
                "Conclusion": {"default": 2, "tip": "Summary, lessons learned, Q&A invitation"}
            }
            
            st.markdown("### Time Allocation per Section")
            allocations = {}
            for section, info in sections.items():
                col1, col2 = st.columns([1, 2])
                with col1:
                    allocations[section] = st.number_input(
                        f"{section} (min):",
                        min_value=1,
                        max_value=15,
                        value=info["default"],
                        key=f"pres_{section}"
                    )
                with col2:
                    st.caption(info["tip"])
            
            allocated = sum(allocations.values())
            if allocated != total_time:
                st.warning(f"Allocated {allocated} minutes, but total is {total_time} minutes. Adjust to match.")
            else:
                st.success(f"Time allocation matches {total_time} minutes")
            
            st.markdown("---")
            st.markdown("### Key Points per Section")
            
            key_points = {}
            for section in sections.keys():
                key_points[section] = st.text_area(
                    f"{section} - Main points:",
                    placeholder=f"What will you cover in the {section.lower()} section?",
                    height=60,
                    key=f"kp_{section}"
                )
            
            if st.button("Generate Presentation Outline", type="primary", icon=":material/slideshow:"):
                st.markdown("---")
                st.markdown("### Your Presentation Outline")
                
                outline = f"""# Project Presentation Outline
Total Time: {total_time} minutes

"""
                for section, minutes in allocations.items():
                    outline += f"## {section} ({minutes} min)\n"
                    outline += f"**Tip:** {sections[section]['tip']}\n"
                    if key_points[section]:
                        outline += f"**Your notes:** {key_points[section]}\n"
                    outline += "\n"
                
                outline += """---
## Presentation Tips
- Practice out loud at least 3 times
- Time yourself to ensure you fit the limit
- Have backup slides for anticipated questions
- Speak to the audience, not the slides
- Pause after key findings to let them sink in
"""
                
                st.text_area("Presentation Outline:", value=outline, height=500)
                st.download_button(
                    "Download Outline",
                    data=outline,
                    file_name="presentation_outline.md",
                    mime="text/markdown",
                    icon=":material/download:"
                )

elif page == "About":
    mui_title("info", "About the Data Analyst Program")
    st.markdown("---")
    
    st.markdown("""
    ## About the Programme
    
    Data analysts have a quintessential portfolio in every modern company ecology. Their ability to guide 
    business leaders to make informed decisions using relevant and up-to-date information 
    based on real-world data makes them a highly desired addition to every managerial team.
    
    **Effective data analysis can:**
    - Isolate workflow bottlenecks
    - Reduce operational costs
    - Solve overarching problems
    - Identify inefficient processes
    
    ---
    
    ## Programme Content
    
    This programme incorporates:
    - ðŸ“š **Theoretical knowledge**
    - ðŸ› ï¸ **Practical skills**
    - ðŸ’» **Technical competency**
    
    ### Tools and Technologies:
    - Microsoft Excel and Google Spreadsheets
    - Python programming (Python 3.x)
    - SQL and databases (on-premises and cloud)
    - Data visualization and dashboards (Tableau, Power BI)
    - Statistical analysis tools
    - Business Intelligence concepts
    
    ---
    
    ## Career Opportunities
    
    After graduation, you may qualify for work as:
    - Financial Analyst
    - Marketing Analyst
    - Logistics Analyst
    - General Data Analyst
    - Technical Analyst
    - Information Scientist
    - Operational Management
    
    ---
    
    ## Study Details
    
    | Detail | Value |
    |--------|-------|
    | Programme Code | PDAN |
    | NQF Level | 5.2 |
    | Total Credits | 120 |
    | Duration | 2 years (4 semesters) |
    | Study Start | Spring 2025 |
    | Total Hours | 3150 |
    | Study Mode | Full-time / Part-time |
    
    ---
    
    ðŸ“– [View full study catalog](https://studiekatalog.edutorium.no/voc/en/programme/PDAN/2025-autumn)
    """)
